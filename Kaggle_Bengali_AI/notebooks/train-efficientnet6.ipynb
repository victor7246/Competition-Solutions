{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datamafia7/efficientnet-b5-on-tpu/data?scriptVersionId=30017888\n",
    "\n",
    "\n",
    "Testing of DropBlock and GEM (generalized mean pooling) with conv layer before Efficient net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (0.16.2)\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (1.0.8)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (5.4.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.4.1)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.6.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.4)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (3.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.18.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (2.10.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0) (4.4.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.4.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.8.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.14.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (45.2.0.post20200210)\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras import layers as L\n",
    "import efficientnet.tfkeras as efn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kds-848f2b0ddff011fd68619e67bccf8feb0f97204d2b47eb622d3a772f'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaggleDatasets().get_gcs_path('tfrecords-grapheme-stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/main.py#L325-L326\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_builder.py#L31-L32\n",
    "  image -= tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # RGB\n",
    "  image /=  tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # RGB\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/bengaliai-cv19/discussion/134905\n",
    "\n",
    "class Generalized_mean_pooling2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=3, epsilon=1e-6, name='', **kwargs):\n",
    "      super(Generalized_mean_pooling2D, self).__init__(name, **kwargs)\n",
    "\n",
    "      self.init_p = p\n",
    "      self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "      if isinstance(input_shape, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "\n",
    "      self.build_shape = input_shape\n",
    "\n",
    "      self.p = self.add_weight(\n",
    "              name='p',\n",
    "              shape=[1,],\n",
    "              initializer=tf.keras.initializers.Constant(value=self.init_p),\n",
    "              regularizer=None,\n",
    "              trainable=True,\n",
    "              dtype=tf.float32\n",
    "              )\n",
    "\n",
    "      self.built=True\n",
    "\n",
    "    def call(self, inputs):\n",
    "      input_shape = inputs.get_shape()\n",
    "      if isinstance(inputs, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "      return (tf.reduce_mean(tf.abs(inputs**self.p), axis=[1,2], keepdims=False) + self.epsilon)**(1.0/self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock1D(tf.keras.layers.Layer):\n",
    "    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 block_size,\n",
    "                 keep_prob,\n",
    "                 sync_channels=False,\n",
    "                 data_format='channels_last',\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param block_size: Size for each mask block.\n",
    "        :param keep_prob: Probability of keeping the original feature.\n",
    "        :param sync_channels: Whether to use the same dropout for all channels.\n",
    "        :param data_format: 'channels_first' or 'channels_last' (default).\n",
    "        :param kwargs: Arguments for parent class.\n",
    "        \"\"\"\n",
    "        super(DropBlock1D, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.sync_channels = sync_channels\n",
    "        self.data_format = data_format #K.normalize_data_format(data_format)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=3)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'block_size': self.block_size,\n",
    "                  'keep_prob': self.keep_prob,\n",
    "                  'sync_channels': self.sync_channels,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(DropBlock1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _get_gamma(self, feature_dim):\n",
    "        \"\"\"Get the number of activation units to drop\"\"\"\n",
    "        feature_dim = K.cast(feature_dim, K.floatx())\n",
    "        block_size = K.constant(self.block_size, dtype=K.floatx())\n",
    "        return ((1.0 - self.keep_prob) / block_size) * (feature_dim / (feature_dim - block_size + 1.0))\n",
    "\n",
    "    def _compute_valid_seed_region(self, seq_length):\n",
    "        positions = K.arange(seq_length)\n",
    "        half_block_size = self.block_size // 2\n",
    "        valid_seed_region = K.switch(\n",
    "            K.all(\n",
    "                K.stack(\n",
    "                    [\n",
    "                        positions >= half_block_size,\n",
    "                        positions < seq_length - half_block_size,\n",
    "                    ],\n",
    "                    axis=-1,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            K.ones((seq_length,)),\n",
    "            K.zeros((seq_length,)),\n",
    "        )\n",
    "        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n",
    "\n",
    "    def _compute_drop_mask(self, shape):\n",
    "        seq_length = shape[1]\n",
    "        mask = K.random_binomial(shape, p=self._get_gamma(seq_length))\n",
    "        mask *= self._compute_valid_seed_region(seq_length)\n",
    "        mask = tf.keras.layers.MaxPool1D(\n",
    "            pool_size=self.block_size,\n",
    "            padding='same',\n",
    "            strides=1,\n",
    "            data_format='channels_last',\n",
    "        )(mask)\n",
    "        return 1.0 - mask\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        def dropped_inputs():\n",
    "            outputs = inputs\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n",
    "            shape = K.shape(outputs)\n",
    "            if self.sync_channels:\n",
    "                mask = self._compute_drop_mask([shape[0], shape[1], 1])\n",
    "            else:\n",
    "                mask = self._compute_drop_mask(shape)\n",
    "            outputs = outputs * mask *\\\n",
    "                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n",
    "            return outputs\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "\n",
    "\n",
    "class DropBlock2D(tf.keras.layers.Layer):\n",
    "    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 block_size,\n",
    "                 keep_prob,\n",
    "                 sync_channels=False,\n",
    "                 data_format='channels_last',\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param block_size: Size for each mask block.\n",
    "        :param keep_prob: Probability of keeping the original feature.\n",
    "        :param sync_channels: Whether to use the same dropout for all channels.\n",
    "        :param data_format: 'channels_first' or 'channels_last' (default).\n",
    "        :param kwargs: Arguments for parent class.\n",
    "        \"\"\"\n",
    "        super(DropBlock2D, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.sync_channels = sync_channels\n",
    "        self.data_format = data_format #K.normalize_data_format(data_format)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'block_size': self.block_size,\n",
    "                  'keep_prob': self.keep_prob,\n",
    "                  'sync_channels': self.sync_channels,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(DropBlock2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _get_gamma(self, height, width):\n",
    "        \"\"\"Get the number of activation units to drop\"\"\"\n",
    "        height, width = K.cast(height, K.floatx()), K.cast(width, K.floatx())\n",
    "        block_size = K.constant(self.block_size, dtype=K.floatx())\n",
    "        return ((1.0 - self.keep_prob) / (block_size ** 2)) *\\\n",
    "               (height * width / ((height - block_size + 1.0) * (width - block_size + 1.0)))\n",
    "\n",
    "    def _compute_valid_seed_region(self, height, width):\n",
    "        positions = K.concatenate([\n",
    "            K.expand_dims(K.tile(K.expand_dims(K.arange(height), axis=1), [1, width]), axis=-1),\n",
    "            K.expand_dims(K.tile(K.expand_dims(K.arange(width), axis=0), [height, 1]), axis=-1),\n",
    "        ], axis=-1)\n",
    "        half_block_size = self.block_size // 2\n",
    "        valid_seed_region = K.switch(\n",
    "            K.all(\n",
    "                K.stack(\n",
    "                    [\n",
    "                        positions[:, :, 0] >= half_block_size,\n",
    "                        positions[:, :, 1] >= half_block_size,\n",
    "                        positions[:, :, 0] < height - half_block_size,\n",
    "                        positions[:, :, 1] < width - half_block_size,\n",
    "                    ],\n",
    "                    axis=-1,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            K.ones((height, width)),\n",
    "            K.zeros((height, width)),\n",
    "        )\n",
    "        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n",
    "\n",
    "    def _compute_drop_mask(self, shape):\n",
    "        height, width = shape[1], shape[2]\n",
    "        mask = K.random_binomial(shape, p=self._get_gamma(height, width))\n",
    "        mask *= self._compute_valid_seed_region(height, width)\n",
    "        mask = tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(self.block_size, self.block_size),\n",
    "            padding='same',\n",
    "            strides=1,\n",
    "            data_format='channels_last',\n",
    "        )(mask)\n",
    "        return 1.0 - mask\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        def dropped_inputs():\n",
    "            outputs = inputs\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 3, 1])\n",
    "            shape = K.shape(outputs)\n",
    "            if self.sync_channels:\n",
    "                mask = self._compute_drop_mask([shape[0], shape[1], shape[2], 1])\n",
    "            else:\n",
    "                mask = self._compute_drop_mask(shape)\n",
    "            outputs = outputs * mask *\\\n",
    "                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])\n",
    "            return outputs\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs, inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, backbone='efficientnet-b6', weights='imagenet', tta=False):\n",
    "    print(f'Using backbone {backbone} and weights {weights}')\n",
    "    x = L.Input(shape=input_size, name='imgs', dtype='float32')\n",
    "    y = normalize(x)\n",
    "    if backbone.startswith('efficientnet'):\n",
    "        model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n",
    "\n",
    "    #y = L.Conv2D(3,(3,3),padding='same')(x)\n",
    "    #y = DropBlock2D(block_size=5, keep_prob=0.7, name='Dropout-1',input_shape=(input_size[0],input_size[1],3))(y)\n",
    "    y_effn = model_fn(input_shape=(input_size[0],input_size[1],3), weights=weights, include_top=False)(y)\n",
    "    \n",
    "    #model_effn = tf.keras.Model(x,y_effn)\n",
    "    \n",
    "    y_pooled = L.GlobalAveragePooling2D()(y_effn) #Generalized_mean_pooling2D()(y)\n",
    "    \n",
    "    #model_pooled = tf.keras.Model(x,y_pooled)\n",
    "    \n",
    "    y = L.Dropout(0.2)(y_pooled)\n",
    "    #y = L.Dense(512)(y)\n",
    "    \n",
    "    # 1292 of 1295 are present\n",
    "    #y1 = DropBlock1D(block_size=5,keep_prob=0.7)(y)\n",
    "    y1 = L.Dense(168, activation='softmax',name='grapheme')(y)\n",
    "\n",
    "    #y2 = DropBlock1D(block_size=5,keep_prob=0.4)(y)\n",
    "    y2 = L.Dense(11, activation='softmax',name='vowel')(y)\n",
    "\n",
    "    #y3 = DropBlock1D(block_size=5,keep_prob=0.4)(y)\n",
    "    y3 = L.Dense(7, activation='softmax',name='consonant')(y)\n",
    "    \n",
    "    model = tf.keras.Model(x, [y1,y2,y3])\n",
    "\n",
    "    if tta:\n",
    "        assert False, 'This does not make sense yet'\n",
    "        x_flip = tf.reverse(x, [2])  # 'NHWC'\n",
    "        y_tta = tf.add(model(x), model(x_flip)) / 2.0\n",
    "        tta_model = tf.keras.Model(x, y_tta)\n",
    "        return model, tta_model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backbone efficientnet-b6 and weights imagenet\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "165527552/165527152 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = get_model((160,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "imgs (InputLayer)               [(None, 160, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(None, 160, 256, 3) 0           imgs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv (TensorFlow [(None, 160, 256, 3) 0           tf_op_layer_sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b6 (Model)         (None, 5, 8, 2304)   40960136    tf_op_layer_truediv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2304)         0           efficientnet-b6[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2304)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "grapheme (Dense)                (None, 168)          387240      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           25355       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            16135       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,388,866\n",
      "Trainable params: 41,164,434\n",
      "Non-trainable params: 224,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(img_batch, label_batch, batch_size):\n",
    "    # https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/cifar10-preact18-mixup.py\n",
    "    weight = tf.random.uniform([batch_size])\n",
    "    x_weight = tf.reshape(weight, [batch_size, 1, 1, 1])\n",
    "    y_weight = tf.reshape(weight, [batch_size, 1])\n",
    "    index = tf.random.shuffle(tf.range(batch_size, dtype=tf.int32))\n",
    "    x1, x2 = img_batch, tf.gather(img_batch, index)\n",
    "    img_batch = x1 * x_weight + x2 * (1. - x_weight)\n",
    "    y1, y2 = label_batch, tf.gather(label_batch, index)\n",
    "    label_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "    return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "    # Detect hardware, return appropriate distribution strategy\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    print('REPLICAS: ', strategy.num_replicas_in_sync)\n",
    "    return strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_concatenated(image, label1, label2, label3):\n",
    "    label = tf.concat([tf.one_hot(label1, 168),tf.one_hot(label2, 11),tf.one_hot(label3, 7)],-1)\n",
    "    return image, label\n",
    "\n",
    "def one_hot(image, label1, label2, label3):\n",
    "    label = (tf.one_hot(label1, 168),tf.one_hot(label2, 11),tf.one_hot(label3, 7))\n",
    "    return image, label\n",
    "\n",
    "def read_tfrecords(example, input_size):\n",
    "    features = {\n",
    "      'img': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'grapheme_root': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'vowel_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'consonant_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'unique_tuple': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    img = tf.image.decode_image(example['img'])\n",
    "    img = tf.reshape(img, input_size + (1, ))\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # grayscale -> RGB\n",
    "    img = tf.repeat(img, 3, -1)\n",
    "\n",
    "    # image_id = tf.cast(example['image_id'], tf.int32)\n",
    "    grapheme_root = tf.cast(example['grapheme_root'], tf.int32)\n",
    "    vowel_diacritic = tf.cast(example['vowel_diacritic'], tf.int32)\n",
    "    consonant_diacritic = tf.cast(example['consonant_diacritic'], tf.int32)\n",
    "    # unique_tuple = tf.cast(example['unique_tuple'], tf.int32)\n",
    "    return img, grapheme_root,vowel_diacritic,consonant_diacritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRecall(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, val_ds):\n",
    "        self.best_score = 0\n",
    "        self.val_ds = val_ds\n",
    "        \n",
    "        self.val_pred_grapheme = []\n",
    "        self.val_pred_vowel = []\n",
    "        self.val_pred_cons = []\n",
    "        \n",
    "        self.val_targ_grapheme = []\n",
    "        self.val_targ_vowel = []\n",
    "        self.val_targ_cons = []\n",
    "        \n",
    "    def on_test_batch_end(self, batch, logs={}):\n",
    "        \n",
    "        print (batch)\n",
    "        \n",
    "        for batch_data in self.val_ds.take(batch+1):\n",
    "            pass\n",
    "        \n",
    "        val_predict = self.model.predict(batch_data[0])\n",
    "        val_targ = batch_data[1]\n",
    "        \n",
    "        self.val_pred_grapheme += val_predict[0].argmax(1).tolist()\n",
    "        self.val_pred_vowel += val_predict[1].argmax(1).tolist()\n",
    "        self.val_pred_cons += val_predict[2].argmax(1).tolist()\n",
    "        \n",
    "        self.val_targ_grapheme += val_targ[0].numpy().argmax(1).tolist()\n",
    "        self.val_targ_vowel += val_targ[1].numpy().argmax(1).tolist()\n",
    "        self.val_targ_cons += val_targ[2].numpy().argmax(1).tolist()\n",
    "    \n",
    "    def on_test_end(self, logs={}):\n",
    "        \n",
    "        recall_grapheme = recall_score(self.val_targ_grapheme, self.val_pred_grapheme, average='macro')\n",
    "        recall_vowel = recall_score(self.val_targ_vowel, self.val_pred_vowel, average='macro')\n",
    "        recall_cons = recall_score(self.val_targ_cons, self.val_pred_cons, average='macro')\n",
    "        \n",
    "        overall_recall = np.average([recall_grapheme,recall_vowel,recall_cons],weights=[.5,.25,.25])\n",
    "        \n",
    "        print (overall_recall, recall_grapheme, recall_vowel, recall_cons)\n",
    "        \n",
    "        if overall_recall > self.best_score:\n",
    "            self.best_score = overall_score\n",
    "            \n",
    "            weight_fn = 'model-%d.h5' % (self.best_score*100)\n",
    "            model.save_weights(weight_fn)\n",
    "            print(f'Saved weights to: {weight_fn}')\n",
    "            \n",
    "    '''\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        recall_grapheme = recall_score(self.val_targ_grapheme, self.val_pred_grapheme, average='macro')\n",
    "        recall_vowel = recall_score(self.val_targ_vowel, self.val_pred_vowel, average='macro')\n",
    "        recall_cons = recall_score(self.val_targ_cons, self.val_pred_cons, average='macro')\n",
    "        \n",
    "        overall_recall = np.average([recall_grapheme,recall_vowel,recall_cons],weights=[.5,.25,.25])\n",
    "        \n",
    "        print (overall_recall, recall_grapheme, recall_vowel, recall_cons)\n",
    "        \n",
    "        if overall_recall > self.best_score:\n",
    "            self.best_score = overall_score\n",
    "            \n",
    "            weight_fn = 'model-%d-%d.h5' % (epoch, self.best_score*100)\n",
    "            model.save_weights(weight_fn)\n",
    "            print(f'Saved weights to: {weight_fn}')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_old(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global parser, train_ds, val_ds, model, num_val_samples, val_step\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_id', type=int, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    parser.add_argument('--lr', type=float, default=2e-4)\n",
    "    parser.add_argument('--input_size', type=str, default='160,256')\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=30)\n",
    "    parser.add_argument('--backbone', type=str, default='efficientnet-b6')\n",
    "    parser.add_argument('--weights', type=str, default='imagenet')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    args.input_size = tuple(int(x) for x in args.input_size.split(','))\n",
    "    np.random.seed(args.seed)\n",
    "    tf.random.set_seed(args.seed)\n",
    "\n",
    "    # build the model\n",
    "    strategy = get_strategy()\n",
    "    with strategy.scope():\n",
    "        model = get_model(input_size=args.input_size + (3, ), backbone=args.backbone,\n",
    "            weights=args.weights)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=args.lr),\n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=[categorical_accuracy, tf.keras.metrics.Recall()]) #\n",
    "    \n",
    "    print(model.summary())\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    # create the training and validation datasets\n",
    "    ds_path = KaggleDatasets().get_gcs_path('tfrecords-grapheme-stratified') #KaggleDatasets().get_gcs_path('bengali-tfrecords-v010')\n",
    "    \n",
    "    train_fns = tf.io.gfile.glob(os.path.join(ds_path, 'train*.tfrec')) #tf.io.gfile.glob(os.path.join(ds_path, 'records/train*.tfrec'))\n",
    "    train_ds = tf.data.TFRecordDataset(train_fns, num_parallel_reads=AUTO)\n",
    "    train_ds = train_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "    train_ds = train_ds.repeat().batch(args.batch_size)\n",
    "    train_ds = train_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "    #train_ds = train_ds.map(lambda a, b: mixup(a, b, args.batch_size), num_parallel_calls=AUTO)\n",
    "\n",
    "    val_fns = tf.io.gfile.glob(os.path.join(ds_path, 'val*.tfrec')) #tf.io.gfile.glob(os.path.join(ds_path, 'records/val*.tfrec'))\n",
    "    val_ds = tf.data.TFRecordDataset(val_fns, num_parallel_reads=AUTO)\n",
    "    val_ds = val_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "    val_ds = val_ds.batch(args.batch_size)\n",
    "    val_ds = val_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='val_grapheme_recall', mode='max', patience=4, verbose=1)\n",
    "    def scheduler(epoch):\n",
    "        if epoch < 4:\n",
    "            return args.lr\n",
    "        else:\n",
    "            return args.lr * tf.math.exp(0.2 * (3 - epoch))\n",
    "    callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "    \n",
    "    weight_fn = 'model-%04d.h5' % args.model_id\n",
    "    \n",
    "    callback3 = tf.keras.callbacks.ModelCheckpoint(monitor='val_grapheme_recall', mode='max', save_best_only=True, filepath=weight_fn, verbose=1) #CustomRecall(val_ds)\n",
    "    \n",
    "    # train\n",
    "    num_train_samples = sum(int(fn.split('_')[2]) for fn in train_fns)\n",
    "    num_val_samples = sum(int(fn.split('_')[2]) for fn in val_fns)\n",
    "    steps_per_epoch = num_train_samples // args.batch_size\n",
    "    val_step = num_val_samples // args.batch_size\n",
    "    \n",
    "    print(f'Training on {num_train_samples} samples. Each epochs requires {steps_per_epoch} steps')\n",
    "    print(f'Validation on {num_val_samples} samples.')\n",
    "    \n",
    "    h = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=args.epochs, verbose=1,\n",
    "      validation_data=val_ds,callbacks=[callback1,callback2,callback3])\n",
    "    \n",
    "    #print(h)\n",
    "    #weight_fn = 'model-%04d.h5' % args.model_id\n",
    "    #model.save_weights(weight_fn)\n",
    "    #print(f'Saved weights to: {weight_fn}')\n",
    "    \n",
    "    #model_effn.save_weights('model_effn.h5')\n",
    "    #model_pooled.save_weights('model_pooled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.0.0.2:8470']\n",
      "REPLICAS:  8\n",
      "Using backbone efficientnet-b6 and weights imagenet\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "imgs (InputLayer)               [(None, 160, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_1 (TensorFlowOp [(None, 160, 256, 3) 0           imgs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_1 (TensorFl [(None, 160, 256, 3) 0           tf_op_layer_sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b6 (Model)         (None, 5, 8, 2304)   40960136    tf_op_layer_truediv_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2304)         0           efficientnet-b6[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2304)         0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "grapheme (Dense)                (None, 168)          387240      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           25355       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            16135       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 41,388,866\n",
      "Trainable params: 41,164,434\n",
      "Non-trainable params: 224,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Training on 160672 samples. Each epochs requires 1255 steps\n",
      "Validation on 40168 samples.\n",
      "Train for 1255 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 1/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 1.9435 - grapheme_loss: 1.3766 - vowel_loss: 0.3199 - consonant_loss: 0.2537 - grapheme_categorical_accuracy: 0.6737 - grapheme_recall: 0.5531 - vowel_categorical_accuracy: 0.9026 - vowel_recall: 0.8696 - consonant_categorical_accuracy: 0.9224 - consonant_recall: 0.9039\n",
      "Epoch 00001: val_grapheme_recall improved from -inf to 0.91137, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 530s 423ms/step - loss: 1.9425 - grapheme_loss: 1.3757 - vowel_loss: 0.3197 - consonant_loss: 0.2536 - grapheme_categorical_accuracy: 0.6740 - grapheme_recall: 0.5534 - vowel_categorical_accuracy: 0.9026 - vowel_recall: 0.8697 - consonant_categorical_accuracy: 0.9225 - consonant_recall: 0.9039 - val_loss: 0.4172 - val_grapheme_loss: 0.2603 - val_vowel_loss: 0.0826 - val_consonant_loss: 0.0671 - val_grapheme_categorical_accuracy: 0.9201 - val_grapheme_recall: 0.9114 - val_vowel_categorical_accuracy: 0.9803 - val_vowel_recall: 0.9793 - val_consonant_categorical_accuracy: 0.9809 - val_consonant_recall: 0.9805\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 2/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.4508 - grapheme_loss: 0.2715 - vowel_loss: 0.0849 - consonant_loss: 0.0791 - grapheme_categorical_accuracy: 0.9238 - grapheme_recall: 0.9022 - vowel_categorical_accuracy: 0.9763 - vowel_recall: 0.9743 - consonant_categorical_accuracy: 0.9768 - consonant_recall: 0.9758\n",
      "Epoch 00002: val_grapheme_recall improved from 0.91137 to 0.93627, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 316s 252ms/step - loss: 0.4506 - grapheme_loss: 0.2713 - vowel_loss: 0.0849 - consonant_loss: 0.0791 - grapheme_categorical_accuracy: 0.9239 - grapheme_recall: 0.9022 - vowel_categorical_accuracy: 0.9762 - vowel_recall: 0.9743 - consonant_categorical_accuracy: 0.9768 - consonant_recall: 0.9758 - val_loss: 0.3344 - val_grapheme_loss: 0.2042 - val_vowel_loss: 0.0719 - val_consonant_loss: 0.0483 - val_grapheme_categorical_accuracy: 0.9399 - val_grapheme_recall: 0.9363 - val_vowel_categorical_accuracy: 0.9847 - val_vowel_recall: 0.9843 - val_consonant_categorical_accuracy: 0.9863 - val_consonant_recall: 0.9859\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 3/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.2819 - grapheme_loss: 0.1685 - vowel_loss: 0.0590 - consonant_loss: 0.0497 - grapheme_categorical_accuracy: 0.9515 - grapheme_recall: 0.9415 - vowel_categorical_accuracy: 0.9839 - vowel_recall: 0.9829 - consonant_categorical_accuracy: 0.9862 - consonant_recall: 0.9858\n",
      "Epoch 00003: val_grapheme_recall improved from 0.93627 to 0.94642, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 315s 251ms/step - loss: 0.2818 - grapheme_loss: 0.1684 - vowel_loss: 0.0590 - consonant_loss: 0.0496 - grapheme_categorical_accuracy: 0.9514 - grapheme_recall: 0.9415 - vowel_categorical_accuracy: 0.9839 - vowel_recall: 0.9829 - consonant_categorical_accuracy: 0.9863 - consonant_recall: 0.9858 - val_loss: 0.3264 - val_grapheme_loss: 0.1945 - val_vowel_loss: 0.0679 - val_consonant_loss: 0.0564 - val_grapheme_categorical_accuracy: 0.9492 - val_grapheme_recall: 0.9464 - val_vowel_categorical_accuracy: 0.9855 - val_vowel_recall: 0.9855 - val_consonant_categorical_accuracy: 0.9851 - val_consonant_recall: 0.9851\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 4/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.1987 - grapheme_loss: 0.1137 - vowel_loss: 0.0377 - consonant_loss: 0.0391 - grapheme_categorical_accuracy: 0.9676 - grapheme_recall: 0.9607 - vowel_categorical_accuracy: 0.9893 - vowel_recall: 0.9886 - consonant_categorical_accuracy: 0.9882 - consonant_recall: 0.9878\n",
      "Epoch 00004: val_grapheme_recall did not improve from 0.94642\n",
      "1255/1255 [==============================] - 309s 246ms/step - loss: 0.1986 - grapheme_loss: 0.1137 - vowel_loss: 0.0377 - consonant_loss: 0.0391 - grapheme_categorical_accuracy: 0.9675 - grapheme_recall: 0.9606 - vowel_categorical_accuracy: 0.9893 - vowel_recall: 0.9886 - consonant_categorical_accuracy: 0.9882 - consonant_recall: 0.9878 - val_loss: 0.3401 - val_grapheme_loss: 0.2032 - val_vowel_loss: 0.0643 - val_consonant_loss: 0.0542 - val_grapheme_categorical_accuracy: 0.9478 - val_grapheme_recall: 0.9460 - val_vowel_categorical_accuracy: 0.9865 - val_vowel_recall: 0.9861 - val_consonant_categorical_accuracy: 0.9884 - val_consonant_recall: 0.9881\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.00016374615, shape=(), dtype=float32).\n",
      "Epoch 5/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.1247 - grapheme_loss: 0.0757 - vowel_loss: 0.0235 - consonant_loss: 0.0252 - grapheme_categorical_accuracy: 0.9776 - grapheme_recall: 0.9738 - vowel_categorical_accuracy: 0.9936 - vowel_recall: 0.9935 - consonant_categorical_accuracy: 0.9932 - consonant_recall: 0.9931\n",
      "Epoch 00005: val_grapheme_recall improved from 0.94642 to 0.95081, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 315s 251ms/step - loss: 0.1247 - grapheme_loss: 0.0756 - vowel_loss: 0.0235 - consonant_loss: 0.0252 - grapheme_categorical_accuracy: 0.9776 - grapheme_recall: 0.9739 - vowel_categorical_accuracy: 0.9936 - vowel_recall: 0.9935 - consonant_categorical_accuracy: 0.9932 - consonant_recall: 0.9931 - val_loss: 0.3233 - val_grapheme_loss: 0.1920 - val_vowel_loss: 0.0686 - val_consonant_loss: 0.0498 - val_grapheme_categorical_accuracy: 0.9540 - val_grapheme_recall: 0.9508 - val_vowel_categorical_accuracy: 0.9865 - val_vowel_recall: 0.9865 - val_consonant_categorical_accuracy: 0.9871 - val_consonant_recall: 0.9871\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.000134064, shape=(), dtype=float32).\n",
      "Epoch 6/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.0811 - grapheme_loss: 0.0497 - vowel_loss: 0.0178 - consonant_loss: 0.0144 - grapheme_categorical_accuracy: 0.9851 - grapheme_recall: 0.9832 - vowel_categorical_accuracy: 0.9947 - vowel_recall: 0.9945 - consonant_categorical_accuracy: 0.9957 - consonant_recall: 0.9955\n",
      "Epoch 00006: val_grapheme_recall improved from 0.95081 to 0.95240, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 315s 251ms/step - loss: 0.0811 - grapheme_loss: 0.0497 - vowel_loss: 0.0178 - consonant_loss: 0.0144 - grapheme_categorical_accuracy: 0.9852 - grapheme_recall: 0.9832 - vowel_categorical_accuracy: 0.9947 - vowel_recall: 0.9945 - consonant_categorical_accuracy: 0.9957 - consonant_recall: 0.9955 - val_loss: 0.3443 - val_grapheme_loss: 0.2077 - val_vowel_loss: 0.0776 - val_consonant_loss: 0.0597 - val_grapheme_categorical_accuracy: 0.9534 - val_grapheme_recall: 0.9524 - val_vowel_categorical_accuracy: 0.9859 - val_vowel_recall: 0.9855 - val_consonant_categorical_accuracy: 0.9877 - val_consonant_recall: 0.9877\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.00010976232, shape=(), dtype=float32).\n",
      "Epoch 7/30\n",
      "1049/1255 [========================>.....] - ETA: 45s - loss: 0.0366 - grapheme_loss: 0.0212 - vowel_loss: 0.0094 - consonant_loss: 0.0063 - grapheme_categorical_accuracy: 0.9942 - grapheme_recall: 0.9933 - vowel_categorical_accuracy: 0.9973 - vowel_recall: 0.9971 - consonant_categorical_accuracy: 0.9983 - consonant_recall: 0.9983\n",
      "Epoch 00009: val_grapheme_recall improved from 0.95718 to 0.95798, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 317s 252ms/step - loss: 0.0239 - grapheme_loss: 0.0140 - vowel_loss: 0.0039 - consonant_loss: 0.0044 - grapheme_categorical_accuracy: 0.9961 - grapheme_recall: 0.9957 - vowel_categorical_accuracy: 0.9991 - vowel_recall: 0.9991 - consonant_categorical_accuracy: 0.9990 - consonant_recall: 0.9990 - val_loss: 0.3508 - val_grapheme_loss: 0.2212 - val_vowel_loss: 0.0714 - val_consonant_loss: 0.0581 - val_grapheme_categorical_accuracy: 0.9590 - val_grapheme_recall: 0.9580 - val_vowel_categorical_accuracy: 0.9877 - val_vowel_recall: 0.9877 - val_consonant_categorical_accuracy: 0.9894 - val_consonant_recall: 0.9894\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(6.0238835e-05, shape=(), dtype=float32).\n",
      "Epoch 10/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.0173 - grapheme_loss: 0.0115 - vowel_loss: 0.0035 - consonant_loss: 0.0035 - grapheme_categorical_accuracy: 0.9972 - grapheme_recall: 0.9969 - vowel_categorical_accuracy: 0.9991 - vowel_recall: 0.9991 - consonant_categorical_accuracy: 0.9987 - consonant_recall: 0.9987\n",
      "Epoch 00010: val_grapheme_recall improved from 0.95798 to 0.95897, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 314s 251ms/step - loss: 0.0173 - grapheme_loss: 0.0115 - vowel_loss: 0.0035 - consonant_loss: 0.0035 - grapheme_categorical_accuracy: 0.9972 - grapheme_recall: 0.9969 - vowel_categorical_accuracy: 0.9991 - vowel_recall: 0.9991 - consonant_categorical_accuracy: 0.9987 - consonant_recall: 0.9987 - val_loss: 0.3518 - val_grapheme_loss: 0.2248 - val_vowel_loss: 0.0742 - val_consonant_loss: 0.0610 - val_grapheme_categorical_accuracy: 0.9608 - val_grapheme_recall: 0.9590 - val_vowel_categorical_accuracy: 0.9894 - val_vowel_recall: 0.9892 - val_consonant_categorical_accuracy: 0.9906 - val_consonant_recall: 0.9906\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(4.9319395e-05, shape=(), dtype=float32).\n",
      "Epoch 11/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.0078 - grapheme_loss: 0.0043 - vowel_loss: 0.0013 - consonant_loss: 0.0015 - grapheme_categorical_accuracy: 0.9991 - grapheme_recall: 0.9990 - vowel_categorical_accuracy: 0.9998 - vowel_recall: 0.9998 - consonant_categorical_accuracy: 0.9997 - consonant_recall: 0.9997\n",
      "Epoch 00013: val_grapheme_recall did not improve from 0.96076\n",
      "1255/1255 [==============================] - 309s 246ms/step - loss: 0.0078 - grapheme_loss: 0.0043 - vowel_loss: 0.0013 - consonant_loss: 0.0015 - grapheme_categorical_accuracy: 0.9991 - grapheme_recall: 0.9990 - vowel_categorical_accuracy: 0.9998 - vowel_recall: 0.9998 - consonant_categorical_accuracy: 0.9997 - consonant_recall: 0.9997 - val_loss: 0.3622 - val_grapheme_loss: 0.2248 - val_vowel_loss: 0.0807 - val_consonant_loss: 0.0611 - val_grapheme_categorical_accuracy: 0.9616 - val_grapheme_recall: 0.9606 - val_vowel_categorical_accuracy: 0.9894 - val_vowel_recall: 0.9894 - val_consonant_categorical_accuracy: 0.9906 - val_consonant_recall: 0.9906\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(2.7067055e-05, shape=(), dtype=float32).\n",
      "Epoch 14/30\n",
      " 476/1255 [==========>...................] - ETA: 2:54 - loss: 0.0072 - grapheme_loss: 0.0045 - vowel_loss: 8.3835e-04 - consonant_loss: 0.0011 - grapheme_categorical_accuracy: 0.9987 - grapheme_recall: 0.9984 - vowel_categorical_accuracy: 0.9999 - vowel_recall: 0.9999 - consonant_categorical_accuracy: 0.9999 - consonant_recall: 0.9999\n",
      "Epoch 00017: val_grapheme_recall did not improve from 0.96296\n",
      "1255/1255 [==============================] - 310s 247ms/step - loss: 0.0042 - grapheme_loss: 0.0030 - vowel_loss: 6.4291e-04 - consonant_loss: 6.9521e-04 - grapheme_categorical_accuracy: 0.9995 - grapheme_recall: 0.9995 - vowel_categorical_accuracy: 1.0000 - vowel_recall: 1.0000 - consonant_categorical_accuracy: 0.9998 - consonant_recall: 0.9998 - val_loss: 0.3675 - val_grapheme_loss: 0.2326 - val_vowel_loss: 0.0819 - val_consonant_loss: 0.0603 - val_grapheme_categorical_accuracy: 0.9628 - val_grapheme_recall: 0.9624 - val_vowel_categorical_accuracy: 0.9892 - val_vowel_recall: 0.9892 - val_consonant_categorical_accuracy: 0.9912 - val_consonant_recall: 0.9912\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(1.2162013e-05, shape=(), dtype=float32).\n",
      "Epoch 18/30\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 0.0035 - grapheme_loss: 0.0017 - vowel_loss: 4.5764e-04 - consonant_loss: 7.9361e-04 - grapheme_categorical_accuracy: 0.9998 - grapheme_recall: 0.9997 - vowel_categorical_accuracy: 1.0000 - vowel_recall: 1.0000 - consonant_categorical_accuracy: 0.9998 - consonant_recall: 0.9998\n",
      "Epoch 00019: val_grapheme_recall did not improve from 0.96296\n",
      "1255/1255 [==============================] - 311s 248ms/step - loss: 0.0035 - grapheme_loss: 0.0017 - vowel_loss: 4.5734e-04 - consonant_loss: 7.9299e-04 - grapheme_categorical_accuracy: 0.9998 - grapheme_recall: 0.9997 - vowel_categorical_accuracy: 1.0000 - vowel_recall: 1.0000 - consonant_categorical_accuracy: 0.9998 - consonant_recall: 0.9998 - val_loss: 0.3685 - val_grapheme_loss: 0.2371 - val_vowel_loss: 0.0817 - val_consonant_loss: 0.0608 - val_grapheme_categorical_accuracy: 0.9634 - val_grapheme_recall: 0.9630 - val_vowel_categorical_accuracy: 0.9896 - val_vowel_recall: 0.9896 - val_consonant_categorical_accuracy: 0.9908 - val_consonant_recall: 0.9908\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(8.15244e-06, shape=(), dtype=float32).\n",
      "Epoch 20/30\n",
      "  45/1255 [>.............................] - ETA: 4:29 - loss: 0.0041 - grapheme_loss: 0.0026 - vowel_loss: 1.8944e-04 - consonant_loss: 0.0019 - grapheme_categorical_accuracy: 0.9986 - grapheme_recall: 0.9986 - vowel_categorical_accuracy: 1.0000 - vowel_recall: 1.0000 - consonant_categorical_accuracy: 0.9986 - consonant_recall: 0.9986"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
