{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datamafia7/efficientnet-b5-on-tpu/data?scriptVersionId=30017888\n",
    "\n",
    "\n",
    "Testing of DropBlock and GEM (generalized mean pooling) with conv layer before Efficient net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (0.16.2)\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (1.0.8)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.4.1)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (5.4.1)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.6.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.4)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (3.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.18.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (2.10.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0) (4.4.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.4.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.8.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.14.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (45.2.0.post20200210)\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kaggle-efficientnet-repo/efficientnet-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras import layers as L\n",
    "import efficientnet.tfkeras as efn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kds-85103a0b82f80def92c1434d603b9689a68feee15283d8f5722d339a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KaggleDatasets().get_gcs_path('tfrecords-grapheme-stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/main.py#L325-L326\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_builder.py#L31-L32\n",
    "  image -= tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # RGB\n",
    "  image /=  tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # RGB\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/bengaliai-cv19/discussion/134905\n",
    "\n",
    "class Generalized_mean_pooling2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=3, epsilon=1e-6, name='', **kwargs):\n",
    "      super(Generalized_mean_pooling2D, self).__init__(name, **kwargs)\n",
    "\n",
    "      self.init_p = p\n",
    "      self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "      if isinstance(input_shape, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "\n",
    "      self.build_shape = input_shape\n",
    "\n",
    "      self.p = self.add_weight(\n",
    "              name='p',\n",
    "              shape=[1,],\n",
    "              initializer=tf.keras.initializers.Constant(value=self.init_p),\n",
    "              regularizer=None,\n",
    "              trainable=True,\n",
    "              dtype=tf.float32\n",
    "              )\n",
    "\n",
    "      self.built=True\n",
    "\n",
    "    def call(self, inputs):\n",
    "      input_shape = inputs.get_shape()\n",
    "      if isinstance(inputs, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "      return (tf.reduce_mean(tf.abs(inputs**self.p), axis=[1,2], keepdims=False) + self.epsilon)**(1.0/self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock1D(tf.keras.layers.Layer):\n",
    "    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 block_size,\n",
    "                 keep_prob,\n",
    "                 sync_channels=False,\n",
    "                 data_format='channels_last',\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param block_size: Size for each mask block.\n",
    "        :param keep_prob: Probability of keeping the original feature.\n",
    "        :param sync_channels: Whether to use the same dropout for all channels.\n",
    "        :param data_format: 'channels_first' or 'channels_last' (default).\n",
    "        :param kwargs: Arguments for parent class.\n",
    "        \"\"\"\n",
    "        super(DropBlock1D, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.sync_channels = sync_channels\n",
    "        self.data_format = data_format #K.normalize_data_format(data_format)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=3)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'block_size': self.block_size,\n",
    "                  'keep_prob': self.keep_prob,\n",
    "                  'sync_channels': self.sync_channels,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(DropBlock1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _get_gamma(self, feature_dim):\n",
    "        \"\"\"Get the number of activation units to drop\"\"\"\n",
    "        feature_dim = K.cast(feature_dim, K.floatx())\n",
    "        block_size = K.constant(self.block_size, dtype=K.floatx())\n",
    "        return ((1.0 - self.keep_prob) / block_size) * (feature_dim / (feature_dim - block_size + 1.0))\n",
    "\n",
    "    def _compute_valid_seed_region(self, seq_length):\n",
    "        positions = K.arange(seq_length)\n",
    "        half_block_size = self.block_size // 2\n",
    "        valid_seed_region = K.switch(\n",
    "            K.all(\n",
    "                K.stack(\n",
    "                    [\n",
    "                        positions >= half_block_size,\n",
    "                        positions < seq_length - half_block_size,\n",
    "                    ],\n",
    "                    axis=-1,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            K.ones((seq_length,)),\n",
    "            K.zeros((seq_length,)),\n",
    "        )\n",
    "        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n",
    "\n",
    "    def _compute_drop_mask(self, shape):\n",
    "        seq_length = shape[1]\n",
    "        mask = K.random_binomial(shape, p=self._get_gamma(seq_length))\n",
    "        mask *= self._compute_valid_seed_region(seq_length)\n",
    "        mask = tf.keras.layers.MaxPool1D(\n",
    "            pool_size=self.block_size,\n",
    "            padding='same',\n",
    "            strides=1,\n",
    "            data_format='channels_last',\n",
    "        )(mask)\n",
    "        return 1.0 - mask\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        def dropped_inputs():\n",
    "            outputs = inputs\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n",
    "            shape = K.shape(outputs)\n",
    "            if self.sync_channels:\n",
    "                mask = self._compute_drop_mask([shape[0], shape[1], 1])\n",
    "            else:\n",
    "                mask = self._compute_drop_mask(shape)\n",
    "            outputs = outputs * mask *\\\n",
    "                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n",
    "            return outputs\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "\n",
    "\n",
    "class DropBlock2D(tf.keras.layers.Layer):\n",
    "    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 block_size,\n",
    "                 keep_prob,\n",
    "                 sync_channels=False,\n",
    "                 data_format='channels_last',\n",
    "                 **kwargs):\n",
    "        \"\"\"Initialize the layer.\n",
    "        :param block_size: Size for each mask block.\n",
    "        :param keep_prob: Probability of keeping the original feature.\n",
    "        :param sync_channels: Whether to use the same dropout for all channels.\n",
    "        :param data_format: 'channels_first' or 'channels_last' (default).\n",
    "        :param kwargs: Arguments for parent class.\n",
    "        \"\"\"\n",
    "        super(DropBlock2D, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.sync_channels = sync_channels\n",
    "        self.data_format = data_format #K.normalize_data_format(data_format)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'block_size': self.block_size,\n",
    "                  'keep_prob': self.keep_prob,\n",
    "                  'sync_channels': self.sync_channels,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(DropBlock2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _get_gamma(self, height, width):\n",
    "        \"\"\"Get the number of activation units to drop\"\"\"\n",
    "        height, width = K.cast(height, K.floatx()), K.cast(width, K.floatx())\n",
    "        block_size = K.constant(self.block_size, dtype=K.floatx())\n",
    "        return ((1.0 - self.keep_prob) / (block_size ** 2)) *\\\n",
    "               (height * width / ((height - block_size + 1.0) * (width - block_size + 1.0)))\n",
    "\n",
    "    def _compute_valid_seed_region(self, height, width):\n",
    "        positions = K.concatenate([\n",
    "            K.expand_dims(K.tile(K.expand_dims(K.arange(height), axis=1), [1, width]), axis=-1),\n",
    "            K.expand_dims(K.tile(K.expand_dims(K.arange(width), axis=0), [height, 1]), axis=-1),\n",
    "        ], axis=-1)\n",
    "        half_block_size = self.block_size // 2\n",
    "        valid_seed_region = K.switch(\n",
    "            K.all(\n",
    "                K.stack(\n",
    "                    [\n",
    "                        positions[:, :, 0] >= half_block_size,\n",
    "                        positions[:, :, 1] >= half_block_size,\n",
    "                        positions[:, :, 0] < height - half_block_size,\n",
    "                        positions[:, :, 1] < width - half_block_size,\n",
    "                    ],\n",
    "                    axis=-1,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            ),\n",
    "            K.ones((height, width)),\n",
    "            K.zeros((height, width)),\n",
    "        )\n",
    "        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n",
    "\n",
    "    def _compute_drop_mask(self, shape):\n",
    "        height, width = shape[1], shape[2]\n",
    "        mask = K.random_binomial(shape, p=self._get_gamma(height, width))\n",
    "        mask *= self._compute_valid_seed_region(height, width)\n",
    "        mask = tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(self.block_size, self.block_size),\n",
    "            padding='same',\n",
    "            strides=1,\n",
    "            data_format='channels_last',\n",
    "        )(mask)\n",
    "        return 1.0 - mask\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        def dropped_inputs():\n",
    "            outputs = inputs\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 2, 3, 1])\n",
    "            shape = K.shape(outputs)\n",
    "            if self.sync_channels:\n",
    "                mask = self._compute_drop_mask([shape[0], shape[1], shape[2], 1])\n",
    "            else:\n",
    "                mask = self._compute_drop_mask(shape)\n",
    "            outputs = outputs * mask *\\\n",
    "                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n",
    "            if self.data_format == 'channels_first':\n",
    "                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])\n",
    "            return outputs\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs, inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, backbone='dense', weights='imagenet', tta=False):\n",
    "    print(f'Using backbone {backbone} and weights {weights}')\n",
    "    x = L.Input(shape=input_size, name='imgs', dtype='float32')\n",
    "    y = normalize(x)\n",
    "    if backbone == 'dense':\n",
    "        model_fn = tf.keras.applications.densenet.DenseNet169(input_shape=(input_size[0],input_size[1],3), weights=weights, include_top=False)\n",
    "    if backbone == 'inception':\n",
    "        model_fn = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=(input_size[0],input_size[1],3), weights=weights, include_top=False)\n",
    "    if backbone == 'xception':\n",
    "        model_fn = tf.keras.applications.xception.Xception(input_shape=(input_size[0],input_size[1],3), weights=weights, include_top=False)\n",
    "        \n",
    "    #y = L.Conv2D(3,(3,3),padding='same')(x)\n",
    "    #y = DropBlock2D(block_size=5, keep_prob=0.7, name='Dropout-1',input_shape=(input_size[0],input_size[1],3))(y)\n",
    "    y_effn = model_fn(y)\n",
    "    \n",
    "    #model_effn = tf.keras.Model(x,y_effn)\n",
    "    \n",
    "    y_pooled = L.GlobalAveragePooling2D()(y_effn) #Generalized_mean_pooling2D()(y)\n",
    "    \n",
    "    #model_pooled = tf.keras.Model(x,y_pooled)\n",
    "    \n",
    "    y = L.Dropout(0.2)(y_pooled)\n",
    "    #y = L.Dense(512)(y)\n",
    "    \n",
    "    # 1292 of 1295 are present\n",
    "    #y1 = DropBlock1D(block_size=5,keep_prob=0.7)(y)\n",
    "    y1 = L.Dense(168, activation='softmax',name='grapheme')(y)\n",
    "\n",
    "    #y2 = DropBlock1D(block_size=5,keep_prob=0.4)(y)\n",
    "    y2 = L.Dense(11, activation='softmax',name='vowel')(y)\n",
    "\n",
    "    #y3 = DropBlock1D(block_size=5,keep_prob=0.4)(y)\n",
    "    y3 = L.Dense(7, activation='softmax',name='consonant')(y)\n",
    "    \n",
    "    model = tf.keras.Model(x, [y1,y2,y3])\n",
    "\n",
    "    if tta:\n",
    "        assert False, 'This does not make sense yet'\n",
    "        x_flip = tf.reverse(x, [2])  # 'NHWC'\n",
    "        y_tta = tf.add(model(x), model(x_flip)) / 2.0\n",
    "        tta_model = tf.keras.Model(x, y_tta)\n",
    "        return model, tta_model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backbone xception and weights imagenet\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = get_model((160,256,3),'xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "imgs (InputLayer)               [(None, 160, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(None, 160, 256, 3) 0           imgs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv (TensorFlow [(None, 160, 256, 3) 0           tf_op_layer_sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "xception (Model)                (None, 5, 8, 2048)   20861480    tf_op_layer_truediv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           xception[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "grapheme (Dense)                (None, 168)          344232      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           22539       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            14343       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,242,594\n",
      "Trainable params: 21,188,066\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(img_batch, label_batch, batch_size):\n",
    "    # https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/cifar10-preact18-mixup.py\n",
    "    weight = tf.random.uniform([batch_size])\n",
    "    x_weight = tf.reshape(weight, [batch_size, 1, 1, 1])\n",
    "    y_weight = tf.reshape(weight, [batch_size, 1])\n",
    "    index = tf.random.shuffle(tf.range(batch_size, dtype=tf.int32))\n",
    "    x1, x2 = img_batch, tf.gather(img_batch, index)\n",
    "    img_batch = x1 * x_weight + x2 * (1. - x_weight)\n",
    "    y1, y2 = label_batch[0], tf.gather(label_batch[0], index)\n",
    "    label1_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "    y1, y2 = label_batch[1], tf.gather(label_batch[1], index)\n",
    "    label2_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "    y1, y2 = label_batch[2], tf.gather(label_batch[2], index)\n",
    "    label3_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "    return img_batch, (label1_batch, label2_batch, label3_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "    # Detect hardware, return appropriate distribution strategy\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    print('REPLICAS: ', strategy.num_replicas_in_sync)\n",
    "    return strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_concatenated(image, label1, label2, label3):\n",
    "    label = tf.concat([tf.one_hot(label1, 168),tf.one_hot(label2, 11),tf.one_hot(label3, 7)],-1)\n",
    "    return image, label\n",
    "\n",
    "def one_hot(image, label1, label2, label3):\n",
    "    label = (tf.one_hot(label1, 168),tf.one_hot(label2, 11),tf.one_hot(label3, 7))\n",
    "    return image, label\n",
    "\n",
    "def read_tfrecords(example, input_size):\n",
    "    features = {\n",
    "      'img': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'grapheme_root': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'vowel_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'consonant_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'unique_tuple': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "    img = tf.image.decode_image(example['img'])\n",
    "    img = tf.reshape(img, input_size + (1, ))\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # grayscale -> RGB\n",
    "    img = tf.repeat(img, 3, -1)\n",
    "\n",
    "    # image_id = tf.cast(example['image_id'], tf.int32)\n",
    "    grapheme_root = tf.cast(example['grapheme_root'], tf.int32)\n",
    "    vowel_diacritic = tf.cast(example['vowel_diacritic'], tf.int32)\n",
    "    consonant_diacritic = tf.cast(example['consonant_diacritic'], tf.int32)\n",
    "    # unique_tuple = tf.cast(example['unique_tuple'], tf.int32)\n",
    "    return img, grapheme_root,vowel_diacritic,consonant_diacritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRecall(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, val_ds):\n",
    "        self.best_score = 0\n",
    "        self.val_ds = val_ds\n",
    "        \n",
    "        self.val_pred_grapheme = []\n",
    "        self.val_pred_vowel = []\n",
    "        self.val_pred_cons = []\n",
    "        \n",
    "        self.val_targ_grapheme = []\n",
    "        self.val_targ_vowel = []\n",
    "        self.val_targ_cons = []\n",
    "        \n",
    "    def on_test_batch_end(self, batch, logs={}):\n",
    "        \n",
    "        print (batch)\n",
    "        \n",
    "        for batch_data in self.val_ds.take(batch+1):\n",
    "            pass\n",
    "        \n",
    "        val_predict = self.model.predict(batch_data[0])\n",
    "        val_targ = batch_data[1]\n",
    "        \n",
    "        self.val_pred_grapheme += val_predict[0].argmax(1).tolist()\n",
    "        self.val_pred_vowel += val_predict[1].argmax(1).tolist()\n",
    "        self.val_pred_cons += val_predict[2].argmax(1).tolist()\n",
    "        \n",
    "        self.val_targ_grapheme += val_targ[0].numpy().argmax(1).tolist()\n",
    "        self.val_targ_vowel += val_targ[1].numpy().argmax(1).tolist()\n",
    "        self.val_targ_cons += val_targ[2].numpy().argmax(1).tolist()\n",
    "    \n",
    "    def on_test_end(self, logs={}):\n",
    "        \n",
    "        recall_grapheme = recall_score(self.val_targ_grapheme, self.val_pred_grapheme, average='macro')\n",
    "        recall_vowel = recall_score(self.val_targ_vowel, self.val_pred_vowel, average='macro')\n",
    "        recall_cons = recall_score(self.val_targ_cons, self.val_pred_cons, average='macro')\n",
    "        \n",
    "        overall_recall = np.average([recall_grapheme,recall_vowel,recall_cons],weights=[.5,.25,.25])\n",
    "        \n",
    "        print (overall_recall, recall_grapheme, recall_vowel, recall_cons)\n",
    "        \n",
    "        if overall_recall > self.best_score:\n",
    "            self.best_score = overall_score\n",
    "            \n",
    "            weight_fn = 'model-%d.h5' % (self.best_score*100)\n",
    "            model.save_weights(weight_fn)\n",
    "            print(f'Saved weights to: {weight_fn}')\n",
    "            \n",
    "    '''\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        recall_grapheme = recall_score(self.val_targ_grapheme, self.val_pred_grapheme, average='macro')\n",
    "        recall_vowel = recall_score(self.val_targ_vowel, self.val_pred_vowel, average='macro')\n",
    "        recall_cons = recall_score(self.val_targ_cons, self.val_pred_cons, average='macro')\n",
    "        \n",
    "        overall_recall = np.average([recall_grapheme,recall_vowel,recall_cons],weights=[.5,.25,.25])\n",
    "        \n",
    "        print (overall_recall, recall_grapheme, recall_vowel, recall_cons)\n",
    "        \n",
    "        if overall_recall > self.best_score:\n",
    "            self.best_score = overall_score\n",
    "            \n",
    "            weight_fn = 'model-%d-%d.h5' % (epoch, self.best_score*100)\n",
    "            model.save_weights(weight_fn)\n",
    "            print(f'Saved weights to: {weight_fn}')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_old(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global parser, train_ds, val_ds, model, num_val_samples, val_step\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_id', type=int, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    parser.add_argument('--lr', type=float, default=2e-4)\n",
    "    parser.add_argument('--input_size', type=str, default='160,256')\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--backbone', type=str, default='xception')\n",
    "    parser.add_argument('--weights', type=str, default='imagenet')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    args.input_size = tuple(int(x) for x in args.input_size.split(','))\n",
    "    np.random.seed(args.seed)\n",
    "    tf.random.set_seed(args.seed)\n",
    "\n",
    "    # build the model\n",
    "    strategy = get_strategy()\n",
    "    with strategy.scope():\n",
    "        model = get_model(input_size=args.input_size + (3, ), backbone=args.backbone,\n",
    "            weights=args.weights)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=args.lr),\n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=[categorical_accuracy, tf.keras.metrics.Recall()]) #\n",
    "    \n",
    "    print(model.summary())\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    # create the training and validation datasets\n",
    "    ds_path = KaggleDatasets().get_gcs_path('tfrecords-grapheme-stratified') #KaggleDatasets().get_gcs_path('bengali-tfrecords-v010')\n",
    "    \n",
    "    train_fns = tf.io.gfile.glob(os.path.join(ds_path, 'train*.tfrec')) #tf.io.gfile.glob(os.path.join(ds_path, 'records/train*.tfrec'))\n",
    "    train_ds = tf.data.TFRecordDataset(train_fns, num_parallel_reads=AUTO)\n",
    "    train_ds = train_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "    train_ds = train_ds.repeat().batch(args.batch_size)\n",
    "    train_ds = train_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "    train_ds = train_ds.map(lambda a, b: mixup(a, b, args.batch_size), num_parallel_calls=AUTO)\n",
    "\n",
    "    val_fns = tf.io.gfile.glob(os.path.join(ds_path, 'val*.tfrec')) #tf.io.gfile.glob(os.path.join(ds_path, 'records/val*.tfrec'))\n",
    "    val_ds = tf.data.TFRecordDataset(val_fns, num_parallel_reads=AUTO)\n",
    "    val_ds = val_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "    val_ds = val_ds.batch(args.batch_size)\n",
    "    val_ds = val_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='val_grapheme_categorical_accuracy', mode='max', patience=5, verbose=1)\n",
    "    def scheduler(epoch):\n",
    "        if epoch < 4:\n",
    "            return args.lr\n",
    "        else:\n",
    "            return args.lr * tf.math.exp(0.2 * (3 - epoch))\n",
    "    callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "    \n",
    "    weight_fn = 'model-%04d.h5' % args.model_id\n",
    "    \n",
    "    callback3 = tf.keras.callbacks.ModelCheckpoint(monitor='val_grapheme_categorical_accuracy', mode='max', save_best_only=True, filepath=weight_fn, verbose=1) #CustomRecall(val_ds)\n",
    "    \n",
    "    # train\n",
    "    num_train_samples = sum(int(fn.split('_')[2]) for fn in train_fns)\n",
    "    num_val_samples = sum(int(fn.split('_')[2]) for fn in val_fns)\n",
    "    steps_per_epoch = num_train_samples // args.batch_size\n",
    "    val_step = num_val_samples // args.batch_size\n",
    "    \n",
    "    print(f'Training on {num_train_samples} samples. Each epochs requires {steps_per_epoch} steps')\n",
    "    print(f'Validation on {num_val_samples} samples.')\n",
    "    \n",
    "    h = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=args.epochs, verbose=1,\n",
    "      validation_data=val_ds,callbacks=[callback1,callback2,callback3])\n",
    "    \n",
    "    #print(h)\n",
    "    #weight_fn = 'model-%04d.h5' % args.model_id\n",
    "    #model.save_weights(weight_fn)\n",
    "    #print(f'Saved weights to: {weight_fn}')\n",
    "    \n",
    "    #model_effn.save_weights('model_effn.h5')\n",
    "    #model_pooled.save_weights('model_pooled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.0.0.2:8470']\n",
      "REPLICAS:  8\n",
      "Using backbone xception and weights imagenet\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "imgs (InputLayer)               [(None, 160, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_1 (TensorFlowOp [(None, 160, 256, 3) 0           imgs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_1 (TensorFl [(None, 160, 256, 3) 0           tf_op_layer_sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "xception (Model)                (None, 5, 8, 2048)   20861480    tf_op_layer_truediv_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           xception[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "grapheme (Dense)                (None, 168)          344232      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "vowel (Dense)                   (None, 11)           22539       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "consonant (Dense)               (None, 7)            14343       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,242,594\n",
      "Trainable params: 21,188,066\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Training on 160672 samples. Each epochs requires 1255 steps\n",
      "Validation on 40168 samples.\n",
      "Train for 1255 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 1/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 4.1847 - grapheme_loss: 2.5902 - vowel_loss: 0.9773 - consonant_loss: 0.5977 - grapheme_categorical_accuracy: 0.5857 - grapheme_recall: 0.1705 - vowel_categorical_accuracy: 0.8022 - vowel_recall: 0.3989 - consonant_categorical_accuracy: 0.8615 - consonant_recall: 0.5720\n",
      "Epoch 00001: val_grapheme_categorical_accuracy improved from -inf to 0.91336, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 204s 163ms/step - loss: 4.1840 - grapheme_loss: 2.5896 - vowel_loss: 0.9771 - consonant_loss: 0.5975 - grapheme_categorical_accuracy: 0.5859 - grapheme_recall: 0.1705 - vowel_categorical_accuracy: 0.8023 - vowel_recall: 0.3990 - consonant_categorical_accuracy: 0.8617 - consonant_recall: 0.5720 - val_loss: 0.5263 - val_grapheme_loss: 0.3291 - val_vowel_loss: 0.1066 - val_consonant_loss: 0.0945 - val_grapheme_categorical_accuracy: 0.9134 - val_grapheme_recall: 0.8857 - val_vowel_categorical_accuracy: 0.9751 - val_vowel_recall: 0.9733 - val_consonant_categorical_accuracy: 0.9785 - val_consonant_recall: 0.9761\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 2/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 2.9952 - grapheme_loss: 1.7604 - vowel_loss: 0.7791 - consonant_loss: 0.4702 - grapheme_categorical_accuracy: 0.7948 - grapheme_recall: 0.3116 - vowel_categorical_accuracy: 0.8607 - vowel_recall: 0.4591 - consonant_categorical_accuracy: 0.9060 - consonant_recall: 0.6061\n",
      "Epoch 00002: val_grapheme_categorical_accuracy improved from 0.91336 to 0.93846, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 141s 112ms/step - loss: 2.9949 - grapheme_loss: 1.7602 - vowel_loss: 0.7792 - consonant_loss: 0.4702 - grapheme_categorical_accuracy: 0.7949 - grapheme_recall: 0.3116 - vowel_categorical_accuracy: 0.8607 - vowel_recall: 0.4591 - consonant_categorical_accuracy: 0.9059 - consonant_recall: 0.6061 - val_loss: 0.4248 - val_grapheme_loss: 0.2561 - val_vowel_loss: 0.0895 - val_consonant_loss: 0.0775 - val_grapheme_categorical_accuracy: 0.9385 - val_grapheme_recall: 0.9197 - val_vowel_categorical_accuracy: 0.9833 - val_vowel_recall: 0.9811 - val_consonant_categorical_accuracy: 0.9807 - val_consonant_recall: 0.9801\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 3/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 2.6827 - grapheme_loss: 1.5248 - vowel_loss: 0.7021 - consonant_loss: 0.4437 - grapheme_categorical_accuracy: 0.8083 - grapheme_recall: 0.3435 - vowel_categorical_accuracy: 0.8695 - vowel_recall: 0.4761 - consonant_categorical_accuracy: 0.9137 - consonant_recall: 0.6076\n",
      "Epoch 00003: val_grapheme_categorical_accuracy improved from 0.93846 to 0.94364, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 141s 112ms/step - loss: 2.6826 - grapheme_loss: 1.5249 - vowel_loss: 0.7021 - consonant_loss: 0.4437 - grapheme_categorical_accuracy: 0.8083 - grapheme_recall: 0.3434 - vowel_categorical_accuracy: 0.8696 - vowel_recall: 0.4761 - consonant_categorical_accuracy: 0.9137 - consonant_recall: 0.6076 - val_loss: 0.3512 - val_grapheme_loss: 0.2148 - val_vowel_loss: 0.0742 - val_consonant_loss: 0.0599 - val_grapheme_categorical_accuracy: 0.9436 - val_grapheme_recall: 0.9345 - val_vowel_categorical_accuracy: 0.9821 - val_vowel_recall: 0.9809 - val_consonant_categorical_accuracy: 0.9843 - val_consonant_recall: 0.9835\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 4/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 2.4529 - grapheme_loss: 1.3694 - vowel_loss: 0.6579 - consonant_loss: 0.4249 - grapheme_categorical_accuracy: 0.8153 - grapheme_recall: 0.3640 - vowel_categorical_accuracy: 0.8815 - vowel_recall: 0.4834 - consonant_categorical_accuracy: 0.9182 - consonant_recall: 0.6114\n",
      "Epoch 00004: val_grapheme_categorical_accuracy improved from 0.94364 to 0.95021, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 139s 111ms/step - loss: 2.4528 - grapheme_loss: 1.3694 - vowel_loss: 0.6579 - consonant_loss: 0.4250 - grapheme_categorical_accuracy: 0.8153 - grapheme_recall: 0.3639 - vowel_categorical_accuracy: 0.8816 - vowel_recall: 0.4834 - consonant_categorical_accuracy: 0.9182 - consonant_recall: 0.6114 - val_loss: 0.3171 - val_grapheme_loss: 0.1938 - val_vowel_loss: 0.0704 - val_consonant_loss: 0.0535 - val_grapheme_categorical_accuracy: 0.9502 - val_grapheme_recall: 0.9454 - val_vowel_categorical_accuracy: 0.9841 - val_vowel_recall: 0.9837 - val_consonant_categorical_accuracy: 0.9863 - val_consonant_recall: 0.9861\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.00016374615, shape=(), dtype=float32).\n",
      "Epoch 5/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 2.2657 - grapheme_loss: 1.2296 - vowel_loss: 0.6145 - consonant_loss: 0.4049 - grapheme_categorical_accuracy: 0.8210 - grapheme_recall: 0.3823 - vowel_categorical_accuracy: 0.8860 - vowel_recall: 0.4945 - consonant_categorical_accuracy: 0.9225 - consonant_recall: 0.6153\n",
      "Epoch 00005: val_grapheme_categorical_accuracy improved from 0.95021 to 0.95359, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 139s 111ms/step - loss: 2.2657 - grapheme_loss: 1.2297 - vowel_loss: 0.6145 - consonant_loss: 0.4048 - grapheme_categorical_accuracy: 0.8210 - grapheme_recall: 0.3822 - vowel_categorical_accuracy: 0.8859 - vowel_recall: 0.4945 - consonant_categorical_accuracy: 0.9225 - consonant_recall: 0.6154 - val_loss: 0.2925 - val_grapheme_loss: 0.1766 - val_vowel_loss: 0.0624 - val_consonant_loss: 0.0474 - val_grapheme_categorical_accuracy: 0.9536 - val_grapheme_recall: 0.9478 - val_vowel_categorical_accuracy: 0.9871 - val_vowel_recall: 0.9867 - val_consonant_categorical_accuracy: 0.9898 - val_consonant_recall: 0.9896\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.000134064, shape=(), dtype=float32).\n",
      "Epoch 6/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 2.1315 - grapheme_loss: 1.1406 - vowel_loss: 0.5959 - consonant_loss: 0.3907 - grapheme_categorical_accuracy: 0.8231 - grapheme_recall: 0.3939 - vowel_categorical_accuracy: 0.8942 - vowel_recall: 0.4994 - consonant_categorical_accuracy: 0.9259 - consonant_recall: 0.6190\n",
      "Epoch 00006: val_grapheme_categorical_accuracy improved from 0.95359 to 0.95439, saving model to model-0000.h5\n",
      "1255/1255 [==============================] - 142s 113ms/step - loss: 2.1316 - grapheme_loss: 1.1407 - vowel_loss: 0.5959 - consonant_loss: 0.3908 - grapheme_categorical_accuracy: 0.8229 - grapheme_recall: 0.3938 - vowel_categorical_accuracy: 0.8941 - vowel_recall: 0.4993 - consonant_categorical_accuracy: 0.9257 - consonant_recall: 0.6189 - val_loss: 0.2834 - val_grapheme_loss: 0.1733 - val_vowel_loss: 0.0645 - val_consonant_loss: 0.0461 - val_grapheme_categorical_accuracy: 0.9544 - val_grapheme_recall: 0.9504 - val_vowel_categorical_accuracy: 0.9859 - val_vowel_recall: 0.9853 - val_consonant_categorical_accuracy: 0.9890 - val_consonant_recall: 0.9890\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.00010976232, shape=(), dtype=float32).\n",
      "Epoch 7/50\n",
      "1254/1255 [============================>.] - ETA: 0s - loss: 1.7037 - grapheme_loss: 0.8394 - vowel_loss: 0.5116 - consonant_loss: 0.3491 - grapheme_categorical_accuracy: 0.8519 - grapheme_recall: 0.4396 - vowel_categorical_accuracy: 0.9276 - vowel_recall: 0.5202 - consonant_categorical_accuracy: 0.9461 - consonant_recall: 0.6264\n",
      "Epoch 00021: val_grapheme_categorical_accuracy did not improve from 0.96196\n",
      "1255/1255 [==============================] - 137s 109ms/step - loss: 1.7037 - grapheme_loss: 0.8395 - vowel_loss: 0.5115 - consonant_loss: 0.3491 - grapheme_categorical_accuracy: 0.8517 - grapheme_recall: 0.4396 - vowel_categorical_accuracy: 0.9276 - vowel_recall: 0.5203 - consonant_categorical_accuracy: 0.9460 - consonant_recall: 0.6264 - val_loss: 0.2285 - val_grapheme_loss: 0.1389 - val_vowel_loss: 0.0522 - val_consonant_loss: 0.0347 - val_grapheme_categorical_accuracy: 0.9618 - val_grapheme_recall: 0.9592 - val_vowel_categorical_accuracy: 0.9875 - val_vowel_recall: 0.9869 - val_consonant_categorical_accuracy: 0.9914 - val_consonant_recall: 0.9912\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(5.464745e-06, shape=(), dtype=float32).\n",
      "Epoch 22/50\n",
      " 244/1255 [====>.........................] - ETA: 1:37 - loss: 1.7110 - grapheme_loss: 0.8397 - vowel_loss: 0.5140 - consonant_loss: 0.3564 - grapheme_categorical_accuracy: 0.8624 - grapheme_recall: 0.4379 - vowel_categorical_accuracy: 0.9334 - vowel_recall: 0.5211 - consonant_categorical_accuracy: 0.9424 - consonant_recall: 0.6254"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
