{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open('../data/train_14k_split_conll.txt','r',encoding='utf8')\n",
    "line_train = f_train.readlines()\n",
    "\n",
    "f_val = open('../data/dev_3k_split_conll.txt','r',encoding='utf8')\n",
    "line_val = f_val.readlines()\n",
    "\n",
    "f_test = open('../data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='utf8')\n",
    "line_test = f_test.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_doc_sentiment = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                sentiment = line.split('\\t')[2]\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_doc_sentiment.append(sentiment)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    train_df['sentiment'] = train_doc_sentiment\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data(line_train)\n",
    "val_df = get_data(line_val)\n",
    "test_df = get_data_test(line_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289253, 5), (62618, 5), (61735, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3000, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.uid.nunique(), val_df.uid.nunique(), test_df.uid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>uid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289248</th>\n",
       "      <td>14000</td>\n",
       "      <td>//</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289249</th>\n",
       "      <td>14000</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289250</th>\n",
       "      <td>14000</td>\n",
       "      <td>co</td>\n",
       "      <td>eng</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289251</th>\n",
       "      <td>14000</td>\n",
       "      <td>/</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289252</th>\n",
       "      <td>14000</td>\n",
       "      <td>cs3vtzop3q</td>\n",
       "      <td>eng</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_id        word word_type   uid sentiment\n",
       "289248   14000          //         o  3308   neutral\n",
       "289249   14000           .         o  3308   neutral\n",
       "289250   14000          co       eng  3308   neutral\n",
       "289251   14000           /         o  3308   neutral\n",
       "289252   14000  cs3vtzop3q       eng  3308   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281408, 5)\n",
      "(60908, 5)\n",
      "(60121, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.word != 'http']\n",
    "train_df = train_df[train_df.word != 'https']\n",
    "#train_df = train_df[train_df.word_type != 'o']\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df = val_df[val_df.word != 'http']\n",
    "val_df = val_df[val_df.word != 'https']\n",
    "#val_df = val_df[val_df.word_type != 'o']\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df = test_df[test_df.word != 'http']\n",
    "test_df = test_df[test_df.word != 'https']\n",
    "#test_df = test_df[test_df.word_type != 'o']\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\ntrain_df = train_df[train_df.word.str.len() >= 3]\\nprint (train_df.shape)\\n\\nval_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\nval_df = val_df[val_df.word.str.len() >= 3]\\nprint (val_df.shape)\\n\\ntest_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\ntest_df = test_df[test_df.word.str.len() >= 3]\\nprint (test_df.shape)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "train_df = train_df[train_df.word.str.len() >= 3]\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "val_df = val_df[val_df.word.str.len() >= 3]\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "test_df = test_df[test_df.word.str.len() >= 3]\n",
    "print (test_df.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "train_texts = pd.merge(train_texts,train_df[['uid','sentiment']],how='left').drop_duplicates().reset_index(drop=True)\n",
    "train_texts.columns = ['uid','text','sentiment']\n",
    "\n",
    "val_texts = val_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "val_texts = pd.merge(val_texts,val_df[['uid','sentiment']],how='left').drop_duplicates()\n",
    "val_texts.columns = ['uid','text','sentiment']\n",
    "\n",
    "test_texts = test_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "test_texts.columns = ['uid','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14000, 3), (3000, 3), (3000, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>rt love looks good maddie !!! ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ min _ _ lyching @ manakgupta mein kahna nae ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>best luck sir world cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>yes . great dialogues one . also chupke chupke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ tarekfatah tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>desh bhakti baat wahi samajh sakte hai jo khud...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>madarchod mulle ye mathura nahi dikha tha jab ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>manya pradhan mantri mahoday shriman narendra ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>_ nsharif kiya tum apne baap ki oulad nahi .??...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>@ youtube tu aa rha h ki nhi wo modi phir se p...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment\n",
       "0    8  rt love looks good maddie !!! ako lang ba yung...   neutral\n",
       "1   14  @ min _ _ lyching @ manakgupta mein kahna nae ...   neutral\n",
       "2   26  best luck sir world cup ke liye bhot bhot subh...  positive\n",
       "3   27  yes . great dialogues one . also chupke chupke...  positive\n",
       "4   33  @ tarekfatah tu tere baap ke liye jo bola wo k...  negative\n",
       "5   38  desh bhakti baat wahi samajh sakte hai jo khud...  negative\n",
       "6   41  madarchod mulle ye mathura nahi dikha tha jab ...  negative\n",
       "7   48  manya pradhan mantri mahoday shriman narendra ...  positive\n",
       "8   56  _ nsharif kiya tum apne baap ki oulad nahi .??...  negative\n",
       "9   59  @ youtube tu aa rha h ki nhi wo modi phir se p...   neutral"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ka ghra tauq pakistan israel ko tasle...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12</td>\n",
       "      <td>ye ye ..... ye ??????? gonna start another jun...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>23</td>\n",
       "      <td>~ caring . ~ bohot jyada caring . ~ courier wa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>24</td>\n",
       "      <td>@ sarfaraza _ 54 nonesense ... kabhi baymani p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team ne 105 % effort ki aagey allah ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>61</td>\n",
       "      <td>@ ankita _ shah8 bhadve ye photo elections se ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>64</td>\n",
       "      <td>_ krishna jcb full trend chal rahi aa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>77</td>\n",
       "      <td>_ sharma1 @ kavita _ sharma4 sunita ji kal jab...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>101</td>\n",
       "      <td>_ nolo weni ankere gae weekend 😂😂😂😂😂</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment\n",
       "0      3  pakistan ka ghra tauq pakistan israel ko tasle...  negative\n",
       "16    12  ye ye ..... ye ??????? gonna start another jun...   neutral\n",
       "35    23  ~ caring . ~ bohot jyada caring . ~ courier wa...  negative\n",
       "56    24  @ sarfaraza _ 54 nonesense ... kabhi baymani p...  positive\n",
       "82    45  pakistani team ne 105 % effort ki aagey allah ...  positive\n",
       "102   61  @ ankita _ shah8 bhadve ye photo elections se ...  negative\n",
       "118   64              _ krishna jcb full trend chal rahi aa  positive\n",
       "126   77  _ sharma1 @ kavita _ sharma4 sunita ji kal jab...  negative\n",
       "148   81  accha kiya invite nai kiya corrupted party ind...  negative\n",
       "161  101               _ nolo weni ankere gae weekend 😂😂😂😂😂  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe ho jaise pakistan wale ni karte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>rt anu's prerna way ran saving ... ram ram jay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai ye pathan nae warna # ptm nay pr b ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision . much option arm . big sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>rt keep saying kenyan rugby beautiful save kru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>guru g give bless sewa simrn parmarth good dee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>136</td>\n",
       "      <td>ðÿ˜¡ tum logo ne zindagi mai kabhi india ka ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>139</td>\n",
       "      <td>_ itne hi gandi g priye h aapke profile pic mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>158</td>\n",
       "      <td>... google translate working .... aisi jagah n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>qaira sahb u r great man apka beta nhayat nfee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text\n",
       "0    1  keh aese rahe ho jaise pakistan wale ni karte ...\n",
       "1    4  rt anu's prerna way ran saving ... ram ram jay...\n",
       "2   17  shukar hai ye pathan nae warna # ptm nay pr b ...\n",
       "3   18  harsh pen decision . much option arm . big sec...\n",
       "4   34  rt keep saying kenyan rugby beautiful save kru...\n",
       "5   43  guru g give bless sewa simrn parmarth good dee...\n",
       "6  136  ðÿ˜¡ tum logo ne zindagi mai kabhi india ka ma...\n",
       "7  139  _ itne hi gandi g priye h aapke profile pic mo...\n",
       "8  158  ... google translate working .... aisi jagah n...\n",
       "9  162  qaira sahb u r great man apka beta nhayat nfee..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        98.461000\n",
       "std         26.943002\n",
       "min          5.000000\n",
       "25%         81.000000\n",
       "50%        105.000000\n",
       "75%        120.000000\n",
       "max        154.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean       99.303667\n",
       "std        26.502722\n",
       "min        16.000000\n",
       "25%        82.750000\n",
       "50%       105.000000\n",
       "75%       120.000000\n",
       "max       144.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean      101.097000\n",
       "std        27.838186\n",
       "min        15.000000\n",
       "25%        84.000000\n",
       "50%       107.000000\n",
       "75%       123.000000\n",
       "max       187.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(\" \".join(train_texts.text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii+1 for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "train_sentences = []\n",
    "val_sentences = []\n",
    "test_sentences = []\n",
    "\n",
    "for i, text in enumerate(train_texts.text):\n",
    "    train_sentences.append([char2int[ch] if ch in char2int else 0 for ch in text])\n",
    "    \n",
    "for i, text in enumerate(val_texts.text):\n",
    "    val_sentences.append([char2int[ch] if ch in char2int else 0 for ch in text])\n",
    "    \n",
    "for i, text in enumerate(test_texts.text):\n",
    "    test_sentences.append([char2int[ch] if ch in char2int else 0 for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ਾ': 1,\n",
       " '🛍': 2,\n",
       " '😩': 3,\n",
       " '<': 4,\n",
       " '#': 5,\n",
       " 'd': 6,\n",
       " '🎥': 7,\n",
       " '🤐': 8,\n",
       " '💟': 9,\n",
       " 'm': 10,\n",
       " '🥰': 11,\n",
       " 'ş': 12,\n",
       " 'و': 13,\n",
       " '🤜': 14,\n",
       " '💸': 15,\n",
       " '?': 16,\n",
       " 'u': 17,\n",
       " '“': 18,\n",
       " '🌵': 19,\n",
       " '😑': 20,\n",
       " '😻': 21,\n",
       " '►': 22,\n",
       " '🐖': 23,\n",
       " '께': 24,\n",
       " '😱': 25,\n",
       " 'ส': 26,\n",
       " '🌹': 27,\n",
       " 'ス': 28,\n",
       " 'д': 29,\n",
       " '😞': 30,\n",
       " '다': 31,\n",
       " '当': 32,\n",
       " '🔐': 33,\n",
       " 'ʳ': 34,\n",
       " '💁': 35,\n",
       " 'ब': 36,\n",
       " '😹': 37,\n",
       " '𝑔': 38,\n",
       " '🙌': 39,\n",
       " 'ē': 40,\n",
       " '❌': 41,\n",
       " '🍰': 42,\n",
       " '🍃': 43,\n",
       " 'g': 44,\n",
       " 'ए': 45,\n",
       " '😰': 46,\n",
       " '🍱': 47,\n",
       " '👍': 48,\n",
       " '✅': 49,\n",
       " '한': 50,\n",
       " '❣': 51,\n",
       " '╭': 52,\n",
       " '์': 53,\n",
       " '❔': 54,\n",
       " '📃': 55,\n",
       " '𝓇': 56,\n",
       " '誕': 57,\n",
       " '⚫': 58,\n",
       " '^': 59,\n",
       " '🥭': 60,\n",
       " 'ล': 61,\n",
       " 'จ': 62,\n",
       " '𝓊': 63,\n",
       " '”': 64,\n",
       " '스': 65,\n",
       " '🍎': 66,\n",
       " '👿': 67,\n",
       " '💅': 68,\n",
       " 'ꕊ': 69,\n",
       " '🔁': 70,\n",
       " 'ủ': 71,\n",
       " '🐷': 72,\n",
       " 'ę': 73,\n",
       " '🛐': 74,\n",
       " '🌴': 75,\n",
       " '🌏': 76,\n",
       " 'ూ': 77,\n",
       " '🤕': 78,\n",
       " '🕌': 79,\n",
       " 'ั': 80,\n",
       " 'ఇ': 81,\n",
       " 'দ': 82,\n",
       " '🇧': 83,\n",
       " '🎀': 84,\n",
       " 'h': 85,\n",
       " '🥃': 86,\n",
       " 'а': 87,\n",
       " 'ㅠ': 88,\n",
       " '😭': 89,\n",
       " '🍏': 90,\n",
       " '😔': 91,\n",
       " '😳': 92,\n",
       " '♂': 93,\n",
       " '🇸': 94,\n",
       " 'l': 95,\n",
       " 'ु': 96,\n",
       " '💋': 97,\n",
       " 's': 98,\n",
       " '✨': 99,\n",
       " '실': 100,\n",
       " '티': 101,\n",
       " '°': 102,\n",
       " '𝐬': 103,\n",
       " 'ğ': 104,\n",
       " 'f': 105,\n",
       " '황': 106,\n",
       " 'パ': 107,\n",
       " '⚕': 108,\n",
       " '☹': 109,\n",
       " '머': 110,\n",
       " 'ᵈ': 111,\n",
       " 'س': 112,\n",
       " 'น': 113,\n",
       " '🤦': 114,\n",
       " '🙉': 115,\n",
       " 'イ': 116,\n",
       " 'द': 117,\n",
       " '📻': 118,\n",
       " '🥳': 119,\n",
       " 'ᶦ': 120,\n",
       " 'ư': 121,\n",
       " '\\U000e0065': 122,\n",
       " 'ア': 123,\n",
       " '🙂': 124,\n",
       " 'ة': 125,\n",
       " 'っ': 126,\n",
       " '니': 127,\n",
       " '✴': 128,\n",
       " '🎶': 129,\n",
       " '😉': 130,\n",
       " 'भ': 131,\n",
       " '𝐢': 132,\n",
       " '↑': 133,\n",
       " '•': 134,\n",
       " 'ɣ': 135,\n",
       " 'r': 136,\n",
       " '🏾': 137,\n",
       " '🎧': 138,\n",
       " '𝒾': 139,\n",
       " '.': 140,\n",
       " '₹': 141,\n",
       " '뷰': 142,\n",
       " '🙋': 143,\n",
       " '멀': 144,\n",
       " '่': 145,\n",
       " 'ा': 146,\n",
       " '>': 147,\n",
       " '్': 148,\n",
       " 'る': 149,\n",
       " '🔛': 150,\n",
       " 'เ': 151,\n",
       " '🐗': 152,\n",
       " 'ズ': 153,\n",
       " '𝟔': 154,\n",
       " '𝐫': 155,\n",
       " '╮': 156,\n",
       " 'ř': 157,\n",
       " '🎟': 158,\n",
       " '🤮': 159,\n",
       " '⬇': 160,\n",
       " 'ค': 161,\n",
       " '💛': 162,\n",
       " 'コ': 163,\n",
       " '🎇': 164,\n",
       " '🤟': 165,\n",
       " 'i': 166,\n",
       " 'క': 167,\n",
       " 'ห': 168,\n",
       " 'ۃ': 169,\n",
       " 'ہ': 170,\n",
       " '🌊': 171,\n",
       " '👊': 172,\n",
       " '😖': 173,\n",
       " 'ò': 174,\n",
       " '🎤': 175,\n",
       " '🤭': 176,\n",
       " 'で': 177,\n",
       " 'о': 178,\n",
       " 'ਤ': 179,\n",
       " '🚶': 180,\n",
       " 'р': 181,\n",
       " 'ੀ': 182,\n",
       " '📿': 183,\n",
       " '💓': 184,\n",
       " 'ベ': 185,\n",
       " '💔': 186,\n",
       " \"'\": 187,\n",
       " 'ร': 188,\n",
       " '˘': 189,\n",
       " '🎓': 190,\n",
       " '\\U000e007f': 191,\n",
       " '😋': 192,\n",
       " 'อ': 193,\n",
       " '😣': 194,\n",
       " '제': 195,\n",
       " '🥧': 196,\n",
       " '국': 197,\n",
       " 'լ': 198,\n",
       " '\\u200d': 199,\n",
       " '𝐂': 200,\n",
       " 'ú': 201,\n",
       " 'と': 202,\n",
       " 'ｔ': 203,\n",
       " '🤚': 204,\n",
       " 'ป': 205,\n",
       " 'ı': 206,\n",
       " '🔞': 207,\n",
       " 'ô': 208,\n",
       " '𝄞': 209,\n",
       " '`': 210,\n",
       " 'आ': 211,\n",
       " 'ɩ': 212,\n",
       " 'ｅ': 213,\n",
       " '🙃': 214,\n",
       " '만': 215,\n",
       " '☀': 216,\n",
       " 'ｓ': 217,\n",
       " 'δ': 218,\n",
       " '💞': 219,\n",
       " 'は': 220,\n",
       " 't': 221,\n",
       " ' ': 222,\n",
       " '😅': 223,\n",
       " '🖐': 224,\n",
       " '❄': 225,\n",
       " '🥺': 226,\n",
       " '🔪': 227,\n",
       " 'ే': 228,\n",
       " '🌸': 229,\n",
       " '|': 230,\n",
       " 'o': 231,\n",
       " '🐶': 232,\n",
       " '🍬': 233,\n",
       " 'د': 234,\n",
       " 'र': 235,\n",
       " '🏖': 236,\n",
       " 'ế': 237,\n",
       " '\\xa0': 238,\n",
       " '👨': 239,\n",
       " 'ق': 240,\n",
       " 'č': 241,\n",
       " 'ย': 242,\n",
       " '😫': 243,\n",
       " '🐈': 244,\n",
       " '𝟐': 245,\n",
       " '♡': 246,\n",
       " '🎋': 247,\n",
       " '😲': 248,\n",
       " 'ạ': 249,\n",
       " '🏏': 250,\n",
       " 'の': 251,\n",
       " '👀': 252,\n",
       " '🤪': 253,\n",
       " '\\U000e0067': 254,\n",
       " '👋': 255,\n",
       " '🇵': 256,\n",
       " 'ผ': 257,\n",
       " '日': 258,\n",
       " '🧚': 259,\n",
       " 'చ': 260,\n",
       " '、': 261,\n",
       " 'వ': 262,\n",
       " '😐': 263,\n",
       " '🍫': 264,\n",
       " '✊': 265,\n",
       " '❗': 266,\n",
       " '🐂': 267,\n",
       " '¥': 268,\n",
       " '💡': 269,\n",
       " 'ష': 270,\n",
       " 'ç': 271,\n",
       " '👉': 272,\n",
       " '⬆': 273,\n",
       " '💚': 274,\n",
       " '☪': 275,\n",
       " 'ड': 276,\n",
       " '와': 277,\n",
       " '👅': 278,\n",
       " 'ि': 279,\n",
       " '🎩': 280,\n",
       " '🔚': 281,\n",
       " '̩': 282,\n",
       " 'す': 283,\n",
       " 'ᵍ': 284,\n",
       " '😓': 285,\n",
       " 'ਸ': 286,\n",
       " '🤞': 287,\n",
       " '💩': 288,\n",
       " '2': 289,\n",
       " '👑': 290,\n",
       " '🌲': 291,\n",
       " '👼': 292,\n",
       " '💏': 293,\n",
       " '😌': 294,\n",
       " 'ů': 295,\n",
       " 'ै': 296,\n",
       " '🔨': 297,\n",
       " 'झ': 298,\n",
       " '🤷': 299,\n",
       " '🤨': 300,\n",
       " 'র': 301,\n",
       " 'が': 302,\n",
       " '8': 303,\n",
       " 'ᵘ': 304,\n",
       " '🌚': 305,\n",
       " '👰': 306,\n",
       " '🍂': 307,\n",
       " 'ู': 308,\n",
       " '🎹': 309,\n",
       " 'ś': 310,\n",
       " '😏': 311,\n",
       " '😃': 312,\n",
       " '📝': 313,\n",
       " '¯': 314,\n",
       " '🗣': 315,\n",
       " '⃑': 316,\n",
       " 'ジ': 317,\n",
       " '😄': 318,\n",
       " 'z': 319,\n",
       " '🕯': 320,\n",
       " '限': 321,\n",
       " '😁': 322,\n",
       " '팀': 323,\n",
       " '💖': 324,\n",
       " 'ธ': 325,\n",
       " 'ビ': 326,\n",
       " '[': 327,\n",
       " 'ي': 328,\n",
       " '🤗': 329,\n",
       " '⭐': 330,\n",
       " '~': 331,\n",
       " 'ｕ': 332,\n",
       " 'ल': 333,\n",
       " '़': 334,\n",
       " 'औ': 335,\n",
       " '⚔': 336,\n",
       " '🧸': 337,\n",
       " '🐿': 338,\n",
       " 'ు': 339,\n",
       " '🎵': 340,\n",
       " '🇩': 341,\n",
       " 'य': 342,\n",
       " 'ネ': 343,\n",
       " '📷': 344,\n",
       " 'ण': 345,\n",
       " '➕': 346,\n",
       " 'º': 347,\n",
       " '🤬': 348,\n",
       " '放': 349,\n",
       " 'ੁ': 350,\n",
       " '시': 351,\n",
       " '생': 352,\n",
       " '🙇': 353,\n",
       " '🔝': 354,\n",
       " 'ẽ': 355,\n",
       " '🇮': 356,\n",
       " 'ి': 357,\n",
       " '🙈': 358,\n",
       " 'j': 359,\n",
       " '𝐩': 360,\n",
       " 'リ': 361,\n",
       " '🏿': 362,\n",
       " '🐼': 363,\n",
       " 'ถ': 364,\n",
       " '🇷': 365,\n",
       " 'ম': 366,\n",
       " '🌌': 367,\n",
       " '⤴': 368,\n",
       " 'đ': 369,\n",
       " 'ం': 370,\n",
       " '𝒻': 371,\n",
       " '𝐮': 372,\n",
       " '🐓': 373,\n",
       " '😷': 374,\n",
       " '\\u2066': 375,\n",
       " '4': 376,\n",
       " '$': 377,\n",
       " 'ا': 378,\n",
       " '€': 379,\n",
       " '🚬': 380,\n",
       " '😗': 381,\n",
       " 'ì': 382,\n",
       " '🌶': 383,\n",
       " '̇': 384,\n",
       " '✌': 385,\n",
       " 'ద': 386,\n",
       " 'ల': 387,\n",
       " 'w': 388,\n",
       " 'ः': 389,\n",
       " 'ョ': 390,\n",
       " '✔': 391,\n",
       " '🇰': 392,\n",
       " '😜': 393,\n",
       " '}': 394,\n",
       " '💫': 395,\n",
       " '🏼': 396,\n",
       " '🤓': 397,\n",
       " '🤙': 398,\n",
       " '🐯': 399,\n",
       " '🤠': 400,\n",
       " 'ठ': 401,\n",
       " '🏆': 402,\n",
       " '🙆': 403,\n",
       " 'ิ': 404,\n",
       " 'î': 405,\n",
       " 'ต': 406,\n",
       " 'å': 407,\n",
       " '🖕': 408,\n",
       " 'ใ': 409,\n",
       " '🙏': 410,\n",
       " '중': 411,\n",
       " 'ᵗ': 412,\n",
       " '℅': 413,\n",
       " '😿': 414,\n",
       " '🤔': 415,\n",
       " 'े': 416,\n",
       " '💑': 417,\n",
       " '―': 418,\n",
       " '🇾': 419,\n",
       " 'こ': 420,\n",
       " '👫': 421,\n",
       " '🎁': 422,\n",
       " '🏻': 423,\n",
       " 'ौ': 424,\n",
       " 'ダ': 425,\n",
       " 'ด': 426,\n",
       " '💘': 427,\n",
       " '‼': 428,\n",
       " 'م': 429,\n",
       " '🌍': 430,\n",
       " '🙄': 431,\n",
       " '🤝': 432,\n",
       " '😀': 433,\n",
       " '¡': 434,\n",
       " '는': 435,\n",
       " '😝': 436,\n",
       " 'ā': 437,\n",
       " '💃': 438,\n",
       " 'छ': 439,\n",
       " 'y': 440,\n",
       " 'ਨ': 441,\n",
       " '🎬': 442,\n",
       " '🏡': 443,\n",
       " 'ｎ': 444,\n",
       " '🥵': 445,\n",
       " '🥾': 446,\n",
       " 'ह': 447,\n",
       " 'া': 448,\n",
       " 'ě': 449,\n",
       " 'ク': 450,\n",
       " 'क': 451,\n",
       " 'خ': 452,\n",
       " '🤣': 453,\n",
       " 'ے': 454,\n",
       " '🍧': 455,\n",
       " '💮': 456,\n",
       " '립': 457,\n",
       " 'n': 458,\n",
       " '%': 459,\n",
       " 'แ': 460,\n",
       " 'ˢ': 461,\n",
       " 'ध': 462,\n",
       " 'ਗ': 463,\n",
       " 'ฟ': 464,\n",
       " '💣': 465,\n",
       " 'ö': 466,\n",
       " '♀': 467,\n",
       " '🌷': 468,\n",
       " 'ว': 469,\n",
       " '।': 470,\n",
       " '🐄': 471,\n",
       " '猫': 472,\n",
       " '生': 473,\n",
       " '😚': 474,\n",
       " '้': 475,\n",
       " 'ᵏ': 476,\n",
       " '😈': 477,\n",
       " '🎂': 478,\n",
       " '£': 479,\n",
       " '–': 480,\n",
       " '🌱': 481,\n",
       " '〣': 482,\n",
       " '_': 483,\n",
       " 'ê': 484,\n",
       " '²': 485,\n",
       " '🐱': 486,\n",
       " 'ź': 487,\n",
       " '×': 488,\n",
       " 'к': 489,\n",
       " '터': 490,\n",
       " 'b': 491,\n",
       " '🌎': 492,\n",
       " 'ో': 493,\n",
       " '(': 494,\n",
       " '😘': 495,\n",
       " 'म': 496,\n",
       " '☔': 497,\n",
       " '🎻': 498,\n",
       " '😟': 499,\n",
       " 'ਿ': 500,\n",
       " 'ृ': 501,\n",
       " 'ر': 502,\n",
       " '즐': 503,\n",
       " '🤲': 504,\n",
       " '🍦': 505,\n",
       " '3': 506,\n",
       " 'బ': 507,\n",
       " 'т': 508,\n",
       " '\\U000e0062': 509,\n",
       " '😼': 510,\n",
       " '▂': 511,\n",
       " '🙊': 512,\n",
       " 'ɲ': 513,\n",
       " '🏴': 514,\n",
       " '💗': 515,\n",
       " 'ʸ': 516,\n",
       " '=': 517,\n",
       " '👐': 518,\n",
       " 'ں': 519,\n",
       " '🦍': 520,\n",
       " 'త': 521,\n",
       " '*': 522,\n",
       " '🚨': 523,\n",
       " '+': 524,\n",
       " '︎': 525,\n",
       " '🍾': 526,\n",
       " '🍇': 527,\n",
       " '😕': 528,\n",
       " '🌺': 529,\n",
       " '✈': 530,\n",
       " '🇨': 531,\n",
       " 'ర': 532,\n",
       " 'ো': 533,\n",
       " '𝐚': 534,\n",
       " '🦄': 535,\n",
       " 'च': 536,\n",
       " '🇬': 537,\n",
       " 'अ': 538,\n",
       " '📄': 539,\n",
       " '\\u200b': 540,\n",
       " '😨': 541,\n",
       " '𝓉': 542,\n",
       " '🤧': 543,\n",
       " '👹': 544,\n",
       " 'ヒ': 545,\n",
       " 'ó': 546,\n",
       " 'ऑ': 547,\n",
       " '✉': 548,\n",
       " '定': 549,\n",
       " '्': 550,\n",
       " 'ی': 551,\n",
       " '😠': 552,\n",
       " 'ँ': 553,\n",
       " '🔴': 554,\n",
       " '💀': 555,\n",
       " 'ਕ': 556,\n",
       " 'ट': 557,\n",
       " '—': 558,\n",
       " '𝐞': 559,\n",
       " '💐': 560,\n",
       " '😆': 561,\n",
       " 'ī': 562,\n",
       " 'บ': 563,\n",
       " '-': 564,\n",
       " '⁉': 565,\n",
       " 'ज': 566,\n",
       " 'ý': 567,\n",
       " 'à': 568,\n",
       " '자': 569,\n",
       " '🙀': 570,\n",
       " '⇒': 571,\n",
       " ']': 572,\n",
       " '😊': 573,\n",
       " 'ص': 574,\n",
       " 'ข': 575,\n",
       " 'š': 576,\n",
       " 'べ': 577,\n",
       " '💬': 578,\n",
       " 'ố': 579,\n",
       " 'ॉ': 580,\n",
       " '@': 581,\n",
       " '❇': 582,\n",
       " '✍': 583,\n",
       " '😡': 584,\n",
       " '🇿': 585,\n",
       " 'ʰ': 586,\n",
       " 'ม': 587,\n",
       " '✋': 588,\n",
       " '{': 589,\n",
       " '💙': 590,\n",
       " '🌼': 591,\n",
       " '😇': 592,\n",
       " 'ỉ': 593,\n",
       " '🎈': 594,\n",
       " '🇫': 595,\n",
       " '👦': 596,\n",
       " '🐮': 597,\n",
       " '🦅': 598,\n",
       " 'a': 599,\n",
       " '…': 600,\n",
       " '𝐧': 601,\n",
       " 'ー': 602,\n",
       " '💕': 603,\n",
       " 'c': 604,\n",
       " 'ﷻ': 605,\n",
       " '🔷': 606,\n",
       " '😶': 607,\n",
       " 'พ': 608,\n",
       " 'ई': 609,\n",
       " 'ね': 610,\n",
       " '6': 611,\n",
       " 'न': 612,\n",
       " '👭': 613,\n",
       " '🗿': 614,\n",
       " 'ท': 615,\n",
       " '🥴': 616,\n",
       " 'ੋ': 617,\n",
       " 'त': 618,\n",
       " '🐕': 619,\n",
       " '🇱': 620,\n",
       " '🏵': 621,\n",
       " '러': 622,\n",
       " 'ੇ': 623,\n",
       " 'て': 624,\n",
       " 'ồ': 625,\n",
       " '🔶': 626,\n",
       " '👌': 627,\n",
       " '🎼': 628,\n",
       " '·': 629,\n",
       " 'ᵉ': 630,\n",
       " 'स': 631,\n",
       " ';': 632,\n",
       " '😬': 633,\n",
       " 'x': 634,\n",
       " 'ت': 635,\n",
       " '🤑': 636,\n",
       " 'ব': 637,\n",
       " '0': 638,\n",
       " 'ム': 639,\n",
       " '👃': 640,\n",
       " '️': 641,\n",
       " '❓': 642,\n",
       " '🚴': 643,\n",
       " '©': 644,\n",
       " '🏽': 645,\n",
       " 'ं': 646,\n",
       " '🎊': 647,\n",
       " '🔫': 648,\n",
       " '👟': 649,\n",
       " 'ᵒ': 650,\n",
       " 'є': 651,\n",
       " '🦉': 652,\n",
       " '5': 653,\n",
       " 'ć': 654,\n",
       " 'ト': 655,\n",
       " '☝': 656,\n",
       " '👞': 657,\n",
       " 'í': 658,\n",
       " '🍨': 659,\n",
       " '🤤': 660,\n",
       " '𝐖': 661,\n",
       " '😎': 662,\n",
       " '👆': 663,\n",
       " 'ధ': 664,\n",
       " '👠': 665,\n",
       " '9': 666,\n",
       " '𝒽': 667,\n",
       " '\\u2069': 668,\n",
       " 'p': 669,\n",
       " '🕺': 670,\n",
       " '🚩': 671,\n",
       " '🦳': 672,\n",
       " 'ờ': 673,\n",
       " '⛽': 674,\n",
       " '💝': 675,\n",
       " '💯': 676,\n",
       " '🌻': 677,\n",
       " 'इ': 678,\n",
       " 'ン': 679,\n",
       " '🖤': 680,\n",
       " 'k': 681,\n",
       " '😦': 682,\n",
       " '은': 683,\n",
       " '🌟': 684,\n",
       " '🤩': 685,\n",
       " '𝓈': 686,\n",
       " '‘': 687,\n",
       " '🧁': 688,\n",
       " 'ň': 689,\n",
       " 'ื': 690,\n",
       " '♥': 691,\n",
       " '👇': 692,\n",
       " 'ณ': 693,\n",
       " '/': 694,\n",
       " '⃣': 695,\n",
       " 'ึ': 696,\n",
       " 'に': 697,\n",
       " 'ᵛ': 698,\n",
       " 'श': 699,\n",
       " '👎': 700,\n",
       " '함': 701,\n",
       " '🙅': 702,\n",
       " 'ক': 703,\n",
       " 'ष': 704,\n",
       " '👩': 705,\n",
       " '✖': 706,\n",
       " 'ع': 707,\n",
       " '❤': 708,\n",
       " 'प': 709,\n",
       " '🐍': 710,\n",
       " '😮': 711,\n",
       " '1': 712,\n",
       " 'ख': 713,\n",
       " 'ो': 714,\n",
       " '💪': 715,\n",
       " '👶': 716,\n",
       " '😒': 717,\n",
       " '𝐭': 718,\n",
       " '𝟏': 719,\n",
       " 'ू': 720,\n",
       " '\"': 721,\n",
       " '😍': 722,\n",
       " 'e': 723,\n",
       " 'న': 724,\n",
       " 'డ': 725,\n",
       " '일': 726,\n",
       " '⌚': 727,\n",
       " '💜': 728,\n",
       " '🔔': 729,\n",
       " '🥂': 730,\n",
       " '😵': 731,\n",
       " '⠀': 732,\n",
       " '𝐹': 733,\n",
       " 'ą': 734,\n",
       " '🏳': 735,\n",
       " 'μ': 736,\n",
       " 'ᵃ': 737,\n",
       " '👈': 738,\n",
       " '™': 739,\n",
       " '👸': 740,\n",
       " '🌿': 741,\n",
       " '😤': 742,\n",
       " 'ี': 743,\n",
       " 'ش': 744,\n",
       " '👺': 745,\n",
       " '🇭': 746,\n",
       " 'ل': 747,\n",
       " '💥': 748,\n",
       " 'ఉ': 749,\n",
       " '🇦': 750,\n",
       " 'ã': 751,\n",
       " '🏁': 752,\n",
       " 'అ': 753,\n",
       " '👾': 754,\n",
       " '🌄': 755,\n",
       " 'స': 756,\n",
       " '➡': 757,\n",
       " '😛': 758,\n",
       " 'ਫ': 759,\n",
       " 'ी': 760,\n",
       " 'い': 761,\n",
       " '🦋': 762,\n",
       " '®': 763,\n",
       " '🐒': 764,\n",
       " '🌳': 765,\n",
       " '👧': 766,\n",
       " '😴': 767,\n",
       " 'ᵐ': 768,\n",
       " 'ル': 769,\n",
       " '😂': 770,\n",
       " 'ਣ': 771,\n",
       " 'ঈ': 772,\n",
       " '7': 773,\n",
       " '😙': 774,\n",
       " '🧡': 775,\n",
       " '댄': 776,\n",
       " '🍟': 777,\n",
       " 'ా': 778,\n",
       " '🤯': 779,\n",
       " '&': 780,\n",
       " 'ü': 781,\n",
       " '🐥': 782,\n",
       " 'ñ': 783,\n",
       " '!': 784,\n",
       " 'м': 785,\n",
       " 'ਰ': 786,\n",
       " 'ż': 787,\n",
       " '행': 788,\n",
       " 'ề': 789,\n",
       " '🍍': 790,\n",
       " '💌': 791,\n",
       " 'ไ': 792,\n",
       " '약': 793,\n",
       " 'ప': 794,\n",
       " 'つ': 795,\n",
       " '’': 796,\n",
       " '🇪': 797,\n",
       " ')': 798,\n",
       " '🏋': 799,\n",
       " 'మ': 800,\n",
       " 'व': 801,\n",
       " 'ł': 802,\n",
       " '🌝': 803,\n",
       " 'v': 804,\n",
       " 'á': 805,\n",
       " 'ਂ': 806,\n",
       " '🤒': 807,\n",
       " '🤘': 808,\n",
       " '💎': 809,\n",
       " 'ग': 810,\n",
       " '´': 811,\n",
       " 'ง': 812,\n",
       " '🐐': 813,\n",
       " '🎉': 814,\n",
       " '📍': 815,\n",
       " '🇲': 816,\n",
       " '𝑜': 817,\n",
       " '기': 818,\n",
       " '계': 819,\n",
       " '🇺': 820,\n",
       " 'थ': 821,\n",
       " '🇳': 822,\n",
       " 'ะ': 823,\n",
       " 'า': 824,\n",
       " '🌙': 825,\n",
       " '🔈': 826,\n",
       " '😪': 827,\n",
       " '開': 828,\n",
       " '🦁': 829,\n",
       " '⛲': 830,\n",
       " '𝐉': 831,\n",
       " 'テ': 832,\n",
       " 'ج': 833,\n",
       " '👏': 834,\n",
       " '🌈': 835,\n",
       " '𝐡': 836,\n",
       " 'ま': 837,\n",
       " 'を': 838,\n",
       " 'య': 839,\n",
       " '🔥': 840,\n",
       " 'ਹ': 841,\n",
       " '☺': 842,\n",
       " '📣': 843,\n",
       " 'q': 844,\n",
       " 'ก': 845,\n",
       " 'ä': 846,\n",
       " '🍀': 847,\n",
       " '★': 848,\n",
       " '\\U000e006e': 849,\n",
       " '📖': 850,\n",
       " '😥': 851,\n",
       " 'é': 852,\n",
       " '😢': 853,\n",
       " 'ᶜ': 854,\n",
       " '🌘': 855,\n",
       " '😯': 856,\n",
       " 'फ': 857,\n",
       " 'ุ': 858,\n",
       " '👻': 859,\n",
       " '⛳': 860,\n",
       " '🙁': 861}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Conv1D, Input, Concatenate, Dropout, MaxPooling1D, MaxPool1D\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import initializers\n",
    "import keras.backend as K\n",
    "from keras.engine import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n"
     ]
    }
   ],
   "source": [
    "max_fatures = len(char2int)\n",
    "print (max_fatures)\n",
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.concat([train_texts[['uid','text','sentiment']],val_texts[['uid','text','sentiment']]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     6392\n",
       "positive    5616\n",
       "negative    4992\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out = 128 #dimension of the lstm output \n",
    "n_output = all_texts.sentiment.nunique() #number of possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_dict = {'negative':0,'neutral':1,'positive':2}\n",
    "in_senti_dict = {0:'negative',1:'neutral',2:'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts.sentiment = all_texts.sentiment.apply(lambda x: senti_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n"
     ]
    }
   ],
   "source": [
    "trainX = pad_sequences(train_sentences,maxlen=max_len,padding=\"post\")\n",
    "valX = pad_sequences(val_sentences,maxlen=max_len,padding=\"post\")\n",
    "testX = pad_sequences(test_sentences,maxlen=max_len,padding=\"post\")\n",
    "\n",
    "y = pd.get_dummies(all_texts.sentiment).values\n",
    "train_len = train_texts.shape[0]\n",
    "print (train_len)\n",
    "trainy = y[:train_len]\n",
    "valy = y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ min _ _ lyching @ manakgupta mein kahna nae chahta qki mere yaha btay … // tco / jwsdvvomt8'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([581, 222,  10, 166, 458, 222, 483, 222, 483, 222,  95, 440, 604,\n",
       "        85, 166, 458,  44, 222, 581, 222,  10, 599, 458, 599, 681,  44,\n",
       "        17, 669, 221, 599, 222,  10, 723, 166, 458, 222, 681, 599,  85,\n",
       "       458, 599, 222, 458, 599, 723, 222, 604,  85, 599,  85, 221, 599,\n",
       "       222, 844, 681, 166, 222,  10, 723, 136, 723, 222, 440, 599,  85,\n",
       "       599, 222, 491, 221, 599, 440, 222, 600, 222, 694, 694, 222, 221,\n",
       "       604, 231, 222, 694, 222, 359, 388,  98,   6, 804, 804, 231,  10,\n",
       "       221, 303,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    #f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)), name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )), name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)), name='u')\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input = Input(shape=(max_len,))\n",
    "    emb1 = Embedding(len(char2int) + 1, embed_dim, trainable=True)(input)\n",
    "    conv1 = Conv1D(filters=100,kernel_size=5,border_mode='valid',subsample_length=1)(emb1)\n",
    "    maxpool1 = MaxPooling1D(pool_length=5)(conv1)\n",
    "    \n",
    "    out1 = Bidirectional(LSTM(lstm_out, return_sequences=True))(maxpool1)\n",
    "    out2 = Bidirectional(LSTM(lstm_out//2, return_sequences=False))(out1)\n",
    "    out1_attn = AttLayer(lstm_out//2)(out1)\n",
    "    \n",
    "    out = Concatenate(-1)([out2,out1_attn])\n",
    "    #out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out)(out)\n",
    "    #out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out//2)(out)\n",
    "    #out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out//4)(out)\n",
    "    #out = Dropout(.2)(out)\n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model(input,out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0001),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0303 11:19:14.833392 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0303 11:19:14.835005 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:532: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0303 11:19:14.837724 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:4719: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=100, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=5)`\n",
      "  \"\"\"\n",
      "W0303 11:19:14.865000 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:4554: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0303 11:19:15.392533 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:4696: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0303 11:19:15.472442 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/optimizers.py:1925: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0303 11:19:15.479263 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:3827: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 200)     172400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 146, 100)     100100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 29, 100)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 29, 256)      234496      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          164352      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_1 (AttLayer)          (None, 256)          16512       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           bidirectional_2[0][0]            \n",
      "                                                                 att_layer_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          49280       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            99          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 747,575\n",
      "Trainable params: 747,575\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0303 11:19:31.017365 4508650944 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0303 11:19:32.785210 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:1051: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0303 11:19:32.993937 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:1038: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0303 11:19:33.220442 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:3171: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0303 11:19:33.225176 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:178: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0303 11:19:33.225911 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:185: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0303 11:19:33.243381 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:198: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0303 11:19:33.244154 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:207: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0303 11:19:33.592381 4508650944 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:214: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 27s - loss: 1.0767 - acc: 0.3846 - val_loss: 1.0583 - val_acc: 0.4227\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.42267, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 2/50\n",
      " - 24s - loss: 1.0174 - acc: 0.4614 - val_loss: 0.9780 - val_acc: 0.5030\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.42267 to 0.50300, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 3/50\n",
      " - 24s - loss: 0.9543 - acc: 0.5230 - val_loss: 0.9369 - val_acc: 0.5303\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50300 to 0.53033, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 4/50\n",
      " - 24s - loss: 0.9208 - acc: 0.5436 - val_loss: 0.9247 - val_acc: 0.5353\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.53033 to 0.53533, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 5/50\n",
      " - 24s - loss: 0.8919 - acc: 0.5627 - val_loss: 0.9135 - val_acc: 0.5407\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.53533 to 0.54067, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 6/50\n",
      " - 24s - loss: 0.8741 - acc: 0.5789 - val_loss: 0.9168 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.54067\n",
      "Epoch 7/50\n",
      " - 24s - loss: 0.8577 - acc: 0.5865 - val_loss: 0.9108 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.54067\n",
      "Epoch 8/50\n",
      " - 24s - loss: 0.8399 - acc: 0.6046 - val_loss: 0.9142 - val_acc: 0.5463\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.54067 to 0.54633, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 9/50\n",
      " - 24s - loss: 0.8186 - acc: 0.6159 - val_loss: 0.9083 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.54633\n",
      "Epoch 10/50\n",
      " - 24s - loss: 0.8054 - acc: 0.6228 - val_loss: 0.9498 - val_acc: 0.5390\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.54633\n",
      "Epoch 11/50\n",
      " - 24s - loss: 0.7878 - acc: 0.6321 - val_loss: 0.9419 - val_acc: 0.5550\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.54633 to 0.55500, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 12/50\n",
      " - 24s - loss: 0.7697 - acc: 0.6446 - val_loss: 0.9671 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.55500 to 0.55633, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 13/50\n",
      " - 24s - loss: 0.7620 - acc: 0.6484 - val_loss: 0.9190 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.55633\n",
      "Epoch 14/50\n",
      " - 24s - loss: 0.7421 - acc: 0.6574 - val_loss: 0.9463 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.55633\n",
      "Epoch 15/50\n",
      " - 24s - loss: 0.7389 - acc: 0.6579 - val_loss: 0.9305 - val_acc: 0.5483\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.55633\n",
      "Epoch 16/50\n",
      " - 24s - loss: 0.7076 - acc: 0.6770 - val_loss: 1.0006 - val_acc: 0.5413\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.55633\n",
      "Epoch 17/50\n",
      " - 24s - loss: 0.7067 - acc: 0.6771 - val_loss: 0.9757 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.55633 to 0.55700, saving model to ../models/weights_cmsa_subwordlevel.hdf5\n",
      "Epoch 18/50\n",
      " - 24s - loss: 0.6783 - acc: 0.6926 - val_loss: 0.9863 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.55700\n",
      "Epoch 19/50\n",
      " - 24s - loss: 0.6474 - acc: 0.7096 - val_loss: 1.0454 - val_acc: 0.5357\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.55700\n",
      "Epoch 20/50\n",
      " - 24s - loss: 0.6341 - acc: 0.7159 - val_loss: 1.1544 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.55700\n",
      "Epoch 21/50\n",
      " - 24s - loss: 0.6300 - acc: 0.7214 - val_loss: 1.0930 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.55700\n",
      "Epoch 22/50\n",
      " - 24s - loss: 0.6220 - acc: 0.7275 - val_loss: 1.0141 - val_acc: 0.5383\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.55700\n",
      "Epoch 23/50\n",
      " - 24s - loss: 0.5716 - acc: 0.7512 - val_loss: 1.1709 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.55700\n",
      "Epoch 24/50\n",
      " - 24s - loss: 0.5643 - acc: 0.7517 - val_loss: 1.1728 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.55700\n",
      "Epoch 25/50\n",
      " - 24s - loss: 0.5374 - acc: 0.7678 - val_loss: 1.3043 - val_acc: 0.5347\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.55700\n",
      "Epoch 26/50\n",
      " - 24s - loss: 0.5339 - acc: 0.7691 - val_loss: 1.2166 - val_acc: 0.5287\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.55700\n",
      "Epoch 27/50\n",
      " - 24s - loss: 0.5162 - acc: 0.7789 - val_loss: 1.2660 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.55700\n",
      "Epoch 28/50\n",
      " - 24s - loss: 0.4956 - acc: 0.7916 - val_loss: 1.3199 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.55700\n",
      "Epoch 29/50\n",
      " - 24s - loss: 0.4841 - acc: 0.7978 - val_loss: 1.3644 - val_acc: 0.5367\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.55700\n",
      "Epoch 30/50\n",
      " - 24s - loss: 0.4733 - acc: 0.8021 - val_loss: 1.4552 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.55700\n",
      "Epoch 31/50\n",
      " - 24s - loss: 0.4698 - acc: 0.8029 - val_loss: 1.4147 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.55700\n",
      "Epoch 32/50\n",
      " - 24s - loss: 0.4459 - acc: 0.8151 - val_loss: 1.5397 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.55700\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=5, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_cmsa_subwordlevel.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(trainX, trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=(valX,valy), callbacks=[early,lr,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snapshotensemble(Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        self.best_value = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        self.test_predictions.append(\n",
    "            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_withemb():\n",
    "    input = Input(shape=(max_len,))\n",
    "    emb1 = Embedding(len(word_index) + 1, embed_dim1, weights=[embedding_matrix1], trainable=True)(input)\n",
    "    emb2 = Embedding(len(word_index) + 1, embed_dim2, weights=[embedding_matrix2], trainable=False)(input)\n",
    "    #cnn = Conv1D(filters=100,kernel_size=5,strides=1,padding=\"same\")(Concatenate(-1)([emb1,emb2]))\n",
    "    cnn = SpatialDropout1D(.2)(Concatenate(-1)([emb1,emb2]))\n",
    "    out = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(cnn)\n",
    "    out = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=True))(out)\n",
    "    out = AttLayer(lstm_out//2)(out)\n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model(input,out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0001),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 20, 200)      3200200     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 20, 300)      4800300     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 20, 500)      0           embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 20, 500)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 20, 256)      644096      spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 20, 128)      164352      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_5 (AttLayer)          (None, 128)          8320        bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            387         att_layer_5[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,817,655\n",
      "Trainable params: 4,017,355\n",
      "Non-trainable params: 4,800,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model_withemb()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      " - 55s - loss: 0.6604 - acc: 0.7216 - val_loss: 0.8584 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61700, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 2/50\n",
      " - 51s - loss: 0.6547 - acc: 0.7259 - val_loss: 0.8563 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61700 to 0.62133, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 3/50\n",
      " - 54s - loss: 0.6477 - acc: 0.7251 - val_loss: 0.8628 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.62133\n",
      "Epoch 4/50\n",
      " - 53s - loss: 0.6405 - acc: 0.7356 - val_loss: 0.8615 - val_acc: 0.6193\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.62133\n",
      "Epoch 5/50\n",
      " - 54s - loss: 0.6414 - acc: 0.7294 - val_loss: 0.8667 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.62133 to 0.62233, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 6/50\n",
      " - 56s - loss: 0.6273 - acc: 0.7412 - val_loss: 0.8732 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.62233 to 0.62400, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 7/50\n",
      " - 65s - loss: 0.6165 - acc: 0.7424 - val_loss: 0.8757 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.62400\n",
      "Epoch 8/50\n",
      " - 60s - loss: 0.6219 - acc: 0.7411 - val_loss: 0.8798 - val_acc: 0.6243\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.62400 to 0.62433, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 9/50\n",
      " - 53s - loss: 0.6123 - acc: 0.7493 - val_loss: 0.8824 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.62433\n",
      "Epoch 10/50\n",
      " - 53s - loss: 0.6013 - acc: 0.7539 - val_loss: 0.8897 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62433\n",
      "Epoch 11/50\n",
      " - 62s - loss: 0.5961 - acc: 0.7568 - val_loss: 0.8878 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62433\n",
      "Epoch 12/50\n",
      " - 50s - loss: 0.5908 - acc: 0.7581 - val_loss: 0.9005 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62433\n",
      "Epoch 13/50\n",
      " - 50s - loss: 0.5922 - acc: 0.7530 - val_loss: 0.8926 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62433\n",
      "Epoch 14/50\n",
      " - 55s - loss: 0.5814 - acc: 0.7639 - val_loss: 0.9066 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62433\n",
      "Epoch 15/50\n",
      " - 53s - loss: 0.5750 - acc: 0.7666 - val_loss: 0.9136 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62433\n",
      "Epoch 16/50\n",
      " - 51s - loss: 0.5642 - acc: 0.7691 - val_loss: 0.9110 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62433\n",
      "Epoch 17/50\n",
      " - 50s - loss: 0.5600 - acc: 0.7716 - val_loss: 0.9116 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62433\n",
      "Epoch 18/50\n",
      " - 50s - loss: 0.5555 - acc: 0.7779 - val_loss: 0.9101 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62433\n",
      "Epoch 19/50\n",
      " - 50s - loss: 0.5470 - acc: 0.7764 - val_loss: 0.9344 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62433\n",
      "Epoch 20/50\n",
      " - 55s - loss: 0.5507 - acc: 0.7786 - val_loss: 0.9198 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62433\n",
      "Epoch 21/50\n",
      " - 56s - loss: 0.5503 - acc: 0.7820 - val_loss: 0.9251 - val_acc: 0.6127\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62433\n",
      "Epoch 22/50\n",
      " - 54s - loss: 0.5387 - acc: 0.7814 - val_loss: 0.9284 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62433\n",
      "Epoch 23/50\n",
      " - 51s - loss: 0.5373 - acc: 0.7875 - val_loss: 0.9371 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62433\n",
      "Epoch 24/50\n",
      " - 50s - loss: 0.5338 - acc: 0.7844 - val_loss: 0.9410 - val_acc: 0.6133\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.6806997336971108e-05.\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62433\n",
      "Epoch 25/50\n",
      " - 50s - loss: 0.5266 - acc: 0.7902 - val_loss: 0.9424 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62433\n",
      "Epoch 26/50\n",
      " - 50s - loss: 0.5217 - acc: 0.7900 - val_loss: 0.9506 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62433\n",
      "Epoch 27/50\n",
      " - 51s - loss: 0.5240 - acc: 0.7888 - val_loss: 0.9517 - val_acc: 0.6137\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62433\n",
      "Epoch 28/50\n",
      " - 60s - loss: 0.5196 - acc: 0.7930 - val_loss: 0.9484 - val_acc: 0.6113\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62433\n",
      "Epoch 29/50\n",
      " - 62s - loss: 0.5124 - acc: 0.7941 - val_loss: 0.9616 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62433\n",
      "Epoch 30/50\n",
      " - 60s - loss: 0.5096 - acc: 0.7984 - val_loss: 0.9651 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62433\n",
      "Epoch 31/50\n",
      " - 57s - loss: 0.5151 - acc: 0.7965 - val_loss: 0.9593 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62433\n",
      "Epoch 32/50\n",
      " - 56s - loss: 0.5184 - acc: 0.7921 - val_loss: 0.9544 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.1764897499233484e-05.\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62433\n",
      "Epoch 33/50\n",
      " - 56s - loss: 0.4957 - acc: 0.8034 - val_loss: 0.9690 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62433\n",
      "Epoch 34/50\n",
      " - 57s - loss: 0.5050 - acc: 0.7975 - val_loss: 0.9678 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62433\n",
      "Epoch 35/50\n",
      " - 63s - loss: 0.4952 - acc: 0.8037 - val_loss: 0.9732 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62433\n",
      "Epoch 36/50\n",
      " - 59s - loss: 0.4944 - acc: 0.8034 - val_loss: 0.9753 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.62433\n",
      "Epoch 37/50\n",
      " - 64s - loss: 0.4976 - acc: 0.8009 - val_loss: 0.9729 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62433\n",
      "Epoch 38/50\n",
      " - 64s - loss: 0.4955 - acc: 0.8046 - val_loss: 0.9785 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.62433\n",
      "Epoch 39/50\n",
      " - 62s - loss: 0.4939 - acc: 0.8051 - val_loss: 0.9782 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62433\n",
      "Epoch 40/50\n",
      " - 60s - loss: 0.4902 - acc: 0.8042 - val_loss: 0.9828 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.235428504121954e-06.\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62433\n",
      "Epoch 41/50\n",
      " - 58s - loss: 0.4872 - acc: 0.8041 - val_loss: 0.9844 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62433\n",
      "Epoch 42/50\n",
      " - 56s - loss: 0.4893 - acc: 0.8073 - val_loss: 0.9850 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62433\n",
      "Epoch 43/50\n",
      " - 57s - loss: 0.4882 - acc: 0.8065 - val_loss: 0.9835 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62433\n",
      "Epoch 44/50\n",
      " - 57s - loss: 0.4794 - acc: 0.8114 - val_loss: 0.9877 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62433\n",
      "Epoch 45/50\n",
      " - 55s - loss: 0.4810 - acc: 0.8084 - val_loss: 0.9906 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62433\n",
      "Epoch 46/50\n",
      " - 56s - loss: 0.4874 - acc: 0.8052 - val_loss: 0.9926 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62433\n",
      "Epoch 47/50\n",
      " - 59s - loss: 0.4794 - acc: 0.8121 - val_loss: 0.9942 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62433\n",
      "Epoch 48/50\n",
      " - 56s - loss: 0.4719 - acc: 0.8129 - val_loss: 0.9955 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 5.764799698226852e-06.\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.62433\n",
      "Epoch 49/50\n",
      " - 56s - loss: 0.4878 - acc: 0.8059 - val_loss: 0.9922 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62433\n",
      "Epoch 50/50\n",
      " - 56s - loss: 0.4802 - acc: 0.8085 - val_loss: 0.9951 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62433\n",
      "Epoch 00050: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=8, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_finalmodel3.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "snapshot = Snapshotensemble([valX,valy],testX)\n",
    "\n",
    "history = model.fit(trainX, trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=(valX,valy), callbacks=[early,lr,checkpointer,snapshot])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/weights_finalmodel3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3000, 3)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(snapshot.valid_predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(snapshot.valid_predictions).mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(valX).argmax(axis=1) #np.array(snapshot.valid_predictions[:5]).mean(0).argmax(axis=1)\n",
    "val_texts['sentiment_pred'] = [in_senti_dict[i] for i in val_pred]\n",
    "#val_texts = val_texts.sort_values(['uid'],ascending=[True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid                                               text sentiment  \\\n",
       "0     3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13   12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23   23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35   24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49   45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "\n",
       "   sentiment_pred sentiment_pred2 sentiment_pred3  \n",
       "0        negative        positive         neutral  \n",
       "13       positive        positive        positive  \n",
       "23       negative        negative        negative  \n",
       "35       negative         neutral         neutral  \n",
       "49       positive         neutral        positive  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6286077414937643\n",
      "0.6243333333333333\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[598, 224,  68],\n",
       "       [274, 564, 290],\n",
       "       [ 82, 209, 691]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val_texts.sentiment,val_texts.sentiment_pred,labels=['negative','neutral','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.array(snapshot.test_predictions).mean(0).argmax(1)\n",
    "test_texts['sentiment_pred2'] = [in_senti_dict[i] for i in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(testX).argmax(axis=1)\n",
    "test_texts['sentiment_pred'] = [in_senti_dict[i] for i in test_pred]\n",
    "#test_texts_without_sort = test_texts.copy()\n",
    "#test_texts = test_texts.sort_values(['uid'],ascending=[True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe jaise pakistan wale karte south ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>anus prerna way ran saving ram ram jay rajaraa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai pathan nae warna ptm nay wae dramay...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision much option arm big second ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>keep saying kenyan rugby beautiful save kru dr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>guru give bless sewa simrn parmarth good deeds...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>136</td>\n",
       "      <td>tum logo zindagi mai kabhi india map nahi dhek...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>139</td>\n",
       "      <td>itne gandi priye aapke profile pic modi lga le...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>158</td>\n",
       "      <td>google translate working aisi jagah net aata</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>qaira sahb great man apka beta nhayat nfees na...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment_pred  \\\n",
       "0    1  keh aese rahe jaise pakistan wale karte south ...       negative   \n",
       "1    4  anus prerna way ran saving ram ram jay rajaraa...        neutral   \n",
       "2   17  shukar hai pathan nae warna ptm nay wae dramay...       negative   \n",
       "3   18  harsh pen decision much option arm big second ...       positive   \n",
       "4   34  keep saying kenyan rugby beautiful save kru dr...       positive   \n",
       "5   43  guru give bless sewa simrn parmarth good deeds...       positive   \n",
       "6  136  tum logo zindagi mai kabhi india map nahi dhek...        neutral   \n",
       "7  139  itne gandi priye aapke profile pic modi lga le...        neutral   \n",
       "8  158       google translate working aisi jagah net aata        neutral   \n",
       "9  162  qaira sahb great man apka beta nhayat nfees na...        neutral   \n",
       "\n",
       "  sentiment_pred3 sentiment_pred2  \n",
       "0        negative        negative  \n",
       "1         neutral         neutral  \n",
       "2         neutral         neutral  \n",
       "3        positive        positive  \n",
       "4         neutral        positive  \n",
       "5        positive        positive  \n",
       "6         neutral         neutral  \n",
       "7         neutral         neutral  \n",
       "8         neutral         neutral  \n",
       "9        positive        positive  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1079\n",
       "positive    1000\n",
       "negative     921\n",
       "Name: sentiment_pred, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.sentiment_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1049\n",
       "neutral      997\n",
       "negative     954\n",
       "Name: sentiment_pred, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.sentiment_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1128\n",
       "positive     982\n",
       "negative     890\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1 of 10\n",
      "Starting iteration 2 of 10\n",
      "Starting iteration 3 of 10\n",
      "Starting iteration 4 of 10\n",
      "Starting iteration 5 of 10\n",
      "Starting iteration 6 of 10\n",
      "Starting iteration 7 of 10\n",
      "Starting iteration 8 of 10\n",
      "Starting iteration 9 of 10\n",
      "Starting iteration 10 of 10\n",
      "done in 1136.884s.\n"
     ]
    }
   ],
   "source": [
    "from LJST_script_BTM import *\n",
    "\n",
    "minlabel = 0\n",
    "maxlabel = 2\n",
    "sentirange = maxlabel-minlabel\n",
    "numwordspertopic=5\n",
    "skipgramwindow=5\n",
    "\n",
    "numsentilabel=3\n",
    "numtopics=10\n",
    "alpha=10\n",
    "beta=.01\n",
    "gamma=10.0\n",
    "maxiter=10\n",
    "\n",
    "unlabelled_data = pd.concat([val_texts[['text']],test_texts[['text']]],axis=0)\n",
    "\n",
    "ljst_model = run_experiment(train_texts.text,trainy.argmax(1),unlabelled_data.text,sentirange,minlabel,maxlabel,numsentilabel,numtopics,skipgramwindow,alpha,beta,gamma,maxiter,numwordspertopic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_estimated = []\n",
    "for i in range(len(val_texts)):\n",
    "    sentiment = 0\n",
    "    index = len(train_texts) + i\n",
    "    temp = np.matmul(ljst_model.dt_distribution[index,:],ljst_model.dts_distribution[index,:,:])\n",
    "    sentiment = temp.argmax()\n",
    "    #for k, val in enumerate(temp):\n",
    "    #    sentiment += k*binsize*val\n",
    "    ds_estimated.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts['sentiment_pred2'] = [in_senti_dict[i] for i in ds_estimated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071254291113468\n",
      "0.5173333333333333\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred2,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>ankita shah8 bhadve photo elections pehle jyad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>krishna jcb full trend chal rahi</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>sharma1 kavita sharma4 sunita kal jab evm seal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>101</td>\n",
       "      <td>nolo weni ankere gae weekend</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>145</td>\n",
       "      <td>abe kutte sakal musalmaan wazah tere ghar roti...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>167</td>\n",
       "      <td>mohsin dawartweets daikho aur kuch kha ker mar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>252</td>\n",
       "      <td>haha soldier reading american gods hardy har h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>288</td>\n",
       "      <td>dor song jis adnan sami mujahid part play keya...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>318</td>\n",
       "      <td>government kya kuch nahi kar sakti tumhare bhe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>337</td>\n",
       "      <td>rao hindi tumhari bass nahi nahi seekh paoge d...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>348</td>\n",
       "      <td>ch1 pakeeza language istemaal koi nooni bhayoo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>352</td>\n",
       "      <td>gulwish nahi news defend kerne koshesh ker rhe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>359</td>\n",
       "      <td>ahmad884 lulli hoti nahi khadi baten karalo ba...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>416</td>\n",
       "      <td>live urdu fridaysermon delivered head ahmadiyy...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>439</td>\n",
       "      <td>color tum india wale sari yahi sochti pakistan...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>462</td>\n",
       "      <td>world cup mein mard nahi khusre gae hein larna...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>466</td>\n",
       "      <td>bhaijaan please mein apka bahot bahot bada fan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>469</td>\n",
       "      <td>postopinions take care bibi allah khair stupid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>497</td>\n",
       "      <td>scratch kfc bhi auqaat nahi rahee todays defea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>529</td>\n",
       "      <td>hai sabse super modi hai sabse uper modi deshk...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>561</td>\n",
       "      <td>mazhab naam par election jeetna koi bada kaarn...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>569</td>\n",
       "      <td>may allah bless strong imaan taqwa piety pure ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>576</td>\n",
       "      <td>super hilarious true reality noora haramkhor f...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>605</td>\n",
       "      <td>skfc good morning cute kartikeyan annaa happy ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>629</td>\n",
       "      <td>hadd hoti hai wrong make account live</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>712</td>\n",
       "      <td>hahahahaah fuck koy salig lucas doh hapit inju...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>715</td>\n",
       "      <td>sacrificed lot happy normal lad liverpool whos...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>740</td>\n",
       "      <td>yeah aap hamara roza kharaab karay hain yeh us...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>743</td>\n",
       "      <td>bollyhungama vidya balan monday bakri eid wom ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>745</td>\n",
       "      <td>amitbehere use ram bas aur yehi log bolte hind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>747</td>\n",
       "      <td>sai institute manimajra chandigarh best place ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>775</td>\n",
       "      <td>jai shree ram jay hanuman jai shree krishna ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>781</td>\n",
       "      <td>khan sahab apko vote diya taky bany alhamdulil...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>797</td>\n",
       "      <td>sir mere rs1180 deduct gaya hai kekin branch o...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>807</td>\n",
       "      <td>would bring great joy hear rendition des ree g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>837</td>\n",
       "      <td>sale owner split level home great neighborhood...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>870</td>\n",
       "      <td>scindia meri baat nhi suni apne congress prem ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>880</td>\n",
       "      <td>dil khush krny ghalib khyal acha myth efnod8ijpj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>885</td>\n",
       "      <td>ngk good one kudos heroes offl thisisysr</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>892</td>\n",
       "      <td>officeofknath sahi hoga sayad tabhi sirf appki...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>895</td>\n",
       "      <td>ez1403 pakistani pig dukkar kutta suvar mix aulad</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>906</td>\n",
       "      <td>bies blocked aapke acche din gaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>939</td>\n",
       "      <td>kalia thaa want hindi uneducated people nice c...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>962</td>\n",
       "      <td>eid mubarak hadiya trabaho vlogs cdjaercsyo</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment  \\\n",
       "0      3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13    12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23    23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35    24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49    45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "59    61  ankita shah8 bhadve photo elections pehle jyad...  negative   \n",
       "69    64                   krishna jcb full trend chal rahi  positive   \n",
       "75    77  sharma1 kavita sharma4 sunita kal jab evm seal...  negative   \n",
       "90    81  accha kiya invite nai kiya corrupted party ind...  negative   \n",
       "103  101                       nolo weni ankere gae weekend  positive   \n",
       "108  145  abe kutte sakal musalmaan wazah tere ghar roti...  negative   \n",
       "125  167  mohsin dawartweets daikho aur kuch kha ker mar...  negative   \n",
       "142  252  haha soldier reading american gods hardy har h...  negative   \n",
       "154  288  dor song jis adnan sami mujahid part play keya...   neutral   \n",
       "170  318  government kya kuch nahi kar sakti tumhare bhe...  negative   \n",
       "186  337  rao hindi tumhari bass nahi nahi seekh paoge d...  negative   \n",
       "200  348  ch1 pakeeza language istemaal koi nooni bhayoo...   neutral   \n",
       "216  352  gulwish nahi news defend kerne koshesh ker rhe...   neutral   \n",
       "226  359  ahmad884 lulli hoti nahi khadi baten karalo ba...   neutral   \n",
       "236  416  live urdu fridaysermon delivered head ahmadiyy...   neutral   \n",
       "248  439  color tum india wale sari yahi sochti pakistan...  positive   \n",
       "267  462  world cup mein mard nahi khusre gae hein larna...  negative   \n",
       "284  466  bhaijaan please mein apka bahot bahot bada fan...   neutral   \n",
       "300  469  postopinions take care bibi allah khair stupid...  negative   \n",
       "311  497  scratch kfc bhi auqaat nahi rahee todays defea...  negative   \n",
       "327  529  hai sabse super modi hai sabse uper modi deshk...   neutral   \n",
       "340  561  mazhab naam par election jeetna koi bada kaarn...  negative   \n",
       "352  569  may allah bless strong imaan taqwa piety pure ...  positive   \n",
       "366  576  super hilarious true reality noora haramkhor f...   neutral   \n",
       "375  605  skfc good morning cute kartikeyan annaa happy ...   neutral   \n",
       "385  629              hadd hoti hai wrong make account live  negative   \n",
       "392  712  hahahahaah fuck koy salig lucas doh hapit inju...   neutral   \n",
       "401  715  sacrificed lot happy normal lad liverpool whos...   neutral   \n",
       "411  740  yeah aap hamara roza kharaab karay hain yeh us...   neutral   \n",
       "429  743  bollyhungama vidya balan monday bakri eid wom ...  positive   \n",
       "444  745  amitbehere use ram bas aur yehi log bolte hind...  negative   \n",
       "458  747  sai institute manimajra chandigarh best place ...   neutral   \n",
       "472  775  jai shree ram jay hanuman jai shree krishna ja...   neutral   \n",
       "489  781  khan sahab apko vote diya taky bany alhamdulil...   neutral   \n",
       "504  797  sir mere rs1180 deduct gaya hai kekin branch o...   neutral   \n",
       "518  807  would bring great joy hear rendition des ree g...   neutral   \n",
       "529  837  sale owner split level home great neighborhood...  positive   \n",
       "544  870  scindia meri baat nhi suni apne congress prem ...  negative   \n",
       "561  880   dil khush krny ghalib khyal acha myth efnod8ijpj   neutral   \n",
       "569  885           ngk good one kudos heroes offl thisisysr  positive   \n",
       "576  892  officeofknath sahi hoga sayad tabhi sirf appki...   neutral   \n",
       "593  895  ez1403 pakistani pig dukkar kutta suvar mix aulad  negative   \n",
       "601  906                  bies blocked aapke acche din gaye   neutral   \n",
       "607  939  kalia thaa want hindi uneducated people nice c...   neutral   \n",
       "617  962        eid mubarak hadiya trabaho vlogs cdjaercsyo   neutral   \n",
       "\n",
       "    sentiment_pred sentiment_pred2  \n",
       "0         negative        positive  \n",
       "13        positive        positive  \n",
       "23        negative        negative  \n",
       "35        negative         neutral  \n",
       "49        positive         neutral  \n",
       "59        negative        negative  \n",
       "69         neutral         neutral  \n",
       "75        negative        negative  \n",
       "90        negative         neutral  \n",
       "103        neutral         neutral  \n",
       "108       negative        negative  \n",
       "125       negative        negative  \n",
       "142       positive        positive  \n",
       "154        neutral        positive  \n",
       "170       negative        negative  \n",
       "186        neutral        negative  \n",
       "200        neutral         neutral  \n",
       "216       negative        negative  \n",
       "226        neutral         neutral  \n",
       "236        neutral         neutral  \n",
       "248       negative        negative  \n",
       "267        neutral        negative  \n",
       "284       positive        positive  \n",
       "300       positive        positive  \n",
       "311        neutral        negative  \n",
       "327       positive        positive  \n",
       "340        neutral        negative  \n",
       "352       positive        positive  \n",
       "366       positive        positive  \n",
       "375       positive        positive  \n",
       "385        neutral        negative  \n",
       "392        neutral        negative  \n",
       "401       positive        positive  \n",
       "411       positive         neutral  \n",
       "429        neutral         neutral  \n",
       "444       negative        negative  \n",
       "458       positive        positive  \n",
       "472        neutral        positive  \n",
       "489       negative         neutral  \n",
       "504        neutral         neutral  \n",
       "518       positive        positive  \n",
       "529       negative        positive  \n",
       "544       negative         neutral  \n",
       "561       positive         neutral  \n",
       "569       positive        positive  \n",
       "576       negative        negative  \n",
       "593       negative        negative  \n",
       "601        neutral        negative  \n",
       "607        neutral        positive  \n",
       "617       positive        positive  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_withtm():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    input2 = Input(shape=(numtopics,))\n",
    "    #input3 = Input(shape=(numtopics*numsentilabel,))\n",
    "    \n",
    "    emb1 = Embedding(len(word_index) + 1, embed_dim1, weights=[embedding_matrix1], trainable=False)(input1)\n",
    "    emb2 = Embedding(len(word_index) + 1, embed_dim2, weights=[embedding_matrix2], trainable=False)(input1)\n",
    "    #cnn = Conv1D(filters=100,kernel_size=5,strides=1,padding=\"same\")(Concatenate(-1)([emb1,emb2]))\n",
    "    cnn = SpatialDropout1D(.2)(Concatenate(-1)([emb1,emb2]))\n",
    "    out = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(cnn)\n",
    "    out = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=True))(out)\n",
    "    out = AttLayer(lstm_out//2)(out)\n",
    "    \n",
    "    out = Concatenate(-1)([out,input2])\n",
    "    \n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model([input1,input2],out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0005),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 20, 200)      3200200     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 20, 300)      4800300     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 20, 500)      0           embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 20, 500)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 20, 256)      644096      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 20, 128)      164352      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_4 (AttLayer)          (None, 128)          8320        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 138)          0           att_layer_4[0][0]                \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            417         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,817,685\n",
      "Trainable params: 817,185\n",
      "Non-trainable params: 8,000,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model_withtm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tm1 = ljst_model.dt_distribution[:len(train_texts)]\n",
    "val_tm1 = ljst_model.dt_distribution[len(train_texts):len(train_texts)+len(val_texts)]\n",
    "test_tm1 = ljst_model.dt_distribution[len(train_texts)+len(val_texts):]\n",
    "\n",
    "train_tm2 = ljst_model.dts_distribution[:len(train_texts)].reshape(len(train_texts),-1)\n",
    "val_tm2 = ljst_model.dts_distribution[len(train_texts):len(train_texts)+len(val_texts)].reshape(len(val_texts),-1)\n",
    "test_tm2 = ljst_model.dts_distribution[len(train_texts)+len(val_texts):].reshape(len(test_texts),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      " - 32s - loss: 0.9028 - acc: 0.5614 - val_loss: 0.8747 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58800, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 2/50\n",
      " - 27s - loss: 0.8697 - acc: 0.5826 - val_loss: 0.8643 - val_acc: 0.5830\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.58800\n",
      "Epoch 3/50\n",
      " - 27s - loss: 0.8608 - acc: 0.5873 - val_loss: 0.8490 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58800 to 0.61000, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 4/50\n",
      " - 27s - loss: 0.8511 - acc: 0.5930 - val_loss: 0.8403 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61000\n",
      "Epoch 5/50\n",
      " - 27s - loss: 0.8462 - acc: 0.5994 - val_loss: 0.8417 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61000 to 0.61600, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 6/50\n",
      " - 31s - loss: 0.8378 - acc: 0.6056 - val_loss: 0.8290 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61600\n",
      "Epoch 7/50\n",
      " - 32s - loss: 0.8308 - acc: 0.6081 - val_loss: 0.8268 - val_acc: 0.6153\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.61600\n",
      "Epoch 8/50\n",
      " - 33s - loss: 0.8287 - acc: 0.6113 - val_loss: 0.8249 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.61600 to 0.61667, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 9/50\n",
      " - 30s - loss: 0.8161 - acc: 0.6211 - val_loss: 0.8311 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61667\n",
      "Epoch 10/50\n",
      " - 30s - loss: 0.8134 - acc: 0.6246 - val_loss: 0.8356 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.61667\n",
      "Epoch 11/50\n",
      " - 30s - loss: 0.8074 - acc: 0.6271 - val_loss: 0.8210 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.61667 to 0.62133, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 12/50\n",
      " - 29s - loss: 0.8011 - acc: 0.6303 - val_loss: 0.8205 - val_acc: 0.6177\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62133\n",
      "Epoch 13/50\n",
      " - 30s - loss: 0.7931 - acc: 0.6354 - val_loss: 0.8235 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62133\n",
      "Epoch 14/50\n",
      " - 30s - loss: 0.7898 - acc: 0.6402 - val_loss: 0.8251 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62133\n",
      "Epoch 15/50\n",
      " - 33s - loss: 0.7840 - acc: 0.6457 - val_loss: 0.8332 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62133\n",
      "Epoch 16/50\n",
      " - 31s - loss: 0.7740 - acc: 0.6468 - val_loss: 0.8444 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00035000001662410796.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62133\n",
      "Epoch 17/50\n",
      " - 32s - loss: 0.7579 - acc: 0.6609 - val_loss: 0.8420 - val_acc: 0.6073\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62133\n",
      "Epoch 18/50\n",
      " - 32s - loss: 0.7532 - acc: 0.6624 - val_loss: 0.8343 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.62133 to 0.62167, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 19/50\n",
      " - 30s - loss: 0.7427 - acc: 0.6659 - val_loss: 0.8401 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62167\n",
      "Epoch 20/50\n",
      " - 31s - loss: 0.7344 - acc: 0.6741 - val_loss: 0.8585 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62167\n",
      "Epoch 21/50\n",
      " - 31s - loss: 0.7279 - acc: 0.6752 - val_loss: 0.8639 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62167\n",
      "Epoch 22/50\n",
      " - 31s - loss: 0.7194 - acc: 0.6801 - val_loss: 0.8649 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62167\n",
      "Epoch 23/50\n",
      " - 31s - loss: 0.7089 - acc: 0.6811 - val_loss: 0.8754 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00024500001163687554.\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62167\n",
      "Epoch 24/50\n",
      " - 33s - loss: 0.6916 - acc: 0.6991 - val_loss: 0.8862 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62167\n",
      "Epoch 25/50\n",
      " - 29s - loss: 0.6788 - acc: 0.7049 - val_loss: 0.9039 - val_acc: 0.6017\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62167\n",
      "Epoch 26/50\n",
      " - 30s - loss: 0.6671 - acc: 0.7092 - val_loss: 0.8928 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62167\n",
      "Epoch 27/50\n",
      " - 34s - loss: 0.6627 - acc: 0.7103 - val_loss: 0.9236 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62167\n",
      "Epoch 28/50\n",
      " - 32s - loss: 0.6523 - acc: 0.7162 - val_loss: 0.9252 - val_acc: 0.6017\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00017150000203400848.\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62167\n",
      "Epoch 29/50\n",
      " - 31s - loss: 0.6368 - acc: 0.7267 - val_loss: 0.9271 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62167\n",
      "Epoch 30/50\n",
      " - 32s - loss: 0.6271 - acc: 0.7306 - val_loss: 0.9472 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62167\n",
      "Epoch 31/50\n",
      " - 30s - loss: 0.6226 - acc: 0.7296 - val_loss: 0.9487 - val_acc: 0.5983\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62167\n",
      "Epoch 32/50\n",
      " - 36s - loss: 0.6109 - acc: 0.7383 - val_loss: 0.9632 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62167\n",
      "Epoch 33/50\n",
      " - 32s - loss: 0.6092 - acc: 0.7390 - val_loss: 0.9535 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00012004999734926967.\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62167\n",
      "Epoch 00033: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=5, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_finalmodel2.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit([trainX,train_tm1], trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=([valX,val_tm1],valy), callbacks=[early,lr,checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict([valX,val_tm1]).argmax(axis=1)\n",
    "val_texts['sentiment_pred3'] = [in_senti_dict[i] for i in val_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>ankita shah8 bhadve photo elections pehle jyad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>krishna jcb full trend chal rahi</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>sharma1 kavita sharma4 sunita kal jab evm seal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>101</td>\n",
       "      <td>nolo weni ankere gae weekend</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>145</td>\n",
       "      <td>abe kutte sakal musalmaan wazah tere ghar roti...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>167</td>\n",
       "      <td>mohsin dawartweets daikho aur kuch kha ker mar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>252</td>\n",
       "      <td>haha soldier reading american gods hardy har h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>288</td>\n",
       "      <td>dor song jis adnan sami mujahid part play keya...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>318</td>\n",
       "      <td>government kya kuch nahi kar sakti tumhare bhe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>337</td>\n",
       "      <td>rao hindi tumhari bass nahi nahi seekh paoge d...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>348</td>\n",
       "      <td>ch1 pakeeza language istemaal koi nooni bhayoo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>352</td>\n",
       "      <td>gulwish nahi news defend kerne koshesh ker rhe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>359</td>\n",
       "      <td>ahmad884 lulli hoti nahi khadi baten karalo ba...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>416</td>\n",
       "      <td>live urdu fridaysermon delivered head ahmadiyy...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>439</td>\n",
       "      <td>color tum india wale sari yahi sochti pakistan...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>462</td>\n",
       "      <td>world cup mein mard nahi khusre gae hein larna...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>466</td>\n",
       "      <td>bhaijaan please mein apka bahot bahot bada fan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>469</td>\n",
       "      <td>postopinions take care bibi allah khair stupid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>497</td>\n",
       "      <td>scratch kfc bhi auqaat nahi rahee todays defea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>529</td>\n",
       "      <td>hai sabse super modi hai sabse uper modi deshk...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>561</td>\n",
       "      <td>mazhab naam par election jeetna koi bada kaarn...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>569</td>\n",
       "      <td>may allah bless strong imaan taqwa piety pure ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>576</td>\n",
       "      <td>super hilarious true reality noora haramkhor f...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>605</td>\n",
       "      <td>skfc good morning cute kartikeyan annaa happy ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>629</td>\n",
       "      <td>hadd hoti hai wrong make account live</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>712</td>\n",
       "      <td>hahahahaah fuck koy salig lucas doh hapit inju...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>715</td>\n",
       "      <td>sacrificed lot happy normal lad liverpool whos...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>740</td>\n",
       "      <td>yeah aap hamara roza kharaab karay hain yeh us...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>743</td>\n",
       "      <td>bollyhungama vidya balan monday bakri eid wom ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>745</td>\n",
       "      <td>amitbehere use ram bas aur yehi log bolte hind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>747</td>\n",
       "      <td>sai institute manimajra chandigarh best place ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>775</td>\n",
       "      <td>jai shree ram jay hanuman jai shree krishna ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>781</td>\n",
       "      <td>khan sahab apko vote diya taky bany alhamdulil...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>797</td>\n",
       "      <td>sir mere rs1180 deduct gaya hai kekin branch o...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>807</td>\n",
       "      <td>would bring great joy hear rendition des ree g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>837</td>\n",
       "      <td>sale owner split level home great neighborhood...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>870</td>\n",
       "      <td>scindia meri baat nhi suni apne congress prem ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>880</td>\n",
       "      <td>dil khush krny ghalib khyal acha myth efnod8ijpj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>885</td>\n",
       "      <td>ngk good one kudos heroes offl thisisysr</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>892</td>\n",
       "      <td>officeofknath sahi hoga sayad tabhi sirf appki...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>895</td>\n",
       "      <td>ez1403 pakistani pig dukkar kutta suvar mix aulad</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>906</td>\n",
       "      <td>bies blocked aapke acche din gaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>939</td>\n",
       "      <td>kalia thaa want hindi uneducated people nice c...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>962</td>\n",
       "      <td>eid mubarak hadiya trabaho vlogs cdjaercsyo</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment  \\\n",
       "0      3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13    12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23    23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35    24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49    45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "59    61  ankita shah8 bhadve photo elections pehle jyad...  negative   \n",
       "69    64                   krishna jcb full trend chal rahi  positive   \n",
       "75    77  sharma1 kavita sharma4 sunita kal jab evm seal...  negative   \n",
       "90    81  accha kiya invite nai kiya corrupted party ind...  negative   \n",
       "103  101                       nolo weni ankere gae weekend  positive   \n",
       "108  145  abe kutte sakal musalmaan wazah tere ghar roti...  negative   \n",
       "125  167  mohsin dawartweets daikho aur kuch kha ker mar...  negative   \n",
       "142  252  haha soldier reading american gods hardy har h...  negative   \n",
       "154  288  dor song jis adnan sami mujahid part play keya...   neutral   \n",
       "170  318  government kya kuch nahi kar sakti tumhare bhe...  negative   \n",
       "186  337  rao hindi tumhari bass nahi nahi seekh paoge d...  negative   \n",
       "200  348  ch1 pakeeza language istemaal koi nooni bhayoo...   neutral   \n",
       "216  352  gulwish nahi news defend kerne koshesh ker rhe...   neutral   \n",
       "226  359  ahmad884 lulli hoti nahi khadi baten karalo ba...   neutral   \n",
       "236  416  live urdu fridaysermon delivered head ahmadiyy...   neutral   \n",
       "248  439  color tum india wale sari yahi sochti pakistan...  positive   \n",
       "267  462  world cup mein mard nahi khusre gae hein larna...  negative   \n",
       "284  466  bhaijaan please mein apka bahot bahot bada fan...   neutral   \n",
       "300  469  postopinions take care bibi allah khair stupid...  negative   \n",
       "311  497  scratch kfc bhi auqaat nahi rahee todays defea...  negative   \n",
       "327  529  hai sabse super modi hai sabse uper modi deshk...   neutral   \n",
       "340  561  mazhab naam par election jeetna koi bada kaarn...  negative   \n",
       "352  569  may allah bless strong imaan taqwa piety pure ...  positive   \n",
       "366  576  super hilarious true reality noora haramkhor f...   neutral   \n",
       "375  605  skfc good morning cute kartikeyan annaa happy ...   neutral   \n",
       "385  629              hadd hoti hai wrong make account live  negative   \n",
       "392  712  hahahahaah fuck koy salig lucas doh hapit inju...   neutral   \n",
       "401  715  sacrificed lot happy normal lad liverpool whos...   neutral   \n",
       "411  740  yeah aap hamara roza kharaab karay hain yeh us...   neutral   \n",
       "429  743  bollyhungama vidya balan monday bakri eid wom ...  positive   \n",
       "444  745  amitbehere use ram bas aur yehi log bolte hind...  negative   \n",
       "458  747  sai institute manimajra chandigarh best place ...   neutral   \n",
       "472  775  jai shree ram jay hanuman jai shree krishna ja...   neutral   \n",
       "489  781  khan sahab apko vote diya taky bany alhamdulil...   neutral   \n",
       "504  797  sir mere rs1180 deduct gaya hai kekin branch o...   neutral   \n",
       "518  807  would bring great joy hear rendition des ree g...   neutral   \n",
       "529  837  sale owner split level home great neighborhood...  positive   \n",
       "544  870  scindia meri baat nhi suni apne congress prem ...  negative   \n",
       "561  880   dil khush krny ghalib khyal acha myth efnod8ijpj   neutral   \n",
       "569  885           ngk good one kudos heroes offl thisisysr  positive   \n",
       "576  892  officeofknath sahi hoga sayad tabhi sirf appki...   neutral   \n",
       "593  895  ez1403 pakistani pig dukkar kutta suvar mix aulad  negative   \n",
       "601  906                  bies blocked aapke acche din gaye   neutral   \n",
       "607  939  kalia thaa want hindi uneducated people nice c...   neutral   \n",
       "617  962        eid mubarak hadiya trabaho vlogs cdjaercsyo   neutral   \n",
       "\n",
       "    sentiment_pred sentiment_pred2 sentiment_pred3  \n",
       "0         negative        positive         neutral  \n",
       "13        positive        positive        positive  \n",
       "23        negative        negative        negative  \n",
       "35        negative         neutral         neutral  \n",
       "49        positive         neutral        positive  \n",
       "59        negative        negative        negative  \n",
       "69         neutral         neutral         neutral  \n",
       "75        negative        negative         neutral  \n",
       "90        negative         neutral        negative  \n",
       "103        neutral         neutral         neutral  \n",
       "108       negative        negative        negative  \n",
       "125       negative        negative        negative  \n",
       "142       positive        positive        positive  \n",
       "154        neutral        positive         neutral  \n",
       "170       negative        negative        negative  \n",
       "186        neutral        negative         neutral  \n",
       "200        neutral         neutral         neutral  \n",
       "216       negative        negative        negative  \n",
       "226        neutral         neutral         neutral  \n",
       "236        neutral         neutral         neutral  \n",
       "248       negative        negative        negative  \n",
       "267        neutral        negative         neutral  \n",
       "284       positive        positive        positive  \n",
       "300       positive        positive        positive  \n",
       "311        neutral        negative         neutral  \n",
       "327       positive        positive         neutral  \n",
       "340        neutral        negative         neutral  \n",
       "352       positive        positive        positive  \n",
       "366       positive        positive        negative  \n",
       "375       positive        positive        positive  \n",
       "385        neutral        negative        negative  \n",
       "392        neutral        negative        negative  \n",
       "401       positive        positive         neutral  \n",
       "411       positive         neutral        positive  \n",
       "429        neutral         neutral        negative  \n",
       "444       negative        negative         neutral  \n",
       "458       positive        positive        positive  \n",
       "472        neutral        positive         neutral  \n",
       "489       negative         neutral        negative  \n",
       "504        neutral         neutral         neutral  \n",
       "518       positive        positive        positive  \n",
       "529       negative        positive        positive  \n",
       "544       negative         neutral        negative  \n",
       "561       positive         neutral         neutral  \n",
       "569       positive        positive        positive  \n",
       "576       negative        negative         neutral  \n",
       "593       negative        negative        negative  \n",
       "601        neutral        negative         neutral  \n",
       "607        neutral        positive         neutral  \n",
       "617       positive        positive        positive  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6023244648780471\n",
      "0.5973333333333334\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred3,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts[['uid','sentiment_pred']].to_csv('../answer.txt',header=['Uid','Sentiment'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts['sentiment'] = test_texts.sentiment_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts[['uid','sentiment']].to_csv('../answer.txt',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
