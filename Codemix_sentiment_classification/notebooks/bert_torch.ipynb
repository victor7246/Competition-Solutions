{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open('../data/train_14k_split_conll.txt','r',encoding='utf8')\n",
    "line_train = f_train.readlines()\n",
    "\n",
    "f_val = open('../data/dev_3k_split_conll.txt','r',encoding='utf8')\n",
    "line_val = f_val.readlines()\n",
    "\n",
    "f_test = open('../data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='utf8')\n",
    "line_test = f_test.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_doc_sentiment = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                sentiment = line.split('\\t')[2]\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_doc_sentiment.append(sentiment)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    train_df['sentiment'] = train_doc_sentiment\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data(line_train)\n",
    "val_df = get_data(line_val)\n",
    "test_df = get_data_test(line_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289253, 5), (62618, 5), (61735, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3000, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.uid.nunique(), val_df.uid.nunique(), test_df.uid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281408, 5)\n",
      "(60908, 5)\n",
      "(60121, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.word != 'http']\n",
    "train_df = train_df[train_df.word != 'https']\n",
    "#train_df = train_df[train_df.word_type != 'o']\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df = val_df[val_df.word != 'http']\n",
    "val_df = val_df[val_df.word != 'https']\n",
    "#val_df = val_df[val_df.word_type != 'o']\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df = test_df[test_df.word != 'http']\n",
    "test_df = test_df[test_df.word != 'https']\n",
    "#test_df = test_df[test_df.word_type != 'o']\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\ntrain_df = train_df[train_df.word.str.len() >= 3]\\nprint (train_df.shape)\\n\\nval_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\nval_df = val_df[val_df.word.str.len() >= 3]\\nprint (val_df.shape)\\n\\ntest_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\\ntest_df = test_df[test_df.word.str.len() >= 3]\\nprint (test_df.shape)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "train_df = train_df[train_df.word.str.len() >= 3]\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "val_df = val_df[val_df.word.str.len() >= 3]\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "test_df = test_df[test_df.word.str.len() >= 3]\n",
    "print (test_df.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words 55793\n"
     ]
    }
   ],
   "source": [
    "all_words = set(pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0).word)\n",
    "print (\"Total number of words {}\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11414, 2)\n",
      "  word  tot_count\n",
      "0    .      13905\n",
      "1    /      11263\n",
      "2   //      11138\n",
      "3   co      10872\n",
      "4    â€¦       9194\n"
     ]
    }
   ],
   "source": [
    "all_words = pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0)\n",
    "all_words = all_words.word.value_counts().reset_index()\n",
    "all_words.columns = ['word','tot_count']\n",
    "top_words = all_words[all_words.tot_count >= 3]\n",
    "print (top_words.shape)\n",
    "print (top_words.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(['[PAD]','[UNK]','[CLS]','[SEP]'],columns=['word'])\n",
    "f = open('../bert-base-uncased-vocab.txt')\n",
    "ll = [i.replace('\\n','') for i in f.readlines()]\n",
    "all_bert_words = pd.DataFrame(ll,columns=['word'])\n",
    "\n",
    "all_bert_words = pd.concat([all_bert_words,top_words[['word']].drop_duplicates().reset_index(drop=True)],axis=0)\n",
    "all_bert_words = all_bert_words.drop_duplicates().reset_index(drop=True)\n",
    "all_bert_words.to_csv(\"../bert_vocab.txt\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "train_texts = pd.merge(train_texts,train_df[['uid','sentiment']],how='left').drop_duplicates().reset_index(drop=True)\n",
    "train_texts.columns = ['uid','text','sentiment']\n",
    "#train_texts.text = train_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
    "\n",
    "val_texts = val_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "val_texts = pd.merge(val_texts,val_df[['uid','sentiment']],how='left').drop_duplicates()\n",
    "val_texts.columns = ['uid','text','sentiment']\n",
    "#val_texts.text = val_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
    "\n",
    "test_texts = test_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "test_texts.columns = ['uid','text']\n",
    "#test_texts.text = test_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>rt love looks good maddie !!! ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ min _ _ lyching @ manakgupta mein kahna nae ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>best luck sir world cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>yes . great dialogues one . also chupke chupke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ tarekfatah tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment\n",
       "0    8  rt love looks good maddie !!! ako lang ba yung...   neutral\n",
       "1   14  @ min _ _ lyching @ manakgupta mein kahna nae ...   neutral\n",
       "2   26  best luck sir world cup ke liye bhot bhot subh...  positive\n",
       "3   27  yes . great dialogues one . also chupke chupke...  positive\n",
       "4   33  @ tarekfatah tu tere baap ke liye jo bola wo k...  negative"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe ho jaise pakistan wale ni karte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>rt anu's prerna way ran saving ... ram ram jay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai ye pathan nae warna # ptm nay pr b ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision . much option arm . big sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>rt keep saying kenyan rugby beautiful save kru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text\n",
       "0    1  keh aese rahe ho jaise pakistan wale ni karte ...\n",
       "1    4  rt anu's prerna way ran saving ... ram ram jay...\n",
       "2   17  shukar hai ye pathan nae warna # ptm nay pr b ...\n",
       "3   18  harsh pen decision . much option arm . big sec...\n",
       "4   34  rt keep saying kenyan rugby beautiful save kru..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     6392\n",
       "positive    5616\n",
       "negative    4992\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = pd.concat([train_texts[['uid','text','sentiment']],val_texts[['uid','text','sentiment']]],axis=0)\n",
    "all_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 11:19:33.086394 4540767680 file_utils.py:38] PyTorch version 1.2.0 available.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('../bert_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36847"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['karnal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        20.100571\n",
       "std          6.089602\n",
       "min          1.000000\n",
       "25%         16.000000\n",
       "50%         21.000000\n",
       "75%         25.000000\n",
       "max         48.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['uid'])['word'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = all_texts.sentiment.nunique() #number of possible outputs\n",
    "senti_dict = {'negative':0,'neutral':1,'positive':2}\n",
    "in_senti_dict = {0:'negative',1:'neutral',2:'positive'}\n",
    "all_texts.sentiment = all_texts.sentiment.apply(lambda x: senti_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17000/17000 [00:05<00:00, 2835.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "for text in tqdm(all_texts.text.values.tolist()):\n",
    "    train_tokens += [tokenizer.encode(text,add_special_tokens=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:01<00:00, 2648.23it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens = []\n",
    "for text in tqdm(test_texts.text.values.tolist()):\n",
    "    test_tokens += [tokenizer.encode(text,add_special_tokens=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17000/17000 [00:00<00:00, 67878.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens_padded = []\n",
    "train_attention_mask = []\n",
    "train_seg_ids = []\n",
    "\n",
    "for tokens in tqdm(train_tokens):\n",
    "    tokens = tokens[:max_len]\n",
    "    token_len = len(tokens)\n",
    "    one_mask = [1]*token_len\n",
    "    zero_mask = [0]*(max_len-token_len)\n",
    "    padded_input = tokens + zero_mask\n",
    "    attention_mask = one_mask + zero_mask\n",
    "    \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == 3:\n",
    "            current_segment_id = 1\n",
    "    segments = segments + [0] * (max_len - len(tokens))\n",
    "    \n",
    "    train_tokens_padded += [padded_input]\n",
    "    train_attention_mask += [attention_mask]\n",
    "    train_seg_ids += [segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00<00:00, 98383.17it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens_padded = []\n",
    "test_attention_mask = []\n",
    "test_seg_ids = []\n",
    "\n",
    "for tokens in tqdm(test_tokens):\n",
    "    tokens = tokens[:max_len]\n",
    "    token_len = len(tokens)\n",
    "    one_mask = [1]*token_len\n",
    "    zero_mask = [0]*(max_len-token_len)\n",
    "    padded_input = tokens + zero_mask\n",
    "    attention_mask = one_mask + zero_mask\n",
    "    \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == 102:\n",
    "            current_segment_id = 1\n",
    "    segments = segments + [0] * (max_len - len(tokens))\n",
    "    \n",
    "    test_tokens_padded += [padded_input]\n",
    "    test_attention_mask += [attention_mask]\n",
    "    test_seg_ids += [segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19387, 2293, 3504, 2204, 17805, 999, 999, 999, 31965, 11374, 8670, 32443, 37041, 37869, 32777, 2089, 37565, 4135, 36791, 1029, 2627, 1062, 19960, 7677, 2659, 14839, 1529, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (train_tokens_padded[0], train_attention_mask[0], train_seg_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = torch.LongTensor(to_categorical(all_texts.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_padded = torch.LongTensor(np.asarray(train_tokens_padded))\n",
    "train_attention_mask = torch.LongTensor(np.asarray(train_attention_mask))\n",
    "train_seg_ids = torch.LongTensor(np.asarray(train_seg_ids))\n",
    "test_tokens_padded = torch.LongTensor(np.asarray(test_tokens_padded))\n",
    "test_attention_mask = torch.LongTensor(np.asarray(test_attention_mask))\n",
    "test_seg_ids = torch.LongTensor(np.asarray(test_seg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17000, 40]) torch.Size([17000, 40]) torch.Size([17000, 40]) torch.Size([3000, 40]) torch.Size([3000, 40]) torch.Size([3000, 40])\n"
     ]
    }
   ],
   "source": [
    "print (train_tokens_padded.shape, train_attention_mask.shape, train_seg_ids.shape, test_tokens_padded.shape, test_attention_mask.shape, test_seg_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tokens_padded = train_tokens_padded[train_texts.shape[0]:]\n",
    "train_tokens_padded = train_tokens_padded[:train_texts.shape[0]]\n",
    "\n",
    "dev_attention_mask = train_attention_mask[train_texts.shape[0]:]\n",
    "train_attention_mask = train_attention_mask[:train_texts.shape[0]]\n",
    "\n",
    "dev_seg_ids = train_seg_ids[train_texts.shape[0]:]\n",
    "train_seg_ids = train_seg_ids[:train_texts.shape[0]]\n",
    "\n",
    "dev_output = train_output[train_texts.shape[0]:]\n",
    "train_output = train_output[:train_texts.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_tokens_padded, train_attention_mask, train_seg_ids, train_output)\n",
    "val_data = TensorDataset(dev_tokens_padded, dev_attention_mask, dev_seg_ids, dev_output)\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class berttuned_model(nn.Module):\n",
    "    def __init__(self, n_output=3, dropout_prob = .3, hidden_size=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_output = n_output\n",
    "        \n",
    "        config = BertConfig(vocab_size=tokenizer.vocab_size)\n",
    "        \n",
    "        self.basemodel = BertModel(config=config)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size,n_output)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, ids, attention_mask, seg_ids):\n",
    "        bert_outputs = self.basemodel(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n",
    "        out = bert_outputs[0]\n",
    "        out = torch.mean(out,1)\n",
    "        out = self.fc(self.dropout(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = berttuned_model()\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "berttuned_model(\n",
       "  (basemodel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(38126, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters to learn 115324419\n"
     ]
    }
   ],
   "source": [
    "print (\"Total number of parameters to learn {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=.00005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if device == 'cpu':\n",
    "        rounded_preds = preds.detach().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().numpy().argmax(1)\n",
    "    else:\n",
    "        rounded_preds = preds.detach().cpu().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().cpu().numpy().argmax(1)\n",
    "    \n",
    "    return accuracy_score(rounded_correct,rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_torch(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if device == 'cpu':\n",
    "        rounded_preds = preds.detach().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().numpy().argmax(1)\n",
    "    else:\n",
    "        rounded_preds = preds.detach().cpu().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().cpu().numpy().argmax(1)\n",
    "    \n",
    "    return f1_score(rounded_correct,rounded_preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    global predictions, labels, loss\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    f1_scores = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    counter = 0\n",
    "    for tokens, attn, seg, labels in tqdm(train_loader):\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(tokens, attn, seg) #.squeeze(1)\n",
    "        predictions = torch.softmax(predictions,dim=-1)\n",
    "        \n",
    "        #loss = criterion(predictions, labels)\n",
    "        loss = criterion(predictions, torch.max(labels, 1)[1])\n",
    "        \n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        f1_score_batch = f1_torch(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        f1_scores += f1_score_batch\n",
    "        \n",
    "    return epoch_loss / counter, epoch_acc / counter, f1_scores/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    f1_scores = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for tokens, attn, seg, labels in tqdm(val_loader):\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "            predictions = model(tokens, attn, seg) #.squeeze(1)\n",
    "            predictions = torch.softmax(predictions,dim=-1)\n",
    "            \n",
    "            #loss = criterion(predictions, labels)\n",
    "            loss = criterion(predictions, torch.max(labels, 1)[1])\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            f1_score_batch = f1_torch(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            f1_scores += f1_score_batch\n",
    "        \n",
    "    return epoch_loss / counter, epoch_acc / counter, f1_scores/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:02:17<00:00, 33.98s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:34<00:00,  6.44s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 64m 52s\n",
      "\tTrain Loss: 1.135 | Train Acc: 35.52% | Train F1: 21.48%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 38.59% | Val F1: 26.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [2:20:11<00:00, 76.47s/it]    \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [04:12<00:00, 10.53s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 144m 24s\n",
      "\tTrain Loss: 1.100 | Train Acc: 36.65% | Train F1: 28.58%\n",
      "\t Val. Loss: 1.082 |  Val. Acc: 38.09% | Val F1: 20.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:25:54<00:00, 46.86s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [04:07<00:00, 10.30s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 90m 2s\n",
      "\tTrain Loss: 1.053 | Train Acc: 43.12% | Train F1: 38.16%\n",
      "\t Val. Loss: 1.004 |  Val. Acc: 49.28% | Val F1: 45.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:25:25<00:00, 46.60s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [04:05<00:00, 10.22s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 89m 31s\n",
      "\tTrain Loss: 0.951 | Train Acc: 57.86% | Train F1: 57.26%\n",
      "\t Val. Loss: 0.964 |  Val. Acc: 56.30% | Val F1: 54.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:25:20<00:00, 46.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:34<00:00,  6.44s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 87m 55s\n",
      "\tTrain Loss: 0.894 | Train Acc: 64.22% | Train F1: 64.09%\n",
      "\t Val. Loss: 0.953 |  Val. Acc: 57.31% | Val F1: 57.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:22:22<00:00, 44.93s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:34<00:00,  6.45s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 84m 57s\n",
      "\tTrain Loss: 0.867 | Train Acc: 67.62% | Train F1: 67.60%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 47.67% | Val F1: 45.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [1:22:01<00:00, 44.74s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:34<00:00,  6.44s/it]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 84m 36s\n",
      "\tTrain Loss: 0.923 | Train Acc: 62.01% | Train F1: 61.13%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 38.43% | Val F1: 30.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 18/110 [13:27<1:10:42, 46.12s/it]I0227 22:21:39.000735 4540767680 utils.py:141] NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b6015734e66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ba70e9181ce7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = 999\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, val_loader, criterion)\n",
    "       \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '../models/model_bert.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val F1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
