{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open('../data/train_14k_split_conll.txt','r',encoding='utf8')\n",
    "line_train = f_train.readlines()\n",
    "\n",
    "f_val = open('../data/dev_3k_split_conll.txt','r',encoding='utf8')\n",
    "line_val = f_val.readlines()\n",
    "\n",
    "f_test = open('../data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='utf8')\n",
    "line_test = f_test.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_doc_sentiment = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                sentiment = line.split('\\t')[2]\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_doc_sentiment.append(sentiment)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    train_df['sentiment'] = train_doc_sentiment\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                 \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data(line_train)\n",
    "val_df = get_data(line_val)\n",
    "test_df = get_data_test(line_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289253, 5), (62618, 5), (61735, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3000, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.uid.nunique(), val_df.uid.nunique(), test_df.uid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>word</th>\n",
       "      <th>word_type</th>\n",
       "      <th>uid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289248</th>\n",
       "      <td>14000</td>\n",
       "      <td>//</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289249</th>\n",
       "      <td>14000</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289250</th>\n",
       "      <td>14000</td>\n",
       "      <td>co</td>\n",
       "      <td>eng</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289251</th>\n",
       "      <td>14000</td>\n",
       "      <td>/</td>\n",
       "      <td>o</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289252</th>\n",
       "      <td>14000</td>\n",
       "      <td>cs3vtzop3q</td>\n",
       "      <td>eng</td>\n",
       "      <td>3308</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_id        word word_type   uid sentiment\n",
       "289248   14000          //         o  3308   neutral\n",
       "289249   14000           .         o  3308   neutral\n",
       "289250   14000          co       eng  3308   neutral\n",
       "289251   14000           /         o  3308   neutral\n",
       "289252   14000  cs3vtzop3q       eng  3308   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223215, 5)\n",
      "(48339, 5)\n",
      "(47683, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.word != 'http']\n",
    "train_df = train_df[train_df.word != 'https']\n",
    "train_df = train_df[train_df.word_type != 'o']\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df = val_df[val_df.word != 'http']\n",
    "val_df = val_df[val_df.word != 'https']\n",
    "val_df = val_df[val_df.word_type != 'o']\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df = test_df[test_df.word != 'http']\n",
    "test_df = test_df[test_df.word != 'https']\n",
    "test_df = test_df[test_df.word_type != 'o']\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175903, 5)\n",
      "(38082, 5)\n",
      "(37583, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "train_df = train_df[train_df.word.str.len() >= 3]\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "val_df = val_df[val_df.word.str.len() >= 3]\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "test_df = test_df[test_df.word.str.len() >= 3]\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        12.564500\n",
       "std          3.873212\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         13.000000\n",
       "75%         15.000000\n",
       "max         25.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['uid'])['word'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean       12.694000\n",
       "std         3.839177\n",
       "min         2.000000\n",
       "25%        10.000000\n",
       "50%        13.000000\n",
       "75%        15.000000\n",
       "max        24.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.groupby(['uid'])['word'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean       12.527667\n",
       "std         3.855756\n",
       "min         2.000000\n",
       "25%        10.000000\n",
       "50%        13.000000\n",
       "75%        15.000000\n",
       "max        23.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby(['uid'])['word'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "train_texts = pd.merge(train_texts,train_df[['uid','sentiment']],how='left').drop_duplicates().reset_index(drop=True)\n",
    "train_texts.columns = ['uid','text','sentiment']\n",
    "\n",
    "val_texts = val_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "val_texts = pd.merge(val_texts,val_df[['uid','sentiment']],how='left').drop_duplicates()\n",
    "val_texts.columns = ['uid','text','sentiment']\n",
    "\n",
    "test_texts = test_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "test_texts.columns = ['uid','text']\n",
    "\n",
    "pd.concat([train_texts[['text']],val_texts[['text']],test_texts[['text']]], axis=0).to_csv('../data/alltexts.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  51516\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    9929 lr:  0.000000 loss:  2.133549 ETA:   0h 0m\n"
     ]
    }
   ],
   "source": [
    "! ../models/fastText-0.2.0/fasttext skipgram -input ../data/alltexts.txt -output ../models/fasttext -minn 2 -maxn 20 -dim 200 -lr .1 -minCount 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! echo \"modi\" | ../models/fastText-0.2.0/fasttext print-word-vectors ../models/fasttext.bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14000, 3), (3000, 3), (3000, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>love looks good maddie ako lang yung sobrang m...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>min lyching manakgupta mein kahna nae chahta q...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>best luck sir world cup liye bhot bhot subhkam...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>yes great dialogues one also chupke chupke chh...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>tarekfatah tere baap liye bola kya tha bhadwe ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>desh bhakti baat wahi samajh sakte hai khud de...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>madarchod mulle mathura nahi dikha tha jab mul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>manya pradhan mantri mahoday shriman narendra ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>nsharif kiya tum apne baap oulad nahi kyun usy...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>youtube rha nhi modi phir ban gya chupchaap aa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment\n",
       "0    8  love looks good maddie ako lang yung sobrang m...   neutral\n",
       "1   14  min lyching manakgupta mein kahna nae chahta q...   neutral\n",
       "2   26  best luck sir world cup liye bhot bhot subhkam...  positive\n",
       "3   27  yes great dialogues one also chupke chupke chh...  positive\n",
       "4   33  tarekfatah tere baap liye bola kya tha bhadwe ...  negative\n",
       "5   38  desh bhakti baat wahi samajh sakte hai khud de...  negative\n",
       "6   41  madarchod mulle mathura nahi dikha tha jab mul...  negative\n",
       "7   48  manya pradhan mantri mahoday shriman narendra ...  positive\n",
       "8   56  nsharif kiya tum apne baap oulad nahi kyun usy...  negative\n",
       "9   59  youtube rha nhi modi phir ban gya chupchaap aa...   neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>ankita shah8 bhadve photo elections pehle jyad...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>krishna jcb full trend chal rahi</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>sharma1 kavita sharma4 sunita kal jab evm seal...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>101</td>\n",
       "      <td>nolo weni ankere gae weekend</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment\n",
       "0      3  pakistan ghra tauq pakistan israel tasleem nah...  negative\n",
       "13    12  gonna start another june sour note uhhhh yes y...   neutral\n",
       "23    23  caring bohot jyada caring courier wale bsdk si...  negative\n",
       "35    24  sarfaraza nonesense kabhi baymani per bani tea...  positive\n",
       "49    45  pakistani team effort aagey allah marziiiiiiii...  positive\n",
       "59    61  ankita shah8 bhadve photo elections pehle jyad...  negative\n",
       "69    64                   krishna jcb full trend chal rahi  positive\n",
       "75    77  sharma1 kavita sharma4 sunita kal jab evm seal...  negative\n",
       "90    81  accha kiya invite nai kiya corrupted party ind...  negative\n",
       "103  101                       nolo weni ankere gae weekend  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe jaise pakistan wale karte south ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>anus prerna way ran saving ram ram jay rajaraa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai pathan nae warna ptm nay wae dramay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision much option arm big second ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>keep saying kenyan rugby beautiful save kru dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>guru give bless sewa simrn parmarth good deeds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>136</td>\n",
       "      <td>tum logo zindagi mai kabhi india map nahi dhek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>139</td>\n",
       "      <td>itne gandi priye aapke profile pic modi lga le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>158</td>\n",
       "      <td>google translate working aisi jagah net aata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>qaira sahb great man apka beta nhayat nfees na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text\n",
       "0    1  keh aese rahe jaise pakistan wale karte south ...\n",
       "1    4  anus prerna way ran saving ram ram jay rajaraa...\n",
       "2   17  shukar hai pathan nae warna ptm nay wae dramay...\n",
       "3   18  harsh pen decision much option arm big second ...\n",
       "4   34  keep saying kenyan rugby beautiful save kru dr...\n",
       "5   43  guru give bless sewa simrn parmarth good deeds...\n",
       "6  136  tum logo zindagi mai kabhi india map nahi dhek...\n",
       "7  139  itne gandi priye aapke profile pic modi lga le...\n",
       "8  158       google translate working aisi jagah net aata\n",
       "9  162  qaira sahb great man apka beta nhayat nfees na..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words 51515\n"
     ]
    }
   ],
   "source": [
    "all_words = set(pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0).word)\n",
    "print (\"Total number of words {}\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in all_words:\n",
    "            data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51516it [00:02, 21990.61it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index1 = load_vectors('../models/fasttext.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999994it [00:20, 49257.39it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index2 = load_vectors('../models/wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim1 = 200\n",
    "embed_dim2 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Conv1D, Input, Concatenate, Dropout\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import initializers\n",
    "import keras.backend as K\n",
    "from keras.engine import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = all_words.word.value_counts().reset_index()\n",
    "all_words.columns = ['word','tot_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16771, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[all_words.tot_count >= 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 16000\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.concat([train_texts[['uid','text','sentiment']],val_texts[['uid','text','sentiment']]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     6392\n",
       "positive    5616\n",
       "negative    4992\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out = 128 #dimension of the lstm output \n",
    "n_output = all_texts.sentiment.nunique() #number of possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_dict = {'negative':0,'neutral':1,'positive':2}\n",
    "in_senti_dict = {0:'negative',1:'neutral',2:'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts.sentiment = all_texts.sentiment.apply(lambda x: senti_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='UNK', num_words=max_fatures+1)\n",
    "tokenizer.fit_on_texts(all_texts.text.values.tolist())\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= max_fatures}\n",
    "#tokenizer.word_index[tokenizer.oov_token] = max_fatures + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(all_texts.text.values)\n",
    "X = pad_sequences(X,maxlen=max_len,padding=\"post\")\n",
    "y = pd.get_dummies(all_texts.sentiment).values\n",
    "\n",
    "testX = tokenizer.texts_to_sequences(test_texts.text.values)\n",
    "testX = pad_sequences(testX,maxlen=max_len,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 (17000, 20) (3000, 20)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print (len(word_index),X.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " 'hai': 2,\n",
       " 'nahi': 3,\n",
       " 'bhi': 4,\n",
       " 'aur': 5,\n",
       " 'modi': 6,\n",
       " 'aap': 7,\n",
       " 'sir': 8,\n",
       " 'koi': 9,\n",
       " 'nhi': 10,\n",
       " 'love': 11,\n",
       " 'bjp': 12,\n",
       " 'hain': 13,\n",
       " 'good': 14,\n",
       " 'kar': 15,\n",
       " 'kya': 16,\n",
       " 'desh': 17,\n",
       " 'bahut': 18,\n",
       " 'tum': 19,\n",
       " 'tha': 20,\n",
       " 'log': 21,\n",
       " 'best': 22,\n",
       " 'happy': 23,\n",
       " 'bhai': 24,\n",
       " 'india': 25,\n",
       " 'pakistan': 26,\n",
       " 'jai': 27,\n",
       " 'kuch': 28,\n",
       " 'congress': 29,\n",
       " 'par': 30,\n",
       " 'mein': 31,\n",
       " 'diya': 32,\n",
       " 'liye': 33,\n",
       " 'main': 34,\n",
       " 'allah': 35,\n",
       " 'apne': 36,\n",
       " 'yeh': 37,\n",
       " 'mai': 38,\n",
       " 'khud': 39,\n",
       " 'toh': 40,\n",
       " 'one': 41,\n",
       " 'har': 42,\n",
       " 'rahe': 43,\n",
       " 'bharat': 44,\n",
       " 'gaya': 45,\n",
       " 'hindu': 46,\n",
       " 'ram': 47,\n",
       " 'media': 48,\n",
       " 'birthday': 49,\n",
       " 'like': 50,\n",
       " 'jaise': 51,\n",
       " 'sab': 52,\n",
       " 'aaj': 53,\n",
       " 'logo': 54,\n",
       " 'hum': 55,\n",
       " 'world': 56,\n",
       " 'chor': 57,\n",
       " 'teri': 58,\n",
       " 'maa': 59,\n",
       " 'kisi': 60,\n",
       " 'kiya': 61,\n",
       " 'raha': 62,\n",
       " 'day': 63,\n",
       " 'jab': 64,\n",
       " 'agar': 65,\n",
       " 'baat': 66,\n",
       " 'thank': 67,\n",
       " 'hoga': 68,\n",
       " 'great': 69,\n",
       " 'tere': 70,\n",
       " 'congratulations': 71,\n",
       " 'jeet': 72,\n",
       " 'dil': 73,\n",
       " 'tak': 74,\n",
       " 'hua': 75,\n",
       " 'kabhi': 76,\n",
       " 'muslim': 77,\n",
       " 'bless': 78,\n",
       " 'sath': 79,\n",
       " 'apni': 80,\n",
       " 'iss': 81,\n",
       " 'wale': 82,\n",
       " 'party': 83,\n",
       " 'may': 84,\n",
       " 'sirf': 85,\n",
       " 'bad': 86,\n",
       " 'team': 87,\n",
       " 'hota': 88,\n",
       " 'band': 89,\n",
       " 'vote': 90,\n",
       " 'much': 91,\n",
       " 'hoti': 92,\n",
       " 'mat': 93,\n",
       " 'kyu': 94,\n",
       " 'see': 95,\n",
       " 'time': 96,\n",
       " 'kaam': 97,\n",
       " 'baap': 98,\n",
       " 'narendramodi': 99,\n",
       " 'morning': 100,\n",
       " 'naam': 101,\n",
       " 'aapko': 102,\n",
       " 'tco': 103,\n",
       " 'apna': 104,\n",
       " 'tera': 105,\n",
       " 'din': 106,\n",
       " 'always': 107,\n",
       " 'hamare': 108,\n",
       " 'cup': 109,\n",
       " 'life': 110,\n",
       " 'mere': 111,\n",
       " 'karne': 112,\n",
       " 'meri': 113,\n",
       " 'god': 114,\n",
       " 'kam': 115,\n",
       " 'karo': 116,\n",
       " 'news': 117,\n",
       " 'new': 118,\n",
       " 'wala': 119,\n",
       " 'thanks': 120,\n",
       " 'well': 121,\n",
       " 'pata': 122,\n",
       " 'sarkar': 123,\n",
       " 'tarah': 124,\n",
       " 'per': 125,\n",
       " 'lekin': 126,\n",
       " 'sale': 127,\n",
       " 'song': 128,\n",
       " 'really': 129,\n",
       " 'phir': 130,\n",
       " 'thi': 131,\n",
       " 'apko': 132,\n",
       " 'janta': 133,\n",
       " 'mera': 134,\n",
       " 'chahiye': 135,\n",
       " 'people': 136,\n",
       " 'rha': 137,\n",
       " 'fake': 138,\n",
       " 'hope': 139,\n",
       " 'movie': 140,\n",
       " 'baar': 141,\n",
       " 'hote': 142,\n",
       " 'khan': 143,\n",
       " 'saal': 144,\n",
       " 'say': 145,\n",
       " 'shri': 146,\n",
       " 'bar': 147,\n",
       " 'sahi': 148,\n",
       " 'abhi': 149,\n",
       " 'bol': 150,\n",
       " 'aise': 151,\n",
       " 'proud': 152,\n",
       " 'please': 153,\n",
       " 'badhai': 154,\n",
       " 'pak': 155,\n",
       " 'ban': 156,\n",
       " 'karte': 157,\n",
       " 'subhkamnaye': 158,\n",
       " 'rahi': 159,\n",
       " 'rahul': 160,\n",
       " 'gaye': 161,\n",
       " 'mar': 162,\n",
       " 'wish': 163,\n",
       " 'nai': 164,\n",
       " 'know': 165,\n",
       " 'better': 166,\n",
       " 'chutiye': 167,\n",
       " 'aapki': 168,\n",
       " 'gandhi': 169,\n",
       " 'kutte': 170,\n",
       " 'mubarak': 171,\n",
       " 'itna': 172,\n",
       " 'kharab': 173,\n",
       " 'election': 174,\n",
       " 'jay': 175,\n",
       " 'minister': 176,\n",
       " 'dhan': 177,\n",
       " 'pehle': 178,\n",
       " 'shree': 179,\n",
       " 'fir': 180,\n",
       " 'chutiya': 181,\n",
       " 'khush': 182,\n",
       " 'video': 183,\n",
       " 'nice': 184,\n",
       " 'even': 185,\n",
       " 'bana': 186,\n",
       " 'sabka': 187,\n",
       " 'karta': 188,\n",
       " 'sala': 189,\n",
       " 'yahi': 190,\n",
       " 'dear': 191,\n",
       " 'fan': 192,\n",
       " 'sabse': 193,\n",
       " 'dekh': 194,\n",
       " 'miss': 195,\n",
       " 'get': 196,\n",
       " 'sakta': 197,\n",
       " 'hone': 198,\n",
       " 'papa': 199,\n",
       " 'problem': 200,\n",
       " 'evm': 201,\n",
       " 'baad': 202,\n",
       " 'many': 203,\n",
       " 'ham': 204,\n",
       " 'bas': 205,\n",
       " 'mujhe': 206,\n",
       " 'shame': 207,\n",
       " 'abe': 208,\n",
       " 'aapke': 209,\n",
       " 'tweet': 210,\n",
       " 'family': 211,\n",
       " 'modiji': 212,\n",
       " 'back': 213,\n",
       " 'itni': 214,\n",
       " 'lagta': 215,\n",
       " 'jis': 216,\n",
       " 'beautiful': 217,\n",
       " 'gya': 218,\n",
       " 'today': 219,\n",
       " 'saale': 220,\n",
       " 'hind': 221,\n",
       " 'app': 222,\n",
       " 'aisa': 223,\n",
       " 'indian': 224,\n",
       " 'bhut': 225,\n",
       " 'match': 226,\n",
       " 'show': 227,\n",
       " 'free': 228,\n",
       " 'karna': 229,\n",
       " 'eid': 230,\n",
       " 'bada': 231,\n",
       " 'kal': 232,\n",
       " 'kaha': 233,\n",
       " 'super': 234,\n",
       " 'narendra': 235,\n",
       " 'want': 236,\n",
       " 'insan': 237,\n",
       " 'apke': 238,\n",
       " 'sewa': 239,\n",
       " 'plz': 240,\n",
       " 'kare': 241,\n",
       " 'gayi': 242,\n",
       " 'support': 243,\n",
       " 'last': 244,\n",
       " 'rhe': 245,\n",
       " 'dete': 246,\n",
       " 'right': 247,\n",
       " 'saath': 248,\n",
       " 'sorry': 249,\n",
       " 'chal': 250,\n",
       " 'aapne': 251,\n",
       " 'sakte': 252,\n",
       " 'man': 253,\n",
       " 'feel': 254,\n",
       " 'laga': 255,\n",
       " 'apka': 256,\n",
       " 'woh': 257,\n",
       " 'first': 258,\n",
       " 'bat': 259,\n",
       " 'ghar': 260,\n",
       " 'baba': 261,\n",
       " 'big': 262,\n",
       " 'kia': 263,\n",
       " 'acha': 264,\n",
       " 'itne': 265,\n",
       " 'true': 266,\n",
       " 'hardik': 267,\n",
       " 'govt': 268,\n",
       " 'rahulgandhi': 269,\n",
       " 'fuck': 270,\n",
       " 'jyada': 271,\n",
       " 'dena': 272,\n",
       " 'cant': 273,\n",
       " 'give': 274,\n",
       " 'kitna': 275,\n",
       " 'made': 276,\n",
       " 'aik': 277,\n",
       " 'shah': 278,\n",
       " 'lag': 279,\n",
       " 'follow': 280,\n",
       " 'yes': 281,\n",
       " 'sabhi': 282,\n",
       " 'luck': 283,\n",
       " 'kyun': 284,\n",
       " 'hui': 285,\n",
       " 'isko': 286,\n",
       " 'police': 287,\n",
       " 'karke': 288,\n",
       " 'work': 289,\n",
       " 'pass': 290,\n",
       " 'hun': 291,\n",
       " 'hona': 292,\n",
       " 'hue': 293,\n",
       " 'nahin': 294,\n",
       " 'think': 295,\n",
       " 'tab': 296,\n",
       " 'also': 297,\n",
       " 'mantri': 298,\n",
       " 'cute': 299,\n",
       " 'way': 300,\n",
       " 'lol': 301,\n",
       " 'dekho': 302,\n",
       " 'vikas': 303,\n",
       " 'kutta': 304,\n",
       " 'deta': 305,\n",
       " 'use': 306,\n",
       " 'film': 307,\n",
       " 'krta': 308,\n",
       " 'hay': 309,\n",
       " 'parmarth': 310,\n",
       " 'kha': 311,\n",
       " 'post': 312,\n",
       " 'look': 313,\n",
       " 'jaye': 314,\n",
       " 'pure': 315,\n",
       " 'duniya': 316,\n",
       " 'job': 317,\n",
       " 'walo': 318,\n",
       " 'sari': 319,\n",
       " 'friends': 320,\n",
       " 'mam': 321,\n",
       " 'warna': 322,\n",
       " 'jaan': 323,\n",
       " 'every': 324,\n",
       " 'public': 325,\n",
       " 'night': 326,\n",
       " 'year': 327,\n",
       " 'bilkul': 328,\n",
       " 'sweet': 329,\n",
       " 'wah': 330,\n",
       " 'haar': 331,\n",
       " 'satguru': 332,\n",
       " 'make': 333,\n",
       " 'keep': 334,\n",
       " 'krne': 335,\n",
       " 'leader': 336,\n",
       " 'hein': 337,\n",
       " 'kro': 338,\n",
       " 'agr': 339,\n",
       " 'kahi': 340,\n",
       " 'still': 341,\n",
       " 'ever': 342,\n",
       " 'kon': 343,\n",
       " 'guys': 344,\n",
       " 'girl': 345,\n",
       " 'neta': 346,\n",
       " 'nikal': 347,\n",
       " 'twitter': 348,\n",
       " 'kaise': 349,\n",
       " 'army': 350,\n",
       " 'need': 351,\n",
       " 'guru': 352,\n",
       " 'month': 353,\n",
       " 'matlab': 354,\n",
       " 'got': 355,\n",
       " 'logon': 356,\n",
       " 'wali': 357,\n",
       " 'liya': 358,\n",
       " 'home': 359,\n",
       " 'wishes': 360,\n",
       " 'debate': 361,\n",
       " 'yaha': 362,\n",
       " 'person': 363,\n",
       " 'apki': 364,\n",
       " 'bro': 365,\n",
       " 'uske': 366,\n",
       " 'never': 367,\n",
       " 'didi': 368,\n",
       " 'friend': 369,\n",
       " 'thing': 370,\n",
       " 'soch': 371,\n",
       " 'harami': 372,\n",
       " 'ker': 373,\n",
       " 'pura': 374,\n",
       " 'randi': 375,\n",
       " 'incindia': 376,\n",
       " 'bus': 377,\n",
       " 'special': 378,\n",
       " 'singh': 379,\n",
       " 'pakistani': 380,\n",
       " 'years': 381,\n",
       " 'hoon': 382,\n",
       " 'aapka': 383,\n",
       " 'tumhare': 384,\n",
       " 'mei': 385,\n",
       " 'kab': 386,\n",
       " 'pappu': 387,\n",
       " 'samajh': 388,\n",
       " 'paisa': 389,\n",
       " 'iska': 390,\n",
       " 'pahle': 391,\n",
       " 'delhi': 392,\n",
       " 'tou': 393,\n",
       " 'jail': 394,\n",
       " 'salman': 395,\n",
       " 'sad': 396,\n",
       " 'unke': 397,\n",
       " 'wrong': 398,\n",
       " 'mata': 399,\n",
       " 'bjp4india': 400,\n",
       " 'amit': 401,\n",
       " 'pta': 402,\n",
       " 'uski': 403,\n",
       " 'hindi': 404,\n",
       " 'galat': 405,\n",
       " 'come': 406,\n",
       " 'sahab': 407,\n",
       " 'hogi': 408,\n",
       " 'bola': 409,\n",
       " 'dua': 410,\n",
       " 'ramadan': 411,\n",
       " 'watch': 412,\n",
       " 'take': 413,\n",
       " 'sharma': 414,\n",
       " 'bata': 415,\n",
       " 'mulk': 416,\n",
       " 'gyi': 417,\n",
       " 'done': 418,\n",
       " 'hate': 419,\n",
       " 'kahan': 420,\n",
       " 'bahar': 421,\n",
       " 'bohot': 422,\n",
       " 'hit': 423,\n",
       " 'aage': 424,\n",
       " 'national': 425,\n",
       " 'aisi': 426,\n",
       " 'cricket': 427,\n",
       " 'bade': 428,\n",
       " 'kaun': 429,\n",
       " 'usko': 430,\n",
       " 'country': 431,\n",
       " 'hamesha': 432,\n",
       " 'shit': 433,\n",
       " 'lanat': 434,\n",
       " 'tweets': 435,\n",
       " 'bolte': 436,\n",
       " 'next': 437,\n",
       " 'amazing': 438,\n",
       " 'kuchh': 439,\n",
       " 'win': 440,\n",
       " 'heart': 441,\n",
       " 'sach': 442,\n",
       " 'dekha': 443,\n",
       " 'dono': 444,\n",
       " 'raho': 445,\n",
       " 'start': 446,\n",
       " 'mandir': 447,\n",
       " 'fail': 448,\n",
       " 'dont': 449,\n",
       " 'jaisa': 450,\n",
       " 'maine': 451,\n",
       " 'everyone': 452,\n",
       " 'maar': 453,\n",
       " 'kay': 454,\n",
       " 'rhi': 455,\n",
       " 'jata': 456,\n",
       " 'badi': 457,\n",
       " 'dia': 458,\n",
       " 'yaar': 459,\n",
       " 'aaye': 460,\n",
       " 'ata': 461,\n",
       " 'president': 462,\n",
       " 'halat': 463,\n",
       " 'bina': 464,\n",
       " 'live': 465,\n",
       " 'respect': 466,\n",
       " 'tho': 467,\n",
       " 'bakwas': 468,\n",
       " 'chod': 469,\n",
       " 'waise': 470,\n",
       " 'thats': 471,\n",
       " 'islam': 472,\n",
       " 'pic': 473,\n",
       " 'hamari': 474,\n",
       " 'real': 475,\n",
       " 'hame': 476,\n",
       " 'sub': 477,\n",
       " 'days': 478,\n",
       " 'jagah': 479,\n",
       " 'bacha': 480,\n",
       " 'uska': 481,\n",
       " 'haram': 482,\n",
       " 'boy': 483,\n",
       " 'congrats': 484,\n",
       " 'sare': 485,\n",
       " 'aapse': 486,\n",
       " 'jao': 487,\n",
       " 'help': 488,\n",
       " 'channel': 489,\n",
       " 'hath': 490,\n",
       " 'wait': 491,\n",
       " 'isse': 492,\n",
       " 'yeah': 493,\n",
       " 'jayega': 494,\n",
       " 'haha': 495,\n",
       " 'hey': 496,\n",
       " 'aukat': 497,\n",
       " 'mil': 498,\n",
       " 'achi': 499,\n",
       " 'iski': 500,\n",
       " 'taraf': 501,\n",
       " 'power': 502,\n",
       " 'zindagi': 503,\n",
       " 'social': 504,\n",
       " 'prime': 505,\n",
       " 'madam': 506,\n",
       " 'amitshah': 507,\n",
       " 'unko': 508,\n",
       " 'sal': 509,\n",
       " 'musalman': 510,\n",
       " 'paise': 511,\n",
       " 'usse': 512,\n",
       " 'going': 513,\n",
       " 'ind': 514,\n",
       " 'name': 515,\n",
       " 'chunav': 516,\n",
       " 'sun': 517,\n",
       " 'tumhe': 518,\n",
       " 'ali': 519,\n",
       " 'looking': 520,\n",
       " 'yaad': 521,\n",
       " 'would': 522,\n",
       " 'khatam': 523,\n",
       " 'inko': 524,\n",
       " 'iske': 525,\n",
       " 'mind': 526,\n",
       " 'hindustan': 527,\n",
       " 'pita': 528,\n",
       " 'khushi': 529,\n",
       " 'aasra': 530,\n",
       " 'hume': 531,\n",
       " 'dekhna': 532,\n",
       " 'blessed': 533,\n",
       " 'unhe': 534,\n",
       " 'gai': 535,\n",
       " 'beta': 536,\n",
       " 'soon': 537,\n",
       " 'nam': 538,\n",
       " 'change': 539,\n",
       " 'hahaha': 540,\n",
       " 'pyar': 541,\n",
       " 'unki': 542,\n",
       " 'aata': 543,\n",
       " 'karega': 544,\n",
       " 'account': 545,\n",
       " 'bht': 546,\n",
       " 'naa': 547,\n",
       " 'rss': 548,\n",
       " 'jhoot': 549,\n",
       " 'excited': 550,\n",
       " 'kis': 551,\n",
       " 'tumhari': 552,\n",
       " 'inke': 553,\n",
       " 'tujhe': 554,\n",
       " 'bolne': 555,\n",
       " 'yahan': 556,\n",
       " 'aaya': 557,\n",
       " 'kyo': 558,\n",
       " 'fans': 559,\n",
       " 'dekhne': 560,\n",
       " 'galti': 561,\n",
       " 'wow': 562,\n",
       " 'beti': 563,\n",
       " 'thik': 564,\n",
       " 'lost': 565,\n",
       " 'school': 566,\n",
       " 'sure': 567,\n",
       " 'opposition': 568,\n",
       " 'bengal': 569,\n",
       " 'hamara': 570,\n",
       " 'bhakt': 571,\n",
       " 'stay': 572,\n",
       " 'full': 573,\n",
       " 'keh': 574,\n",
       " 'jee': 575,\n",
       " 'banne': 576,\n",
       " 'pagal': 577,\n",
       " 'fun': 578,\n",
       " 'krishna': 579,\n",
       " 'bhagwan': 580,\n",
       " 'mamta': 581,\n",
       " 'aulad': 582,\n",
       " 'nazar': 583,\n",
       " 'dega': 584,\n",
       " 'bahot': 585,\n",
       " 'bohat': 586,\n",
       " 'mamataofficial': 587,\n",
       " 'pls': 588,\n",
       " 'seat': 589,\n",
       " 'krna': 590,\n",
       " 'said': 591,\n",
       " 'says': 592,\n",
       " 'aane': 593,\n",
       " 'kiye': 594,\n",
       " 'sri': 595,\n",
       " 'jarur': 596,\n",
       " 'puri': 597,\n",
       " 'naya': 598,\n",
       " 'mila': 599,\n",
       " 'han': 600,\n",
       " 'jese': 601,\n",
       " 'gye': 602,\n",
       " 'dene': 603,\n",
       " 'aati': 604,\n",
       " 'hon': 605,\n",
       " 'tabhi': 606,\n",
       " 'samaj': 607,\n",
       " 'muslims': 608,\n",
       " 'wajah': 609,\n",
       " 'long': 610,\n",
       " 'jaldi': 611,\n",
       " 'request': 612,\n",
       " 'tumko': 613,\n",
       " 'gurmeetramrahim': 614,\n",
       " 'politics': 615,\n",
       " 'kyuki': 616,\n",
       " 'ready': 617,\n",
       " 'kch': 618,\n",
       " 'diye': 619,\n",
       " 'coming': 620,\n",
       " 'government': 621,\n",
       " 'imran': 622,\n",
       " 'fav': 623,\n",
       " 'karenge': 624,\n",
       " 'without': 625,\n",
       " 'actor': 626,\n",
       " 'lot': 627,\n",
       " 'bura': 628,\n",
       " 'words': 629,\n",
       " 'bal': 630,\n",
       " 'cool': 631,\n",
       " 'achha': 632,\n",
       " 'part': 633,\n",
       " 'care': 634,\n",
       " 'bhaiya': 635,\n",
       " 'mile': 636,\n",
       " 'ache': 637,\n",
       " 'likha': 638,\n",
       " 'gaali': 639,\n",
       " 'gaand': 640,\n",
       " 'dalal': 641,\n",
       " 'exam': 642,\n",
       " 'jayegi': 643,\n",
       " 'pay': 644,\n",
       " 'muh': 645,\n",
       " 'pyaar': 646,\n",
       " 'raj': 647,\n",
       " 'sharam': 648,\n",
       " 'gandi': 649,\n",
       " 'hard': 650,\n",
       " 'jumma': 651,\n",
       " 'simran': 652,\n",
       " 'aadmi': 653,\n",
       " 'jcb': 654,\n",
       " 'accha': 655,\n",
       " 'bole': 656,\n",
       " 'madarchod': 657,\n",
       " 'banaya': 658,\n",
       " 'khane': 659,\n",
       " 'kafi': 660,\n",
       " 'insaan': 661,\n",
       " 'pakvwi': 662,\n",
       " 'jane': 663,\n",
       " 'raat': 664,\n",
       " 'cwc19': 665,\n",
       " 'jate': 666,\n",
       " 'sakti': 667,\n",
       " 'sumiran': 668,\n",
       " 'dher': 669,\n",
       " 'captain': 670,\n",
       " 'cheez': 671,\n",
       " 'bollywood': 672,\n",
       " 'strong': 673,\n",
       " 'jati': 674,\n",
       " 'aam': 675,\n",
       " 'bhar': 676,\n",
       " 'rakh': 677,\n",
       " 'mom': 678,\n",
       " 'chup': 679,\n",
       " 'chahta': 680,\n",
       " 'bhot': 681,\n",
       " 'mara': 682,\n",
       " 'pradhan': 683,\n",
       " 'future': 684,\n",
       " 'chori': 685,\n",
       " 'lovely': 686,\n",
       " 'someone': 687,\n",
       " 'piche': 688,\n",
       " 'pad': 689,\n",
       " 'flop': 690,\n",
       " 'remember': 691,\n",
       " 'bolta': 692,\n",
       " 'isliye': 693,\n",
       " 'dikha': 694,\n",
       " 'already': 695,\n",
       " 'bsdk': 696,\n",
       " 'ise': 697,\n",
       " 'awesome': 698,\n",
       " 'aya': 699,\n",
       " 'bolna': 700,\n",
       " 'paas': 701,\n",
       " 'gonna': 702,\n",
       " 'hehe': 703,\n",
       " 'rah': 704,\n",
       " 'tune': 705,\n",
       " 'reply': 706,\n",
       " 'baki': 707,\n",
       " 'haan': 708,\n",
       " 'gud': 709,\n",
       " 'gali': 710,\n",
       " 'trust': 711,\n",
       " 'bolo': 712,\n",
       " 'call': 713,\n",
       " 'banane': 714,\n",
       " 'state': 715,\n",
       " 'nawaz': 716,\n",
       " 'haal': 717,\n",
       " 'izzat': 718,\n",
       " 'phr': 719,\n",
       " 'waiting': 720,\n",
       " 'lakin': 721,\n",
       " 'khuda': 722,\n",
       " 'krte': 723,\n",
       " 'dukh': 724,\n",
       " 'enjoy': 725,\n",
       " 'followers': 726,\n",
       " 'believe': 727,\n",
       " 'old': 728,\n",
       " 'welcome': 729,\n",
       " 'lene': 730,\n",
       " 'performance': 731,\n",
       " 'age': 732,\n",
       " 'second': 733,\n",
       " 'wishing': 734,\n",
       " 'sara': 735,\n",
       " 'ghatiya': 736,\n",
       " 'face': 737,\n",
       " 'hero': 738,\n",
       " 'ghanta': 739,\n",
       " 'behen': 740,\n",
       " 'lena': 741,\n",
       " 'walon': 742,\n",
       " 'game': 743,\n",
       " 'kitne': 744,\n",
       " 'kbhi': 745,\n",
       " 'leke': 746,\n",
       " 'payega': 747,\n",
       " 'father': 748,\n",
       " 'gaddar': 749,\n",
       " 'baby': 750,\n",
       " 'kyunki': 751,\n",
       " 'chance': 752,\n",
       " 'feeling': 753,\n",
       " 'giving': 754,\n",
       " 'dard': 755,\n",
       " 'number': 756,\n",
       " 'ati': 757,\n",
       " 'jana': 758,\n",
       " 'nothing': 759,\n",
       " 'buri': 760,\n",
       " 'tumse': 761,\n",
       " 'omg': 762,\n",
       " 'ramzan': 763,\n",
       " 'ill': 764,\n",
       " 'vande': 765,\n",
       " 'kashmir': 766,\n",
       " 'rahega': 767,\n",
       " 'line': 768,\n",
       " 'saala': 769,\n",
       " 'gadkari': 770,\n",
       " 'nay': 771,\n",
       " 'lage': 772,\n",
       " 'humare': 773,\n",
       " 'maro': 774,\n",
       " 'meet': 775,\n",
       " 'chala': 776,\n",
       " 'late': 777,\n",
       " 'photo': 778,\n",
       " 'kitni': 779,\n",
       " 'pray': 780,\n",
       " 'health': 781,\n",
       " 'matter': 782,\n",
       " 'isi': 783,\n",
       " 'uss': 784,\n",
       " 'actually': 785,\n",
       " 'umid': 786,\n",
       " 'bhe': 787,\n",
       " 'rhy': 788,\n",
       " 'yar': 789,\n",
       " 'jaha': 790,\n",
       " 'dimag': 791,\n",
       " 'kyon': 792,\n",
       " 'pani': 793,\n",
       " 'ani': 794,\n",
       " 'msg': 795,\n",
       " 'far': 796,\n",
       " 'inka': 797,\n",
       " 'mean': 798,\n",
       " 'yehi': 799,\n",
       " 'rathee': 800,\n",
       " 'favorite': 801,\n",
       " 'saab': 802,\n",
       " 'kehte': 803,\n",
       " 'ameen': 804,\n",
       " 'bachpan': 805,\n",
       " 'pas': 806,\n",
       " 'sister': 807,\n",
       " 'rakhe': 808,\n",
       " 'godi': 809,\n",
       " 'chu': 810,\n",
       " 'women': 811,\n",
       " 'thy': 812,\n",
       " 'means': 813,\n",
       " 'dar': 814,\n",
       " 'nafrat': 815,\n",
       " 'parents': 816,\n",
       " 'joke': 817,\n",
       " 'aate': 818,\n",
       " 'perfect': 819,\n",
       " 'asli': 820,\n",
       " 'something': 821,\n",
       " 'odisha': 822,\n",
       " 'finally': 823,\n",
       " 'light': 824,\n",
       " 'chuki': 825,\n",
       " 'jan': 826,\n",
       " 'layak': 827,\n",
       " 'bhul': 828,\n",
       " 'jawab': 829,\n",
       " 'case': 830,\n",
       " 'suna': 831,\n",
       " 'dum': 832,\n",
       " 'times': 833,\n",
       " 'episode': 834,\n",
       " 'men': 835,\n",
       " 'bap': 836,\n",
       " 'khel': 837,\n",
       " 'jitna': 838,\n",
       " 'sardanarohit': 839,\n",
       " 'ministry': 840,\n",
       " 'wohi': 841,\n",
       " 'cong': 842,\n",
       " 'lye': 843,\n",
       " 'paper': 844,\n",
       " 'feku': 845,\n",
       " 'result': 846,\n",
       " 'rahy': 847,\n",
       " 'everything': 848,\n",
       " 'awam': 849,\n",
       " 'ghr': 850,\n",
       " 'apse': 851,\n",
       " 'dusre': 852,\n",
       " 'jaati': 853,\n",
       " 'yah': 854,\n",
       " 'lagi': 855,\n",
       " 'boys': 856,\n",
       " 'gay': 857,\n",
       " 'dikh': 858,\n",
       " 'oye': 859,\n",
       " 'aajtak': 860,\n",
       " 'via': 861,\n",
       " 'admi': 862,\n",
       " 'sai': 863,\n",
       " 'btw': 864,\n",
       " 'bare': 865,\n",
       " 'asadowaisi': 866,\n",
       " 'trailer': 867,\n",
       " 'karan': 868,\n",
       " 'kamal': 869,\n",
       " 'saying': 870,\n",
       " 'action': 871,\n",
       " 'jin': 872,\n",
       " 'karti': 873,\n",
       " 'mauka': 874,\n",
       " 'class': 875,\n",
       " 'dharm': 876,\n",
       " 'banda': 877,\n",
       " 'west': 878,\n",
       " 'train': 879,\n",
       " 'kumar': 880,\n",
       " 'funny': 881,\n",
       " 'nehi': 882,\n",
       " 'called': 883,\n",
       " 'milna': 884,\n",
       " 'saare': 885,\n",
       " 'bane': 886,\n",
       " 'bna': 887,\n",
       " 'let': 888,\n",
       " 'vishwas': 889,\n",
       " 'jindabaad': 890,\n",
       " 'son': 891,\n",
       " 'bank': 892,\n",
       " 'making': 893,\n",
       " 'june': 894,\n",
       " 'musalmano': 895,\n",
       " 'brother': 896,\n",
       " 'official': 897,\n",
       " 'absolutely': 898,\n",
       " 'surya': 899,\n",
       " 'kejriwal': 900,\n",
       " 'phle': 901,\n",
       " 'laat': 902,\n",
       " 'sohne': 903,\n",
       " 'friday': 904,\n",
       " 'jit': 905,\n",
       " 'umeed': 906,\n",
       " 'kind': 907,\n",
       " 'rehne': 908,\n",
       " 'tell': 909,\n",
       " 'court': 910,\n",
       " 'patrag': 911,\n",
       " 'understand': 912,\n",
       " 'inki': 913,\n",
       " 'stop': 914,\n",
       " 'theek': 915,\n",
       " 'twt': 916,\n",
       " 'looks': 917,\n",
       " 'kai': 918,\n",
       " 'lia': 919,\n",
       " 'south': 920,\n",
       " 'seriously': 921,\n",
       " 'arvindkejriwal': 922,\n",
       " 'lie': 923,\n",
       " 'thoda': 924,\n",
       " 'maryamnsharif': 925,\n",
       " 'lete': 926,\n",
       " 'nhe': 927,\n",
       " 'comment': 928,\n",
       " 'hoo': 929,\n",
       " 'reh': 930,\n",
       " 'marne': 931,\n",
       " 'lekar': 932,\n",
       " 'unka': 933,\n",
       " 'top': 934,\n",
       " 'english': 935,\n",
       " 'jal': 936,\n",
       " 'hen': 937,\n",
       " 'zyada': 938,\n",
       " 'krti': 939,\n",
       " 'yogi': 940,\n",
       " 'haa': 941,\n",
       " 'jitega': 942,\n",
       " 'bache': 943,\n",
       " 'enough': 944,\n",
       " 'language': 945,\n",
       " 'wahi': 946,\n",
       " 'gyan': 947,\n",
       " 'weekend': 948,\n",
       " 'kami': 949,\n",
       " 'fucking': 950,\n",
       " 'chale': 951,\n",
       " 'crush': 952,\n",
       " 'jaa': 953,\n",
       " 'dream': 954,\n",
       " 'amethi': 955,\n",
       " 'bts': 956,\n",
       " 'salam': 957,\n",
       " 'oath': 958,\n",
       " 'waha': 959,\n",
       " 'aurat': 960,\n",
       " 'abi': 961,\n",
       " 'high': 962,\n",
       " 'yadav': 963,\n",
       " 'ese': 964,\n",
       " 'drama': 965,\n",
       " 'shayad': 966,\n",
       " 'jaane': 967,\n",
       " 'chowkidar': 968,\n",
       " 'saint': 969,\n",
       " 'deserve': 970,\n",
       " 'victory': 971,\n",
       " 'fight': 972,\n",
       " 'milta': 973,\n",
       " 'reason': 974,\n",
       " 'jisme': 975,\n",
       " 'umar': 976,\n",
       " 'damn': 977,\n",
       " 'corrupt': 978,\n",
       " 'side': 979,\n",
       " 'sha': 980,\n",
       " 'check': 981,\n",
       " 'mard': 982,\n",
       " 'happiness': 983,\n",
       " 'goli': 984,\n",
       " 'ticket': 985,\n",
       " 'salute': 986,\n",
       " 'jitne': 987,\n",
       " 'system': 988,\n",
       " 'gussa': 989,\n",
       " '1st': 990,\n",
       " 'students': 991,\n",
       " 'congressi': 992,\n",
       " 'dost': 993,\n",
       " 'mehnat': 994,\n",
       " 'hae': 995,\n",
       " 'honge': 996,\n",
       " 'aao': 997,\n",
       " 'khelne': 998,\n",
       " 'isme': 999,\n",
       " 'yogendrayadav': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n"
     ]
    }
   ],
   "source": [
    "train_len = train_texts.shape[0]\n",
    "print (train_len)\n",
    "trainX = X[:train_len]\n",
    "trainy = y[:train_len]\n",
    "valX = X[train_len:]\n",
    "valy = y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'min lyching manakgupta mein kahna nae chahta qki mere yaha btay tco jwsdvvomt8'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5706,  9326,  2220,    31,  2221,  1128,   680,  2222,   111,\n",
       "         362, 14901,   103, 14902,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word not in embeddings_index1:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix1 = np.zeros((len(word_index) + 1, embed_dim1))\n",
    "embedding_matrix2 = np.zeros((len(word_index) + 1, embed_dim2))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index1:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_vector = embeddings_index1.get(word)\n",
    "        embedding_matrix1[i,:] = np.array(embedding_vector)\n",
    "        \n",
    "for word, i in word_index.items():    \n",
    "    if word in embeddings_index2:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_vector = embeddings_index2.get(word)\n",
    "        embedding_matrix2[i,:] = np.array(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16001, 200) (16001, 300)\n"
     ]
    }
   ],
   "source": [
    "print (embedding_matrix1.shape, embedding_matrix2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    #f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)), name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )), name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)), name='u')\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input = Input(shape=(max_len,))\n",
    "    emb1 = Embedding(len(word_index) + 1, embed_dim1, weights=[embedding_matrix1], trainable=True)(input)\n",
    "    emb2 = Embedding(len(word_index) + 1, embed_dim2, weights=[embedding_matrix2], trainable=False)(input)\n",
    "    #cnn = Conv1D(filters=100,kernel_size=5,strides=1,padding=\"same\")(Concatenate(-1)([emb1,emb2]))\n",
    "    #cnn = SpatialDropout1D(.2)(Concatenate(-1)([emb1,emb2]))\n",
    "    out1 = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(emb1)\n",
    "    out2 = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=False))(out1)\n",
    "    out1_attn = AttLayer(lstm_out//2)(out1)\n",
    "    \n",
    "    out3 = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(emb2)\n",
    "    out4 = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=False))(out3)\n",
    "    out3_attn = AttLayer(lstm_out//2)(out1)\n",
    "    \n",
    "    out = Concatenate(-1)([out2,out1_attn,out4,out3_attn])\n",
    "    out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out)(out)\n",
    "    out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out//2)(out)\n",
    "    out = Dropout(.2)(out)\n",
    "    out = Dense(lstm_out//4)(out)\n",
    "    out = Dropout(.2)(out)\n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model(input,out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0001),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0303 09:36:55.860455 4484668864 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/optimizers.py:1925: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0303 09:36:55.866902 4484668864 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:3827: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 200)      3200200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 20, 300)      4800300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 20, 256)      336896      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 20, 256)      439296      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128)          164352      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_3 (AttLayer)          (None, 256)          16512       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128)          164352      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_4 (AttLayer)          (None, 256)          16512       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768)          0           bidirectional_6[0][0]            \n",
      "                                                                 att_layer_3[0][0]                \n",
      "                                                                 bidirectional_8[0][0]            \n",
      "                                                                 att_layer_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          98432       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            99          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,247,287\n",
      "Trainable params: 4,446,987\n",
      "Non-trainable params: 4,800,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0303 09:37:54.532935 4484668864 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0303 09:37:58.794992 4484668864 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:1051: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0303 09:37:59.189319 4484668864 module_wrapper.py:139] From /Users/victor/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py:1038: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      " - 70s - loss: 0.9311 - acc: 0.5388 - val_loss: 0.8804 - val_acc: 0.5850\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58500, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 2/50\n",
      " - 57s - loss: 0.8842 - acc: 0.5803 - val_loss: 0.8626 - val_acc: 0.5943\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58500 to 0.59433, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 3/50\n",
      " - 58s - loss: 0.8660 - acc: 0.5939 - val_loss: 0.8497 - val_acc: 0.6073\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.59433 to 0.60733, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 4/50\n",
      " - 62s - loss: 0.8379 - acc: 0.6142 - val_loss: 0.8408 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.60733 to 0.61200, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 5/50\n",
      " - 56s - loss: 0.8187 - acc: 0.6269 - val_loss: 0.8361 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61200 to 0.62067, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 6/50\n",
      " - 55s - loss: 0.7904 - acc: 0.6426 - val_loss: 0.8474 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.62067\n",
      "Epoch 7/50\n",
      " - 57s - loss: 0.7558 - acc: 0.6683 - val_loss: 0.8380 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.62067 to 0.62267, saving model to ../models/weights_cmsa_wordlevel.hdf5\n",
      "Epoch 8/50\n",
      " - 61s - loss: 0.7319 - acc: 0.6796 - val_loss: 0.8437 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.62267\n",
      "Epoch 9/50\n",
      " - 62s - loss: 0.6845 - acc: 0.7046 - val_loss: 0.8712 - val_acc: 0.6177\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.62267\n",
      "Epoch 10/50\n",
      " - 61s - loss: 0.6486 - acc: 0.7224 - val_loss: 0.8751 - val_acc: 0.6203\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62267\n",
      "Epoch 11/50\n",
      " - 61s - loss: 0.6175 - acc: 0.7418 - val_loss: 0.9094 - val_acc: 0.6133\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62267\n",
      "Epoch 12/50\n",
      " - 63s - loss: 0.5881 - acc: 0.7573 - val_loss: 0.9289 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62267\n",
      "Epoch 13/50\n",
      " - 63s - loss: 0.5565 - acc: 0.7661 - val_loss: 0.9444 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62267\n",
      "Epoch 14/50\n",
      " - 60s - loss: 0.5143 - acc: 0.7904 - val_loss: 1.0018 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62267\n",
      "Epoch 15/50\n",
      " - 59s - loss: 0.4976 - acc: 0.8006 - val_loss: 1.0246 - val_acc: 0.5993\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62267\n",
      "Epoch 16/50\n",
      " - 59s - loss: 0.4815 - acc: 0.8069 - val_loss: 1.0608 - val_acc: 0.5977\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62267\n",
      "Epoch 17/50\n",
      " - 61s - loss: 0.4764 - acc: 0.8101 - val_loss: 1.0809 - val_acc: 0.5937\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62267\n",
      "Epoch 18/50\n",
      " - 58s - loss: 0.4453 - acc: 0.8237 - val_loss: 1.1042 - val_acc: 0.5883\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62267\n",
      "Epoch 19/50\n",
      " - 59s - loss: 0.4308 - acc: 0.8276 - val_loss: 1.1351 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62267\n",
      "Epoch 20/50\n",
      " - 60s - loss: 0.4208 - acc: 0.8331 - val_loss: 1.1612 - val_acc: 0.5843\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62267\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6e6b12960838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../models/weights_cmsa_wordlevel.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/nmt-keras/src/keras/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Documents/nmt-keras/src/keras/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/nmt-keras/src/keras/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 3120\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   3121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=5, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_cmsa_wordlevel.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(trainX, trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=(valX,valy), callbacks=[early,lr,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snapshotensemble(Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        self.best_value = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        self.test_predictions.append(\n",
    "            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_withemb():\n",
    "    input = Input(shape=(max_len,))\n",
    "    emb1 = Embedding(len(word_index) + 1, embed_dim1, weights=[embedding_matrix1], trainable=True)(input)\n",
    "    emb2 = Embedding(len(word_index) + 1, embed_dim2, weights=[embedding_matrix2], trainable=False)(input)\n",
    "    #cnn = Conv1D(filters=100,kernel_size=5,strides=1,padding=\"same\")(Concatenate(-1)([emb1,emb2]))\n",
    "    cnn = SpatialDropout1D(.2)(Concatenate(-1)([emb1,emb2]))\n",
    "    out = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(cnn)\n",
    "    out = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=True))(out)\n",
    "    out = AttLayer(lstm_out//2)(out)\n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model(input,out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0001),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 20, 200)      3200200     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 20, 300)      4800300     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 20, 500)      0           embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 20, 500)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 20, 256)      644096      spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 20, 128)      164352      bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_5 (AttLayer)          (None, 128)          8320        bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            387         att_layer_5[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,817,655\n",
      "Trainable params: 4,017,355\n",
      "Non-trainable params: 4,800,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model_withemb()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      " - 55s - loss: 0.6604 - acc: 0.7216 - val_loss: 0.8584 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61700, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 2/50\n",
      " - 51s - loss: 0.6547 - acc: 0.7259 - val_loss: 0.8563 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61700 to 0.62133, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 3/50\n",
      " - 54s - loss: 0.6477 - acc: 0.7251 - val_loss: 0.8628 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.62133\n",
      "Epoch 4/50\n",
      " - 53s - loss: 0.6405 - acc: 0.7356 - val_loss: 0.8615 - val_acc: 0.6193\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.62133\n",
      "Epoch 5/50\n",
      " - 54s - loss: 0.6414 - acc: 0.7294 - val_loss: 0.8667 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.62133 to 0.62233, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 6/50\n",
      " - 56s - loss: 0.6273 - acc: 0.7412 - val_loss: 0.8732 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.62233 to 0.62400, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 7/50\n",
      " - 65s - loss: 0.6165 - acc: 0.7424 - val_loss: 0.8757 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.62400\n",
      "Epoch 8/50\n",
      " - 60s - loss: 0.6219 - acc: 0.7411 - val_loss: 0.8798 - val_acc: 0.6243\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.62400 to 0.62433, saving model to ../models/weights_finalmodel3.hdf5\n",
      "Epoch 9/50\n",
      " - 53s - loss: 0.6123 - acc: 0.7493 - val_loss: 0.8824 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.62433\n",
      "Epoch 10/50\n",
      " - 53s - loss: 0.6013 - acc: 0.7539 - val_loss: 0.8897 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62433\n",
      "Epoch 11/50\n",
      " - 62s - loss: 0.5961 - acc: 0.7568 - val_loss: 0.8878 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62433\n",
      "Epoch 12/50\n",
      " - 50s - loss: 0.5908 - acc: 0.7581 - val_loss: 0.9005 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62433\n",
      "Epoch 13/50\n",
      " - 50s - loss: 0.5922 - acc: 0.7530 - val_loss: 0.8926 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62433\n",
      "Epoch 14/50\n",
      " - 55s - loss: 0.5814 - acc: 0.7639 - val_loss: 0.9066 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62433\n",
      "Epoch 15/50\n",
      " - 53s - loss: 0.5750 - acc: 0.7666 - val_loss: 0.9136 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62433\n",
      "Epoch 16/50\n",
      " - 51s - loss: 0.5642 - acc: 0.7691 - val_loss: 0.9110 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62433\n",
      "Epoch 17/50\n",
      " - 50s - loss: 0.5600 - acc: 0.7716 - val_loss: 0.9116 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62433\n",
      "Epoch 18/50\n",
      " - 50s - loss: 0.5555 - acc: 0.7779 - val_loss: 0.9101 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62433\n",
      "Epoch 19/50\n",
      " - 50s - loss: 0.5470 - acc: 0.7764 - val_loss: 0.9344 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62433\n",
      "Epoch 20/50\n",
      " - 55s - loss: 0.5507 - acc: 0.7786 - val_loss: 0.9198 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62433\n",
      "Epoch 21/50\n",
      " - 56s - loss: 0.5503 - acc: 0.7820 - val_loss: 0.9251 - val_acc: 0.6127\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62433\n",
      "Epoch 22/50\n",
      " - 54s - loss: 0.5387 - acc: 0.7814 - val_loss: 0.9284 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62433\n",
      "Epoch 23/50\n",
      " - 51s - loss: 0.5373 - acc: 0.7875 - val_loss: 0.9371 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62433\n",
      "Epoch 24/50\n",
      " - 50s - loss: 0.5338 - acc: 0.7844 - val_loss: 0.9410 - val_acc: 0.6133\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.6806997336971108e-05.\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62433\n",
      "Epoch 25/50\n",
      " - 50s - loss: 0.5266 - acc: 0.7902 - val_loss: 0.9424 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62433\n",
      "Epoch 26/50\n",
      " - 50s - loss: 0.5217 - acc: 0.7900 - val_loss: 0.9506 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62433\n",
      "Epoch 27/50\n",
      " - 51s - loss: 0.5240 - acc: 0.7888 - val_loss: 0.9517 - val_acc: 0.6137\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62433\n",
      "Epoch 28/50\n",
      " - 60s - loss: 0.5196 - acc: 0.7930 - val_loss: 0.9484 - val_acc: 0.6113\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62433\n",
      "Epoch 29/50\n",
      " - 62s - loss: 0.5124 - acc: 0.7941 - val_loss: 0.9616 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62433\n",
      "Epoch 30/50\n",
      " - 60s - loss: 0.5096 - acc: 0.7984 - val_loss: 0.9651 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62433\n",
      "Epoch 31/50\n",
      " - 57s - loss: 0.5151 - acc: 0.7965 - val_loss: 0.9593 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62433\n",
      "Epoch 32/50\n",
      " - 56s - loss: 0.5184 - acc: 0.7921 - val_loss: 0.9544 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.1764897499233484e-05.\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62433\n",
      "Epoch 33/50\n",
      " - 56s - loss: 0.4957 - acc: 0.8034 - val_loss: 0.9690 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62433\n",
      "Epoch 34/50\n",
      " - 57s - loss: 0.5050 - acc: 0.7975 - val_loss: 0.9678 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62433\n",
      "Epoch 35/50\n",
      " - 63s - loss: 0.4952 - acc: 0.8037 - val_loss: 0.9732 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62433\n",
      "Epoch 36/50\n",
      " - 59s - loss: 0.4944 - acc: 0.8034 - val_loss: 0.9753 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.62433\n",
      "Epoch 37/50\n",
      " - 64s - loss: 0.4976 - acc: 0.8009 - val_loss: 0.9729 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62433\n",
      "Epoch 38/50\n",
      " - 64s - loss: 0.4955 - acc: 0.8046 - val_loss: 0.9785 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.62433\n",
      "Epoch 39/50\n",
      " - 62s - loss: 0.4939 - acc: 0.8051 - val_loss: 0.9782 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62433\n",
      "Epoch 40/50\n",
      " - 60s - loss: 0.4902 - acc: 0.8042 - val_loss: 0.9828 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.235428504121954e-06.\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62433\n",
      "Epoch 41/50\n",
      " - 58s - loss: 0.4872 - acc: 0.8041 - val_loss: 0.9844 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62433\n",
      "Epoch 42/50\n",
      " - 56s - loss: 0.4893 - acc: 0.8073 - val_loss: 0.9850 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62433\n",
      "Epoch 43/50\n",
      " - 57s - loss: 0.4882 - acc: 0.8065 - val_loss: 0.9835 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62433\n",
      "Epoch 44/50\n",
      " - 57s - loss: 0.4794 - acc: 0.8114 - val_loss: 0.9877 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62433\n",
      "Epoch 45/50\n",
      " - 55s - loss: 0.4810 - acc: 0.8084 - val_loss: 0.9906 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62433\n",
      "Epoch 46/50\n",
      " - 56s - loss: 0.4874 - acc: 0.8052 - val_loss: 0.9926 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62433\n",
      "Epoch 47/50\n",
      " - 59s - loss: 0.4794 - acc: 0.8121 - val_loss: 0.9942 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62433\n",
      "Epoch 48/50\n",
      " - 56s - loss: 0.4719 - acc: 0.8129 - val_loss: 0.9955 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 5.764799698226852e-06.\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.62433\n",
      "Epoch 49/50\n",
      " - 56s - loss: 0.4878 - acc: 0.8059 - val_loss: 0.9922 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62433\n",
      "Epoch 50/50\n",
      " - 56s - loss: 0.4802 - acc: 0.8085 - val_loss: 0.9951 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62433\n",
      "Epoch 00050: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=8, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_finalmodel3.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "snapshot = Snapshotensemble([valX,valy],testX)\n",
    "\n",
    "history = model.fit(trainX, trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=(valX,valy), callbacks=[early,lr,checkpointer,snapshot])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/weights_finalmodel3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3000, 3)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(snapshot.valid_predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(snapshot.valid_predictions).mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(valX).argmax(axis=1) #np.array(snapshot.valid_predictions[:5]).mean(0).argmax(axis=1)\n",
    "val_texts['sentiment_pred'] = [in_senti_dict[i] for i in val_pred]\n",
    "#val_texts = val_texts.sort_values(['uid'],ascending=[True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid                                               text sentiment  \\\n",
       "0     3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13   12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23   23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35   24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49   45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "\n",
       "   sentiment_pred sentiment_pred2 sentiment_pred3  \n",
       "0        negative        positive         neutral  \n",
       "13       positive        positive        positive  \n",
       "23       negative        negative        negative  \n",
       "35       negative         neutral         neutral  \n",
       "49       positive         neutral        positive  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6286077414937643\n",
      "0.6243333333333333\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[598, 224,  68],\n",
       "       [274, 564, 290],\n",
       "       [ 82, 209, 691]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val_texts.sentiment,val_texts.sentiment_pred,labels=['negative','neutral','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.array(snapshot.test_predictions).mean(0).argmax(1)\n",
    "test_texts['sentiment_pred2'] = [in_senti_dict[i] for i in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(testX).argmax(axis=1)\n",
    "test_texts['sentiment_pred'] = [in_senti_dict[i] for i in test_pred]\n",
    "#test_texts_without_sort = test_texts.copy()\n",
    "#test_texts = test_texts.sort_values(['uid'],ascending=[True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe jaise pakistan wale karte south ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>anus prerna way ran saving ram ram jay rajaraa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai pathan nae warna ptm nay wae dramay...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision much option arm big second ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>keep saying kenyan rugby beautiful save kru dr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>guru give bless sewa simrn parmarth good deeds...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>136</td>\n",
       "      <td>tum logo zindagi mai kabhi india map nahi dhek...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>139</td>\n",
       "      <td>itne gandi priye aapke profile pic modi lga le...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>158</td>\n",
       "      <td>google translate working aisi jagah net aata</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162</td>\n",
       "      <td>qaira sahb great man apka beta nhayat nfees na...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment_pred  \\\n",
       "0    1  keh aese rahe jaise pakistan wale karte south ...       negative   \n",
       "1    4  anus prerna way ran saving ram ram jay rajaraa...        neutral   \n",
       "2   17  shukar hai pathan nae warna ptm nay wae dramay...       negative   \n",
       "3   18  harsh pen decision much option arm big second ...       positive   \n",
       "4   34  keep saying kenyan rugby beautiful save kru dr...       positive   \n",
       "5   43  guru give bless sewa simrn parmarth good deeds...       positive   \n",
       "6  136  tum logo zindagi mai kabhi india map nahi dhek...        neutral   \n",
       "7  139  itne gandi priye aapke profile pic modi lga le...        neutral   \n",
       "8  158       google translate working aisi jagah net aata        neutral   \n",
       "9  162  qaira sahb great man apka beta nhayat nfees na...        neutral   \n",
       "\n",
       "  sentiment_pred3 sentiment_pred2  \n",
       "0        negative        negative  \n",
       "1         neutral         neutral  \n",
       "2         neutral         neutral  \n",
       "3        positive        positive  \n",
       "4         neutral        positive  \n",
       "5        positive        positive  \n",
       "6         neutral         neutral  \n",
       "7         neutral         neutral  \n",
       "8         neutral         neutral  \n",
       "9        positive        positive  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1079\n",
       "positive    1000\n",
       "negative     921\n",
       "Name: sentiment_pred, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.sentiment_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1049\n",
       "neutral      997\n",
       "negative     954\n",
       "Name: sentiment_pred, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.sentiment_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     1128\n",
       "positive     982\n",
       "negative     890\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1 of 10\n",
      "Starting iteration 2 of 10\n",
      "Starting iteration 3 of 10\n",
      "Starting iteration 4 of 10\n",
      "Starting iteration 5 of 10\n",
      "Starting iteration 6 of 10\n",
      "Starting iteration 7 of 10\n",
      "Starting iteration 8 of 10\n",
      "Starting iteration 9 of 10\n",
      "Starting iteration 10 of 10\n",
      "done in 1136.884s.\n"
     ]
    }
   ],
   "source": [
    "from LJST_script_BTM import *\n",
    "\n",
    "minlabel = 0\n",
    "maxlabel = 2\n",
    "sentirange = maxlabel-minlabel\n",
    "numwordspertopic=5\n",
    "skipgramwindow=5\n",
    "\n",
    "numsentilabel=3\n",
    "numtopics=10\n",
    "alpha=10\n",
    "beta=.01\n",
    "gamma=10.0\n",
    "maxiter=10\n",
    "\n",
    "unlabelled_data = pd.concat([val_texts[['text']],test_texts[['text']]],axis=0)\n",
    "\n",
    "ljst_model = run_experiment(train_texts.text,trainy.argmax(1),unlabelled_data.text,sentirange,minlabel,maxlabel,numsentilabel,numtopics,skipgramwindow,alpha,beta,gamma,maxiter,numwordspertopic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_estimated = []\n",
    "for i in range(len(val_texts)):\n",
    "    sentiment = 0\n",
    "    index = len(train_texts) + i\n",
    "    temp = np.matmul(ljst_model.dt_distribution[index,:],ljst_model.dts_distribution[index,:,:])\n",
    "    sentiment = temp.argmax()\n",
    "    #for k, val in enumerate(temp):\n",
    "    #    sentiment += k*binsize*val\n",
    "    ds_estimated.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts['sentiment_pred2'] = [in_senti_dict[i] for i in ds_estimated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071254291113468\n",
      "0.5173333333333333\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred2,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>ankita shah8 bhadve photo elections pehle jyad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>krishna jcb full trend chal rahi</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>sharma1 kavita sharma4 sunita kal jab evm seal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>101</td>\n",
       "      <td>nolo weni ankere gae weekend</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>145</td>\n",
       "      <td>abe kutte sakal musalmaan wazah tere ghar roti...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>167</td>\n",
       "      <td>mohsin dawartweets daikho aur kuch kha ker mar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>252</td>\n",
       "      <td>haha soldier reading american gods hardy har h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>288</td>\n",
       "      <td>dor song jis adnan sami mujahid part play keya...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>318</td>\n",
       "      <td>government kya kuch nahi kar sakti tumhare bhe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>337</td>\n",
       "      <td>rao hindi tumhari bass nahi nahi seekh paoge d...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>348</td>\n",
       "      <td>ch1 pakeeza language istemaal koi nooni bhayoo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>352</td>\n",
       "      <td>gulwish nahi news defend kerne koshesh ker rhe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>359</td>\n",
       "      <td>ahmad884 lulli hoti nahi khadi baten karalo ba...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>416</td>\n",
       "      <td>live urdu fridaysermon delivered head ahmadiyy...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>439</td>\n",
       "      <td>color tum india wale sari yahi sochti pakistan...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>462</td>\n",
       "      <td>world cup mein mard nahi khusre gae hein larna...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>466</td>\n",
       "      <td>bhaijaan please mein apka bahot bahot bada fan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>469</td>\n",
       "      <td>postopinions take care bibi allah khair stupid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>497</td>\n",
       "      <td>scratch kfc bhi auqaat nahi rahee todays defea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>529</td>\n",
       "      <td>hai sabse super modi hai sabse uper modi deshk...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>561</td>\n",
       "      <td>mazhab naam par election jeetna koi bada kaarn...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>569</td>\n",
       "      <td>may allah bless strong imaan taqwa piety pure ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>576</td>\n",
       "      <td>super hilarious true reality noora haramkhor f...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>605</td>\n",
       "      <td>skfc good morning cute kartikeyan annaa happy ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>629</td>\n",
       "      <td>hadd hoti hai wrong make account live</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>712</td>\n",
       "      <td>hahahahaah fuck koy salig lucas doh hapit inju...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>715</td>\n",
       "      <td>sacrificed lot happy normal lad liverpool whos...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>740</td>\n",
       "      <td>yeah aap hamara roza kharaab karay hain yeh us...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>743</td>\n",
       "      <td>bollyhungama vidya balan monday bakri eid wom ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>745</td>\n",
       "      <td>amitbehere use ram bas aur yehi log bolte hind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>747</td>\n",
       "      <td>sai institute manimajra chandigarh best place ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>775</td>\n",
       "      <td>jai shree ram jay hanuman jai shree krishna ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>781</td>\n",
       "      <td>khan sahab apko vote diya taky bany alhamdulil...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>797</td>\n",
       "      <td>sir mere rs1180 deduct gaya hai kekin branch o...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>807</td>\n",
       "      <td>would bring great joy hear rendition des ree g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>837</td>\n",
       "      <td>sale owner split level home great neighborhood...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>870</td>\n",
       "      <td>scindia meri baat nhi suni apne congress prem ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>880</td>\n",
       "      <td>dil khush krny ghalib khyal acha myth efnod8ijpj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>885</td>\n",
       "      <td>ngk good one kudos heroes offl thisisysr</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>892</td>\n",
       "      <td>officeofknath sahi hoga sayad tabhi sirf appki...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>895</td>\n",
       "      <td>ez1403 pakistani pig dukkar kutta suvar mix aulad</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>906</td>\n",
       "      <td>bies blocked aapke acche din gaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>939</td>\n",
       "      <td>kalia thaa want hindi uneducated people nice c...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>962</td>\n",
       "      <td>eid mubarak hadiya trabaho vlogs cdjaercsyo</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment  \\\n",
       "0      3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13    12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23    23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35    24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49    45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "59    61  ankita shah8 bhadve photo elections pehle jyad...  negative   \n",
       "69    64                   krishna jcb full trend chal rahi  positive   \n",
       "75    77  sharma1 kavita sharma4 sunita kal jab evm seal...  negative   \n",
       "90    81  accha kiya invite nai kiya corrupted party ind...  negative   \n",
       "103  101                       nolo weni ankere gae weekend  positive   \n",
       "108  145  abe kutte sakal musalmaan wazah tere ghar roti...  negative   \n",
       "125  167  mohsin dawartweets daikho aur kuch kha ker mar...  negative   \n",
       "142  252  haha soldier reading american gods hardy har h...  negative   \n",
       "154  288  dor song jis adnan sami mujahid part play keya...   neutral   \n",
       "170  318  government kya kuch nahi kar sakti tumhare bhe...  negative   \n",
       "186  337  rao hindi tumhari bass nahi nahi seekh paoge d...  negative   \n",
       "200  348  ch1 pakeeza language istemaal koi nooni bhayoo...   neutral   \n",
       "216  352  gulwish nahi news defend kerne koshesh ker rhe...   neutral   \n",
       "226  359  ahmad884 lulli hoti nahi khadi baten karalo ba...   neutral   \n",
       "236  416  live urdu fridaysermon delivered head ahmadiyy...   neutral   \n",
       "248  439  color tum india wale sari yahi sochti pakistan...  positive   \n",
       "267  462  world cup mein mard nahi khusre gae hein larna...  negative   \n",
       "284  466  bhaijaan please mein apka bahot bahot bada fan...   neutral   \n",
       "300  469  postopinions take care bibi allah khair stupid...  negative   \n",
       "311  497  scratch kfc bhi auqaat nahi rahee todays defea...  negative   \n",
       "327  529  hai sabse super modi hai sabse uper modi deshk...   neutral   \n",
       "340  561  mazhab naam par election jeetna koi bada kaarn...  negative   \n",
       "352  569  may allah bless strong imaan taqwa piety pure ...  positive   \n",
       "366  576  super hilarious true reality noora haramkhor f...   neutral   \n",
       "375  605  skfc good morning cute kartikeyan annaa happy ...   neutral   \n",
       "385  629              hadd hoti hai wrong make account live  negative   \n",
       "392  712  hahahahaah fuck koy salig lucas doh hapit inju...   neutral   \n",
       "401  715  sacrificed lot happy normal lad liverpool whos...   neutral   \n",
       "411  740  yeah aap hamara roza kharaab karay hain yeh us...   neutral   \n",
       "429  743  bollyhungama vidya balan monday bakri eid wom ...  positive   \n",
       "444  745  amitbehere use ram bas aur yehi log bolte hind...  negative   \n",
       "458  747  sai institute manimajra chandigarh best place ...   neutral   \n",
       "472  775  jai shree ram jay hanuman jai shree krishna ja...   neutral   \n",
       "489  781  khan sahab apko vote diya taky bany alhamdulil...   neutral   \n",
       "504  797  sir mere rs1180 deduct gaya hai kekin branch o...   neutral   \n",
       "518  807  would bring great joy hear rendition des ree g...   neutral   \n",
       "529  837  sale owner split level home great neighborhood...  positive   \n",
       "544  870  scindia meri baat nhi suni apne congress prem ...  negative   \n",
       "561  880   dil khush krny ghalib khyal acha myth efnod8ijpj   neutral   \n",
       "569  885           ngk good one kudos heroes offl thisisysr  positive   \n",
       "576  892  officeofknath sahi hoga sayad tabhi sirf appki...   neutral   \n",
       "593  895  ez1403 pakistani pig dukkar kutta suvar mix aulad  negative   \n",
       "601  906                  bies blocked aapke acche din gaye   neutral   \n",
       "607  939  kalia thaa want hindi uneducated people nice c...   neutral   \n",
       "617  962        eid mubarak hadiya trabaho vlogs cdjaercsyo   neutral   \n",
       "\n",
       "    sentiment_pred sentiment_pred2  \n",
       "0         negative        positive  \n",
       "13        positive        positive  \n",
       "23        negative        negative  \n",
       "35        negative         neutral  \n",
       "49        positive         neutral  \n",
       "59        negative        negative  \n",
       "69         neutral         neutral  \n",
       "75        negative        negative  \n",
       "90        negative         neutral  \n",
       "103        neutral         neutral  \n",
       "108       negative        negative  \n",
       "125       negative        negative  \n",
       "142       positive        positive  \n",
       "154        neutral        positive  \n",
       "170       negative        negative  \n",
       "186        neutral        negative  \n",
       "200        neutral         neutral  \n",
       "216       negative        negative  \n",
       "226        neutral         neutral  \n",
       "236        neutral         neutral  \n",
       "248       negative        negative  \n",
       "267        neutral        negative  \n",
       "284       positive        positive  \n",
       "300       positive        positive  \n",
       "311        neutral        negative  \n",
       "327       positive        positive  \n",
       "340        neutral        negative  \n",
       "352       positive        positive  \n",
       "366       positive        positive  \n",
       "375       positive        positive  \n",
       "385        neutral        negative  \n",
       "392        neutral        negative  \n",
       "401       positive        positive  \n",
       "411       positive         neutral  \n",
       "429        neutral         neutral  \n",
       "444       negative        negative  \n",
       "458       positive        positive  \n",
       "472        neutral        positive  \n",
       "489       negative         neutral  \n",
       "504        neutral         neutral  \n",
       "518       positive        positive  \n",
       "529       negative        positive  \n",
       "544       negative         neutral  \n",
       "561       positive         neutral  \n",
       "569       positive        positive  \n",
       "576       negative        negative  \n",
       "593       negative        negative  \n",
       "601        neutral        negative  \n",
       "607        neutral        positive  \n",
       "617       positive        positive  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_withtm():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    input2 = Input(shape=(numtopics,))\n",
    "    #input3 = Input(shape=(numtopics*numsentilabel,))\n",
    "    \n",
    "    emb1 = Embedding(len(word_index) + 1, embed_dim1, weights=[embedding_matrix1], trainable=False)(input1)\n",
    "    emb2 = Embedding(len(word_index) + 1, embed_dim2, weights=[embedding_matrix2], trainable=False)(input1)\n",
    "    #cnn = Conv1D(filters=100,kernel_size=5,strides=1,padding=\"same\")(Concatenate(-1)([emb1,emb2]))\n",
    "    cnn = SpatialDropout1D(.2)(Concatenate(-1)([emb1,emb2]))\n",
    "    out = Bidirectional(LSTM(lstm_out, dropout=0.2,return_sequences=True))(cnn)\n",
    "    out = Bidirectional(LSTM(lstm_out//2, dropout=0.2, return_sequences=True))(out)\n",
    "    out = AttLayer(lstm_out//2)(out)\n",
    "    \n",
    "    out = Concatenate(-1)([out,input2])\n",
    "    \n",
    "    out = Dense(n_output,activation='softmax')(out)\n",
    "    model = Model([input1,input2],out)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(.0005),metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 20, 200)      3200200     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 20, 300)      4800300     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 20, 500)      0           embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 20, 500)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 20, 256)      644096      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 20, 128)      164352      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer_4 (AttLayer)          (None, 128)          8320        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 138)          0           att_layer_4[0][0]                \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            417         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,817,685\n",
      "Trainable params: 817,185\n",
      "Non-trainable params: 8,000,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model_withtm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tm1 = ljst_model.dt_distribution[:len(train_texts)]\n",
    "val_tm1 = ljst_model.dt_distribution[len(train_texts):len(train_texts)+len(val_texts)]\n",
    "test_tm1 = ljst_model.dt_distribution[len(train_texts)+len(val_texts):]\n",
    "\n",
    "train_tm2 = ljst_model.dts_distribution[:len(train_texts)].reshape(len(train_texts),-1)\n",
    "val_tm2 = ljst_model.dts_distribution[len(train_texts):len(train_texts)+len(val_texts)].reshape(len(val_texts),-1)\n",
    "test_tm2 = ljst_model.dts_distribution[len(train_texts)+len(val_texts):].reshape(len(test_texts),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      " - 32s - loss: 0.9028 - acc: 0.5614 - val_loss: 0.8747 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58800, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 2/50\n",
      " - 27s - loss: 0.8697 - acc: 0.5826 - val_loss: 0.8643 - val_acc: 0.5830\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.58800\n",
      "Epoch 3/50\n",
      " - 27s - loss: 0.8608 - acc: 0.5873 - val_loss: 0.8490 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58800 to 0.61000, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 4/50\n",
      " - 27s - loss: 0.8511 - acc: 0.5930 - val_loss: 0.8403 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61000\n",
      "Epoch 5/50\n",
      " - 27s - loss: 0.8462 - acc: 0.5994 - val_loss: 0.8417 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61000 to 0.61600, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 6/50\n",
      " - 31s - loss: 0.8378 - acc: 0.6056 - val_loss: 0.8290 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61600\n",
      "Epoch 7/50\n",
      " - 32s - loss: 0.8308 - acc: 0.6081 - val_loss: 0.8268 - val_acc: 0.6153\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.61600\n",
      "Epoch 8/50\n",
      " - 33s - loss: 0.8287 - acc: 0.6113 - val_loss: 0.8249 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.61600 to 0.61667, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 9/50\n",
      " - 30s - loss: 0.8161 - acc: 0.6211 - val_loss: 0.8311 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.61667\n",
      "Epoch 10/50\n",
      " - 30s - loss: 0.8134 - acc: 0.6246 - val_loss: 0.8356 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.61667\n",
      "Epoch 11/50\n",
      " - 30s - loss: 0.8074 - acc: 0.6271 - val_loss: 0.8210 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.61667 to 0.62133, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 12/50\n",
      " - 29s - loss: 0.8011 - acc: 0.6303 - val_loss: 0.8205 - val_acc: 0.6177\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62133\n",
      "Epoch 13/50\n",
      " - 30s - loss: 0.7931 - acc: 0.6354 - val_loss: 0.8235 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62133\n",
      "Epoch 14/50\n",
      " - 30s - loss: 0.7898 - acc: 0.6402 - val_loss: 0.8251 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62133\n",
      "Epoch 15/50\n",
      " - 33s - loss: 0.7840 - acc: 0.6457 - val_loss: 0.8332 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62133\n",
      "Epoch 16/50\n",
      " - 31s - loss: 0.7740 - acc: 0.6468 - val_loss: 0.8444 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00035000001662410796.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62133\n",
      "Epoch 17/50\n",
      " - 32s - loss: 0.7579 - acc: 0.6609 - val_loss: 0.8420 - val_acc: 0.6073\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62133\n",
      "Epoch 18/50\n",
      " - 32s - loss: 0.7532 - acc: 0.6624 - val_loss: 0.8343 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.62133 to 0.62167, saving model to ../models/weights_finalmodel2.hdf5\n",
      "Epoch 19/50\n",
      " - 30s - loss: 0.7427 - acc: 0.6659 - val_loss: 0.8401 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62167\n",
      "Epoch 20/50\n",
      " - 31s - loss: 0.7344 - acc: 0.6741 - val_loss: 0.8585 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62167\n",
      "Epoch 21/50\n",
      " - 31s - loss: 0.7279 - acc: 0.6752 - val_loss: 0.8639 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62167\n",
      "Epoch 22/50\n",
      " - 31s - loss: 0.7194 - acc: 0.6801 - val_loss: 0.8649 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62167\n",
      "Epoch 23/50\n",
      " - 31s - loss: 0.7089 - acc: 0.6811 - val_loss: 0.8754 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00024500001163687554.\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62167\n",
      "Epoch 24/50\n",
      " - 33s - loss: 0.6916 - acc: 0.6991 - val_loss: 0.8862 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62167\n",
      "Epoch 25/50\n",
      " - 29s - loss: 0.6788 - acc: 0.7049 - val_loss: 0.9039 - val_acc: 0.6017\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62167\n",
      "Epoch 26/50\n",
      " - 30s - loss: 0.6671 - acc: 0.7092 - val_loss: 0.8928 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62167\n",
      "Epoch 27/50\n",
      " - 34s - loss: 0.6627 - acc: 0.7103 - val_loss: 0.9236 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62167\n",
      "Epoch 28/50\n",
      " - 32s - loss: 0.6523 - acc: 0.7162 - val_loss: 0.9252 - val_acc: 0.6017\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00017150000203400848.\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62167\n",
      "Epoch 29/50\n",
      " - 31s - loss: 0.6368 - acc: 0.7267 - val_loss: 0.9271 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62167\n",
      "Epoch 30/50\n",
      " - 32s - loss: 0.6271 - acc: 0.7306 - val_loss: 0.9472 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62167\n",
      "Epoch 31/50\n",
      " - 30s - loss: 0.6226 - acc: 0.7296 - val_loss: 0.9487 - val_acc: 0.5983\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62167\n",
      "Epoch 32/50\n",
      " - 36s - loss: 0.6109 - acc: 0.7383 - val_loss: 0.9632 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62167\n",
      "Epoch 33/50\n",
      " - 32s - loss: 0.6092 - acc: 0.7390 - val_loss: 0.9535 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00012004999734926967.\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62167\n",
      "Epoch 00033: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "lr = ReduceLROnPlateau(monitor='val_acc', factor=0.7, patience=5, verbose=1, mode='auto', min_lr=0.000001)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc',filepath='../models/weights_finalmodel2.hdf5', mode='max',verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit([trainX,train_tm1], trainy, epochs = n_epochs, batch_size=batch_size, verbose = 2, validation_data=([valX,val_tm1],valy), callbacks=[early,lr,checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict([valX,val_tm1]).argmax(axis=1)\n",
    "val_texts['sentiment_pred3'] = [in_senti_dict[i] for i in val_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_pred2</th>\n",
       "      <th>sentiment_pred3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>gonna start another june sour note uhhhh yes y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>caring bohot jyada caring courier wale bsdk si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>sarfaraza nonesense kabhi baymani per bani tea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>45</td>\n",
       "      <td>pakistani team effort aagey allah marziiiiiiii...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>61</td>\n",
       "      <td>ankita shah8 bhadve photo elections pehle jyad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>64</td>\n",
       "      <td>krishna jcb full trend chal rahi</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>77</td>\n",
       "      <td>sharma1 kavita sharma4 sunita kal jab evm seal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>81</td>\n",
       "      <td>accha kiya invite nai kiya corrupted party ind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>101</td>\n",
       "      <td>nolo weni ankere gae weekend</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>145</td>\n",
       "      <td>abe kutte sakal musalmaan wazah tere ghar roti...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>167</td>\n",
       "      <td>mohsin dawartweets daikho aur kuch kha ker mar...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>252</td>\n",
       "      <td>haha soldier reading american gods hardy har h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>288</td>\n",
       "      <td>dor song jis adnan sami mujahid part play keya...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>318</td>\n",
       "      <td>government kya kuch nahi kar sakti tumhare bhe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>337</td>\n",
       "      <td>rao hindi tumhari bass nahi nahi seekh paoge d...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>348</td>\n",
       "      <td>ch1 pakeeza language istemaal koi nooni bhayoo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>352</td>\n",
       "      <td>gulwish nahi news defend kerne koshesh ker rhe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>359</td>\n",
       "      <td>ahmad884 lulli hoti nahi khadi baten karalo ba...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>416</td>\n",
       "      <td>live urdu fridaysermon delivered head ahmadiyy...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>439</td>\n",
       "      <td>color tum india wale sari yahi sochti pakistan...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>462</td>\n",
       "      <td>world cup mein mard nahi khusre gae hein larna...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>466</td>\n",
       "      <td>bhaijaan please mein apka bahot bahot bada fan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>469</td>\n",
       "      <td>postopinions take care bibi allah khair stupid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>497</td>\n",
       "      <td>scratch kfc bhi auqaat nahi rahee todays defea...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>529</td>\n",
       "      <td>hai sabse super modi hai sabse uper modi deshk...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>561</td>\n",
       "      <td>mazhab naam par election jeetna koi bada kaarn...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>569</td>\n",
       "      <td>may allah bless strong imaan taqwa piety pure ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>576</td>\n",
       "      <td>super hilarious true reality noora haramkhor f...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>605</td>\n",
       "      <td>skfc good morning cute kartikeyan annaa happy ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>629</td>\n",
       "      <td>hadd hoti hai wrong make account live</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>712</td>\n",
       "      <td>hahahahaah fuck koy salig lucas doh hapit inju...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>715</td>\n",
       "      <td>sacrificed lot happy normal lad liverpool whos...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>740</td>\n",
       "      <td>yeah aap hamara roza kharaab karay hain yeh us...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>743</td>\n",
       "      <td>bollyhungama vidya balan monday bakri eid wom ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>745</td>\n",
       "      <td>amitbehere use ram bas aur yehi log bolte hind...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>747</td>\n",
       "      <td>sai institute manimajra chandigarh best place ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>775</td>\n",
       "      <td>jai shree ram jay hanuman jai shree krishna ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>781</td>\n",
       "      <td>khan sahab apko vote diya taky bany alhamdulil...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>797</td>\n",
       "      <td>sir mere rs1180 deduct gaya hai kekin branch o...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>807</td>\n",
       "      <td>would bring great joy hear rendition des ree g...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>837</td>\n",
       "      <td>sale owner split level home great neighborhood...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>870</td>\n",
       "      <td>scindia meri baat nhi suni apne congress prem ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>880</td>\n",
       "      <td>dil khush krny ghalib khyal acha myth efnod8ijpj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>885</td>\n",
       "      <td>ngk good one kudos heroes offl thisisysr</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>892</td>\n",
       "      <td>officeofknath sahi hoga sayad tabhi sirf appki...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>895</td>\n",
       "      <td>ez1403 pakistani pig dukkar kutta suvar mix aulad</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>906</td>\n",
       "      <td>bies blocked aapke acche din gaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>939</td>\n",
       "      <td>kalia thaa want hindi uneducated people nice c...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>962</td>\n",
       "      <td>eid mubarak hadiya trabaho vlogs cdjaercsyo</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                                               text sentiment  \\\n",
       "0      3  pakistan ghra tauq pakistan israel tasleem nah...  negative   \n",
       "13    12  gonna start another june sour note uhhhh yes y...   neutral   \n",
       "23    23  caring bohot jyada caring courier wale bsdk si...  negative   \n",
       "35    24  sarfaraza nonesense kabhi baymani per bani tea...  positive   \n",
       "49    45  pakistani team effort aagey allah marziiiiiiii...  positive   \n",
       "59    61  ankita shah8 bhadve photo elections pehle jyad...  negative   \n",
       "69    64                   krishna jcb full trend chal rahi  positive   \n",
       "75    77  sharma1 kavita sharma4 sunita kal jab evm seal...  negative   \n",
       "90    81  accha kiya invite nai kiya corrupted party ind...  negative   \n",
       "103  101                       nolo weni ankere gae weekend  positive   \n",
       "108  145  abe kutte sakal musalmaan wazah tere ghar roti...  negative   \n",
       "125  167  mohsin dawartweets daikho aur kuch kha ker mar...  negative   \n",
       "142  252  haha soldier reading american gods hardy har h...  negative   \n",
       "154  288  dor song jis adnan sami mujahid part play keya...   neutral   \n",
       "170  318  government kya kuch nahi kar sakti tumhare bhe...  negative   \n",
       "186  337  rao hindi tumhari bass nahi nahi seekh paoge d...  negative   \n",
       "200  348  ch1 pakeeza language istemaal koi nooni bhayoo...   neutral   \n",
       "216  352  gulwish nahi news defend kerne koshesh ker rhe...   neutral   \n",
       "226  359  ahmad884 lulli hoti nahi khadi baten karalo ba...   neutral   \n",
       "236  416  live urdu fridaysermon delivered head ahmadiyy...   neutral   \n",
       "248  439  color tum india wale sari yahi sochti pakistan...  positive   \n",
       "267  462  world cup mein mard nahi khusre gae hein larna...  negative   \n",
       "284  466  bhaijaan please mein apka bahot bahot bada fan...   neutral   \n",
       "300  469  postopinions take care bibi allah khair stupid...  negative   \n",
       "311  497  scratch kfc bhi auqaat nahi rahee todays defea...  negative   \n",
       "327  529  hai sabse super modi hai sabse uper modi deshk...   neutral   \n",
       "340  561  mazhab naam par election jeetna koi bada kaarn...  negative   \n",
       "352  569  may allah bless strong imaan taqwa piety pure ...  positive   \n",
       "366  576  super hilarious true reality noora haramkhor f...   neutral   \n",
       "375  605  skfc good morning cute kartikeyan annaa happy ...   neutral   \n",
       "385  629              hadd hoti hai wrong make account live  negative   \n",
       "392  712  hahahahaah fuck koy salig lucas doh hapit inju...   neutral   \n",
       "401  715  sacrificed lot happy normal lad liverpool whos...   neutral   \n",
       "411  740  yeah aap hamara roza kharaab karay hain yeh us...   neutral   \n",
       "429  743  bollyhungama vidya balan monday bakri eid wom ...  positive   \n",
       "444  745  amitbehere use ram bas aur yehi log bolte hind...  negative   \n",
       "458  747  sai institute manimajra chandigarh best place ...   neutral   \n",
       "472  775  jai shree ram jay hanuman jai shree krishna ja...   neutral   \n",
       "489  781  khan sahab apko vote diya taky bany alhamdulil...   neutral   \n",
       "504  797  sir mere rs1180 deduct gaya hai kekin branch o...   neutral   \n",
       "518  807  would bring great joy hear rendition des ree g...   neutral   \n",
       "529  837  sale owner split level home great neighborhood...  positive   \n",
       "544  870  scindia meri baat nhi suni apne congress prem ...  negative   \n",
       "561  880   dil khush krny ghalib khyal acha myth efnod8ijpj   neutral   \n",
       "569  885           ngk good one kudos heroes offl thisisysr  positive   \n",
       "576  892  officeofknath sahi hoga sayad tabhi sirf appki...   neutral   \n",
       "593  895  ez1403 pakistani pig dukkar kutta suvar mix aulad  negative   \n",
       "601  906                  bies blocked aapke acche din gaye   neutral   \n",
       "607  939  kalia thaa want hindi uneducated people nice c...   neutral   \n",
       "617  962        eid mubarak hadiya trabaho vlogs cdjaercsyo   neutral   \n",
       "\n",
       "    sentiment_pred sentiment_pred2 sentiment_pred3  \n",
       "0         negative        positive         neutral  \n",
       "13        positive        positive        positive  \n",
       "23        negative        negative        negative  \n",
       "35        negative         neutral         neutral  \n",
       "49        positive         neutral        positive  \n",
       "59        negative        negative        negative  \n",
       "69         neutral         neutral         neutral  \n",
       "75        negative        negative         neutral  \n",
       "90        negative         neutral        negative  \n",
       "103        neutral         neutral         neutral  \n",
       "108       negative        negative        negative  \n",
       "125       negative        negative        negative  \n",
       "142       positive        positive        positive  \n",
       "154        neutral        positive         neutral  \n",
       "170       negative        negative        negative  \n",
       "186        neutral        negative         neutral  \n",
       "200        neutral         neutral         neutral  \n",
       "216       negative        negative        negative  \n",
       "226        neutral         neutral         neutral  \n",
       "236        neutral         neutral         neutral  \n",
       "248       negative        negative        negative  \n",
       "267        neutral        negative         neutral  \n",
       "284       positive        positive        positive  \n",
       "300       positive        positive        positive  \n",
       "311        neutral        negative         neutral  \n",
       "327       positive        positive         neutral  \n",
       "340        neutral        negative         neutral  \n",
       "352       positive        positive        positive  \n",
       "366       positive        positive        negative  \n",
       "375       positive        positive        positive  \n",
       "385        neutral        negative        negative  \n",
       "392        neutral        negative        negative  \n",
       "401       positive        positive         neutral  \n",
       "411       positive         neutral        positive  \n",
       "429        neutral         neutral        negative  \n",
       "444       negative        negative         neutral  \n",
       "458       positive        positive        positive  \n",
       "472        neutral        positive         neutral  \n",
       "489       negative         neutral        negative  \n",
       "504        neutral         neutral         neutral  \n",
       "518       positive        positive        positive  \n",
       "529       negative        positive        positive  \n",
       "544       negative         neutral        negative  \n",
       "561       positive         neutral         neutral  \n",
       "569       positive        positive        positive  \n",
       "576       negative        negative         neutral  \n",
       "593       negative        negative        negative  \n",
       "601        neutral        negative         neutral  \n",
       "607        neutral        positive         neutral  \n",
       "617       positive        positive        positive  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6023244648780471\n",
      "0.5973333333333334\n"
     ]
    }
   ],
   "source": [
    "print (f1_score(val_texts.sentiment,val_texts.sentiment_pred3,average='macro'))\n",
    "print (accuracy_score(val_texts.sentiment,val_texts.sentiment_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts[['uid','sentiment_pred']].to_csv('../answer.txt',header=['Uid','Sentiment'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts['sentiment'] = test_texts.sentiment_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts[['uid','sentiment']].to_csv('../answer.txt',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
