{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open('../data/train_14k_split_conll.txt','r',encoding='utf8')\n",
    "line_train = f_train.readlines()\n",
    "\n",
    "f_val = open('../data/dev_3k_split_conll.txt','r',encoding='utf8')\n",
    "line_val = f_val.readlines()\n",
    "\n",
    "f_test = open('../data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='utf8')\n",
    "line_test = f_test.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_doc_sentiment = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                sentiment = line.split('\\t')[2]\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_doc_sentiment.append(sentiment)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    train_df['sentiment'] = train_doc_sentiment\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip().lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data(line_train)\n",
    "val_df = get_data(line_val)\n",
    "test_df = get_data_test(line_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289253, 5), (62618, 5), (61735, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3000, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.uid.nunique(), val_df.uid.nunique(), test_df.uid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223215, 5)\n",
      "(48339, 5)\n",
      "(47683, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.word != 'http']\n",
    "train_df = train_df[train_df.word != 'https']\n",
    "train_df = train_df[train_df.word_type != 'o']\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df = val_df[val_df.word != 'http']\n",
    "val_df = val_df[val_df.word != 'https']\n",
    "val_df = val_df[val_df.word_type != 'o']\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df = test_df[test_df.word != 'http']\n",
    "test_df = test_df[test_df.word != 'https']\n",
    "test_df = test_df[test_df.word_type != 'o']\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175903, 5)\n",
      "(38082, 5)\n",
      "(37583, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df.word = train_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "train_df = train_df[train_df.word.str.len() >= 3]\n",
    "print (train_df.shape)\n",
    "\n",
    "val_df.word = val_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "val_df = val_df[val_df.word.str.len() >= 3]\n",
    "print (val_df.shape)\n",
    "\n",
    "test_df.word = test_df.word.apply(lambda x: re.sub(\"[^a-zA-Z0-9]\", \"\",x))\n",
    "test_df = test_df[test_df.word.str.len() >= 3]\n",
    "print (test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words 51515\n"
     ]
    }
   ],
   "source": [
    "all_words = set(pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0).word)\n",
    "print (\"Total number of words {}\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16771, 2)\n",
      "   word  tot_count\n",
      "0   hai       5902\n",
      "1  nahi       3744\n",
      "2   bhi       1965\n",
      "3   aur       1799\n",
      "4  modi       1415\n"
     ]
    }
   ],
   "source": [
    "all_words = pd.concat([train_df[['word']],val_df[['word']],test_df[['word']]], axis=0)\n",
    "all_words = all_words.word.value_counts().reset_index()\n",
    "all_words.columns = ['word','tot_count']\n",
    "top_words = all_words[all_words.tot_count >= 2]\n",
    "print (top_words.shape)\n",
    "print (top_words.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bert_words = pd.DataFrame(['[PAD]','[UNK]','[CLS]','[SEP]'],columns=['word'])\n",
    "all_bert_words = pd.concat([all_bert_words,top_words[['word']].drop_duplicates().reset_index(drop=True)],axis=0)\n",
    "all_bert_words.to_csv(\"../bert_vocab.txt\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "train_texts = pd.merge(train_texts,train_df[['uid','sentiment']],how='left').drop_duplicates().reset_index(drop=True)\n",
    "train_texts.columns = ['uid','text','sentiment']\n",
    "#train_texts.text = train_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
    "\n",
    "val_texts = val_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "val_texts = pd.merge(val_texts,val_df[['uid','sentiment']],how='left').drop_duplicates()\n",
    "val_texts.columns = ['uid','text','sentiment']\n",
    "#val_texts.text = val_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n",
    "\n",
    "test_texts = test_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "test_texts.columns = ['uid','text']\n",
    "#test_texts.text = test_texts.text.apply(lambda x: '[CLS] ' + x + ' [SEP]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>love looks good maddie ako lang yung sobrang m...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>min lyching manakgupta mein kahna nae chahta q...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>best luck sir world cup liye bhot bhot subhkam...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>yes great dialogues one also chupke chupke chh...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>tarekfatah tere baap liye bola kya tha bhadwe ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment\n",
       "0    8  love looks good maddie ako lang yung sobrang m...   neutral\n",
       "1   14  min lyching manakgupta mein kahna nae chahta q...   neutral\n",
       "2   26  best luck sir world cup liye bhot bhot subhkam...  positive\n",
       "3   27  yes great dialogues one also chupke chupke chh...  positive\n",
       "4   33  tarekfatah tere baap liye bola kya tha bhadwe ...  negative"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>keh aese rahe jaise pakistan wale karte south ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>anus prerna way ran saving ram ram jay rajaraa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>shukar hai pathan nae warna ptm nay wae dramay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>harsh pen decision much option arm big second ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>keep saying kenyan rugby beautiful save kru dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text\n",
       "0    1  keh aese rahe jaise pakistan wale karte south ...\n",
       "1    4  anus prerna way ran saving ram ram jay rajaraa...\n",
       "2   17  shukar hai pathan nae warna ptm nay wae dramay...\n",
       "3   18  harsh pen decision much option arm big second ...\n",
       "4   34  keep saying kenyan rugby beautiful save kru dr..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     6392\n",
       "positive    5616\n",
       "negative    4992\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = pd.concat([train_texts[['uid','text','sentiment']],val_texts[['uid','text','sentiment']]],axis=0)\n",
    "all_texts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 21:23:37.660035 4645359040 file_utils.py:38] PyTorch version 1.2.0 available.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 21:23:49.973178 4645359040 tokenization_utils.py:335] Model name '../bert_vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../bert_vocab.txt' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "W0226 21:23:49.974266 4645359040 tokenization_utils.py:347] Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "I0226 21:23:49.975219 4645359040 tokenization_utils.py:416] loading file ../bert_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../bert_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = all_texts.sentiment.nunique() #number of possible outputs\n",
    "senti_dict = {'negative':0,'neutral':1,'positive':2}\n",
    "in_senti_dict = {0:'negative',1:'neutral',2:'positive'}\n",
    "all_texts.sentiment = all_texts.sentiment.apply(lambda x: senti_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [00:04<00:00, 3798.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "for text in tqdm(all_texts.text.values.tolist()):\n",
    "    train_tokens += [tokenizer.encode(text,add_special_tokens=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 3306.15it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens = []\n",
    "for text in tqdm(test_texts.text.values.tolist()):\n",
    "    test_tokens += [tokenizer.encode(text,add_special_tokens=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [00:00<00:00, 191819.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens_padded = []\n",
    "train_attention_mask = []\n",
    "train_seg_ids = []\n",
    "\n",
    "for tokens in tqdm(train_tokens):\n",
    "    tokens = tokens[:max_len]\n",
    "    token_len = len(tokens)\n",
    "    one_mask = [1]*token_len\n",
    "    zero_mask = [0]*(max_len-token_len)\n",
    "    padded_input = tokens + zero_mask\n",
    "    attention_mask = one_mask + zero_mask\n",
    "    \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == 3:\n",
    "            current_segment_id = 1\n",
    "    segments = segments + [0] * (max_len - len(tokens))\n",
    "    \n",
    "    train_tokens_padded += [padded_input]\n",
    "    train_attention_mask += [attention_mask]\n",
    "    train_seg_ids += [segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 192431.63it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tokens_padded = []\n",
    "test_attention_mask = []\n",
    "test_seg_ids = []\n",
    "\n",
    "for tokens in tqdm(test_tokens):\n",
    "    tokens = tokens[:max_len]\n",
    "    token_len = len(tokens)\n",
    "    one_mask = [1]*token_len\n",
    "    zero_mask = [0]*(max_len-token_len)\n",
    "    padded_input = tokens + zero_mask\n",
    "    attention_mask = one_mask + zero_mask\n",
    "    \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == 102:\n",
    "            current_segment_id = 1\n",
    "    segments = segments + [0] * (max_len - len(tokens))\n",
    "    \n",
    "    test_tokens_padded += [padded_input]\n",
    "    test_attention_mask += [attention_mask]\n",
    "    test_seg_ids += [segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 865, 17, 1, 2223, 2737, 2918, 9642, 8725, 3591, 86, 1, 9033, 2145, 1, 1, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (train_tokens_padded[0], train_attention_mask[0], train_seg_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = torch.LongTensor(to_categorical(all_texts.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_padded = torch.LongTensor(np.asarray(train_tokens_padded))\n",
    "train_attention_mask = torch.LongTensor(np.asarray(train_attention_mask))\n",
    "train_seg_ids = torch.LongTensor(np.asarray(train_seg_ids))\n",
    "test_tokens_padded = torch.LongTensor(np.asarray(test_tokens_padded))\n",
    "test_attention_mask = torch.LongTensor(np.asarray(test_attention_mask))\n",
    "test_seg_ids = torch.LongTensor(np.asarray(test_seg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17000, 20]) torch.Size([17000, 20]) torch.Size([17000, 20]) torch.Size([3000, 20]) torch.Size([3000, 20]) torch.Size([3000, 20])\n"
     ]
    }
   ],
   "source": [
    "print (train_tokens_padded.shape, train_attention_mask.shape, train_seg_ids.shape, test_tokens_padded.shape, test_attention_mask.shape, test_seg_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tokens_padded = train_tokens_padded[train_texts.shape[0]:]\n",
    "train_tokens_padded = train_tokens_padded[:train_texts.shape[0]]\n",
    "\n",
    "dev_attention_mask = train_attention_mask[train_texts.shape[0]:]\n",
    "train_attention_mask = train_attention_mask[:train_texts.shape[0]]\n",
    "\n",
    "dev_seg_ids = train_seg_ids[train_texts.shape[0]:]\n",
    "train_seg_ids = train_seg_ids[:train_texts.shape[0]]\n",
    "\n",
    "dev_output = train_output[train_texts.shape[0]:]\n",
    "train_output = train_output[:train_texts.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_tokens_padded, train_output)\n",
    "val_data = TensorDataset(dev_tokens_padded, dev_output)\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, nout, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, nout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        #print (src.shape)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(torch.mean(output,1))\n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) #.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (self.pe.shape)\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ntokens = tokenizer.vocab_size # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "nout = all_texts.sentiment.nunique()\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nout, dropout)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(16775, 200)\n",
       "  (decoder): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters to learn 4323603\n"
     ]
    }
   ],
   "source": [
    "print (\"Total number of parameters to learn {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if device == 'cpu':\n",
    "        rounded_preds = preds.detach().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().numpy().argmax(1)\n",
    "    else:\n",
    "        rounded_preds = preds.detach().cpu().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().cpu().numpy().argmax(1)\n",
    "    \n",
    "    return accuracy_score(rounded_correct,rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_torch(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    if device == 'cpu':\n",
    "        rounded_preds = preds.detach().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().numpy().argmax(1)\n",
    "    else:\n",
    "        rounded_preds = preds.detach().cpu().numpy().argmax(1)\n",
    "        rounded_correct = y.detach().cpu().numpy().argmax(1)\n",
    "    \n",
    "    return f1_score(rounded_correct,rounded_preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    global predictions, labels, loss\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    f1_scores = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    counter = 0\n",
    "    for tokens, labels in tqdm(train_loader):\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(tokens) #.squeeze(1)\n",
    "        predictions = torch.softmax(predictions,dim=-1)\n",
    "        \n",
    "        #loss = criterion(predictions, labels)\n",
    "        loss = criterion(predictions, torch.max(labels, 1)[1])\n",
    "        \n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        f1_score_batch = f1_torch(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        f1_scores += f1_score_batch\n",
    "        \n",
    "    return epoch_loss / counter, epoch_acc / counter, f1_scores/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    f1_scores = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for tokens, labels in tqdm(val_loader):\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "            predictions = model(tokens) #.squeeze(1)\n",
    "            predictions = torch.softmax(predictions,dim=-1)\n",
    "            \n",
    "            #loss = criterion(predictions, labels)\n",
    "            loss = criterion(predictions, torch.max(labels, 1)[1])\n",
    "\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            f1_score_batch = f1_torch(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            f1_scores += f1_score_batch\n",
    "        \n",
    "    return epoch_loss / counter, epoch_acc / counter, f1_scores/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:23<00:00,  1.31it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.66it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 26s\n",
      "\tTrain Loss: 1.076 | Train Acc: 44.01% | Train F1: 31.11%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 45.55% | Val F1: 35.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:22<00:00,  1.34it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.68it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 1m 25s\n",
      "\tTrain Loss: 0.961 | Train Acc: 56.94% | Train F1: 52.26%\n",
      "\t Val. Loss: 0.983 |  Val. Acc: 53.86% | Val F1: 53.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:21<00:00,  1.35it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.05it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 1m 24s\n",
      "\tTrain Loss: 0.877 | Train Acc: 67.27% | Train F1: 66.28%\n",
      "\t Val. Loss: 0.961 |  Val. Acc: 56.89% | Val F1: 57.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:25<00:00,  1.29it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.19it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 1m 28s\n",
      "\tTrain Loss: 0.809 | Train Acc: 74.56% | Train F1: 74.33%\n",
      "\t Val. Loss: 0.962 |  Val. Acc: 57.23% | Val F1: 57.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:23<00:00,  1.31it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.61it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 1m 27s\n",
      "\tTrain Loss: 0.768 | Train Acc: 78.82% | Train F1: 78.67%\n",
      "\t Val. Loss: 0.966 |  Val. Acc: 57.32% | Val F1: 57.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:21<00:00,  1.36it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.63it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 1m 24s\n",
      "\tTrain Loss: 0.741 | Train Acc: 81.74% | Train F1: 81.60%\n",
      "\t Val. Loss: 0.977 |  Val. Acc: 55.17% | Val F1: 55.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:21<00:00,  1.36it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.65it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 1m 24s\n",
      "\tTrain Loss: 0.721 | Train Acc: 83.82% | Train F1: 83.72%\n",
      "\t Val. Loss: 0.974 |  Val. Acc: 56.13% | Val F1: 56.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:22<00:00,  1.33it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.83it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 1m 25s\n",
      "\tTrain Loss: 0.707 | Train Acc: 85.16% | Train F1: 85.10%\n",
      "\t Val. Loss: 0.976 |  Val. Acc: 55.51% | Val F1: 55.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:25<00:00,  1.28it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.15it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 1m 29s\n",
      "\tTrain Loss: 0.696 | Train Acc: 86.18% | Train F1: 86.05%\n",
      "\t Val. Loss: 0.978 |  Val. Acc: 55.75% | Val F1: 55.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:26<00:00,  1.27it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.80it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 1m 30s\n",
      "\tTrain Loss: 0.687 | Train Acc: 86.82% | Train F1: 86.75%\n",
      "\t Val. Loss: 0.986 |  Val. Acc: 55.07% | Val F1: 55.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:26<00:00,  1.27it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.07it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Epoch Time: 1m 30s\n",
      "\tTrain Loss: 0.678 | Train Acc: 87.94% | Train F1: 87.90%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 54.91% | Val F1: 54.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:26<00:00,  1.27it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.70it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Epoch Time: 1m 29s\n",
      "\tTrain Loss: 0.677 | Train Acc: 88.01% | Train F1: 87.97%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 55.06% | Val F1: 55.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:23<00:00,  1.31it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.42it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Epoch Time: 1m 27s\n",
      "\tTrain Loss: 0.669 | Train Acc: 88.70% | Train F1: 88.64%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 55.23% | Val F1: 55.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:29<00:00,  1.22it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.36it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.666 | Train Acc: 88.88% | Train F1: 88.77%\n",
      "\t Val. Loss: 0.986 |  Val. Acc: 55.10% | Val F1: 55.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:33<00:00,  1.18it/s]\n",
      "100%|██████████| 24/24 [00:04<00:00,  5.75it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Epoch Time: 1m 37s\n",
      "\tTrain Loss: 0.660 | Train Acc: 89.42% | Train F1: 89.35%\n",
      "\t Val. Loss: 0.988 |  Val. Acc: 55.44% | Val F1: 55.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:37<00:00,  1.13it/s]\n",
      "100%|██████████| 24/24 [00:04<00:00,  5.30it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Epoch Time: 1m 41s\n",
      "\tTrain Loss: 0.655 | Train Acc: 90.03% | Train F1: 89.94%\n",
      "\t Val. Loss: 0.991 |  Val. Acc: 54.63% | Val F1: 54.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:32<00:00,  1.19it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.18it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Epoch Time: 1m 36s\n",
      "\tTrain Loss: 0.651 | Train Acc: 90.45% | Train F1: 90.39%\n",
      "\t Val. Loss: 0.992 |  Val. Acc: 54.84% | Val F1: 54.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:34<00:00,  1.17it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  6.42it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Epoch Time: 1m 38s\n",
      "\tTrain Loss: 0.650 | Train Acc: 90.42% | Train F1: 90.36%\n",
      "\t Val. Loss: 0.990 |  Val. Acc: 54.83% | Val F1: 54.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:26<00:00,  1.27it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.44it/s]\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Epoch Time: 1m 30s\n",
      "\tTrain Loss: 0.646 | Train Acc: 90.76% | Train F1: 90.71%\n",
      "\t Val. Loss: 0.988 |  Val. Acc: 55.42% | Val F1: 55.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:24<00:00,  1.31it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Epoch Time: 1m 27s\n",
      "\tTrain Loss: 0.643 | Train Acc: 90.99% | Train F1: 90.94%\n",
      "\t Val. Loss: 0.994 |  Val. Acc: 54.56% | Val F1: 54.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = 999\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, val_loader, criterion)\n",
    "       \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '../models/model_transformer.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val F1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
