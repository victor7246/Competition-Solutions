{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open('../data/train_14k_split_conll.txt','r',encoding='utf8')\n",
    "line_train = f_train.readlines()\n",
    "\n",
    "f_val = open('../data/dev_3k_split_conll.txt','r',encoding='utf8')\n",
    "line_val = f_val.readlines()\n",
    "\n",
    "f_test = open('../data/Hindi_test_unalbelled_conll_updated.txt','r',encoding='utf8')\n",
    "line_test = f_test.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_doc_sentiment = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip() #.lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                sentiment = line.split('\\t')[2]\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_doc_sentiment.append(sentiment)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    train_df['sentiment'] = train_doc_sentiment\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(lines):\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    #global train_word, train_word_type, train_doc_sentiment, train_uid, train_doc_id\n",
    "    train_word = []\n",
    "    train_word_type = []\n",
    "    train_uid = []\n",
    "    train_doc_id = []\n",
    "\n",
    "    while i < len(lines):\n",
    "        '''\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['http','https']:\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i].split('\\t')[0] != '/':\n",
    "                i += 1\n",
    "                \n",
    "            i += 1    \n",
    "        ''' \n",
    "\n",
    "        if i < len(lines) and lines[i].split('\\t')[0] in ['@']:\n",
    "            i += 2\n",
    "\n",
    "        if i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.replace('\\n','').strip() #.lower()\n",
    "            if line.split('\\t')[0] == 'meta':\n",
    "                doc_count += 1\n",
    "                uid = int(line.split('\\t')[1])\n",
    "                #if doc_count == 1575:\n",
    "                #    print (uid)\n",
    "                i += 1\n",
    "\n",
    "            elif len(line.split('\\t')) >= 2:\n",
    "                if line.split('\\t')[0] not in stop:\n",
    "                    train_uid.append(uid)\n",
    "                    train_word.append(line.split('\\t')[0])\n",
    "                    train_word_type.append(line.split('\\t')[1])\n",
    "                    train_doc_id.append(doc_count)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['doc_id'] = train_doc_id\n",
    "    train_df['word'] = train_word\n",
    "    train_df['word_type'] = train_word_type\n",
    "    train_df['uid'] = train_uid\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data(line_train)\n",
    "val_df = get_data(line_val)\n",
    "test_df = get_data_test(line_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_types = train_df.groupby(['uid','word_type'])['word'].count().reset_index().pivot_table(index='uid',columns='word_type',values='word').reset_index().fillna(0)\n",
    "val_word_types = val_df.groupby(['uid','word_type'])['word'].count().reset_index().pivot_table(index='uid',columns='word_type',values='word').reset_index().fillna(0)\n",
    "test_word_types = test_df.groupby(['uid','word_type'])['word'].count().reset_index().pivot_table(index='uid',columns='word_type',values='word').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "train_texts = pd.merge(train_texts,train_df[['uid','sentiment']],how='left').drop_duplicates().reset_index(drop=True)\n",
    "train_texts.columns = ['uid','text','sentiment']\n",
    "train_texts = pd.merge(train_texts,train_word_types,how='left').fillna(0)\n",
    "\n",
    "val_texts = val_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "val_texts = pd.merge(val_texts,val_df[['uid','sentiment']],how='left').drop_duplicates()\n",
    "val_texts.columns = ['uid','text','sentiment']\n",
    "val_texts = pd.merge(val_texts,val_word_types,how='left').fillna(0)\n",
    "\n",
    "test_texts = test_df.groupby(['uid'],sort=True)['word'].apply(lambda x: \" \".join(x)).reset_index().reset_index(drop=True)\n",
    "test_texts.columns = ['uid','text']\n",
    "test_texts = pd.merge(test_texts,test_word_types,how='left').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>EMT</th>\n",
       "      <th>Eng</th>\n",
       "      <th>Hin</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>RT Love looks good Maddie !!! Ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ Min _ Of _ Lyching @ manakgupta Mein kahna n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Best luck sir World Cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Yes . Great dialogues one . Also Chupke Chupke...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ TarekFatah Tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment  EMT  \\\n",
       "0    8  RT Love looks good Maddie !!! Ako lang ba yung...   neutral  0.0   \n",
       "1   14  @ Min _ Of _ Lyching @ manakgupta Mein kahna n...   neutral  0.0   \n",
       "2   26  Best luck sir World Cup ke liye bhot bhot subh...  positive  0.0   \n",
       "3   27  Yes . Great dialogues one . Also Chupke Chupke...  positive  0.0   \n",
       "4   33  @ TarekFatah Tu tere baap ke liye jo bola wo k...  negative  0.0   \n",
       "\n",
       "    Eng   Hin     O  \n",
       "0  16.0   4.0   3.0  \n",
       "1   6.0   9.0   7.0  \n",
       "2   3.0   7.0   0.0  \n",
       "3  11.0   7.0  10.0  \n",
       "4   2.0  18.0   6.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>EMT</th>\n",
       "      <th>Eng</th>\n",
       "      <th>Hin</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>RT Love looks good Maddie !!! Ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ Min _ Of _ Lyching @ manakgupta Mein kahna n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Best luck sir World Cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Yes . Great dialogues one . Also Chupke Chupke...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ TarekFatah Tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment  EMT  \\\n",
       "0    8  RT Love looks good Maddie !!! Ako lang ba yung...   neutral  0.0   \n",
       "1   14  @ Min _ Of _ Lyching @ manakgupta Mein kahna n...   neutral  0.0   \n",
       "2   26  Best luck sir World Cup ke liye bhot bhot subh...  positive  0.0   \n",
       "3   27  Yes . Great dialogues one . Also Chupke Chupke...  positive  0.0   \n",
       "4   33  @ TarekFatah Tu tere baap ke liye jo bola wo k...  negative  0.0   \n",
       "\n",
       "    Eng   Hin     O  \n",
       "0  16.0   4.0   3.0  \n",
       "1   6.0   9.0   7.0  \n",
       "2   3.0   7.0   0.0  \n",
       "3  11.0   7.0  10.0  \n",
       "4   2.0  18.0   6.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def uniq_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def len_char(text):\n",
    "    return len(text)\n",
    "\n",
    "def len_non_chars(text):\n",
    "    return len(text) - len(re.sub(\"[^a-zA-Z0-9]\", \"\",text))\n",
    "\n",
    "def num_caps_words(text):\n",
    "    return len([i for i in text.split() if i.capitalize() == i])\n",
    "\n",
    "def num_all_caps(text):\n",
    "    return len([i for i in text.split() if i.upper() == i])\n",
    "\n",
    "def num_repeat_words(text):\n",
    "    def word_extended(word):\n",
    "        i = 1\n",
    "        count = 0\n",
    "        while i < len(word):\n",
    "            if word[i] == word[i-1]:\n",
    "                count += 1\n",
    "            i += 1\n",
    "\n",
    "        if count > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    return sum([word_extended(i) for i in text.split()])\n",
    "\n",
    "def avg_word_len(text):\n",
    "    return np.mean([len(i) for i in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts['num_words'] = train_texts.text.apply(num_words)\n",
    "train_texts['num_uniq_words'] = train_texts.text.apply(uniq_words)\n",
    "train_texts['len_char'] = train_texts.text.apply(len_char)\n",
    "train_texts['avg_word_len'] = train_texts.text.apply(avg_word_len)\n",
    "train_texts['len_non_chars'] = train_texts.text.apply(len_non_chars)/train_texts['len_char']\n",
    "train_texts['num_caps_words'] = train_texts.text.apply(num_caps_words)/train_texts['num_words']\n",
    "train_texts['num_uniq_words'] = train_texts['num_uniq_words']/train_texts['num_words']\n",
    "train_texts['num_all_caps'] = train_texts.text.apply(num_all_caps)/train_texts['num_words']\n",
    "train_texts['num_repeat_words'] = train_texts.text.apply(num_repeat_words)/train_texts['num_words']\n",
    "\n",
    "train_texts['EMT'] = train_texts['EMT']/train_texts['num_words']\n",
    "train_texts['Eng'] = train_texts['Eng']/train_texts['num_words']\n",
    "train_texts['Hin'] = train_texts['Hin']/train_texts['num_words']\n",
    "train_texts['O'] = train_texts['O']/train_texts['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts['num_words'] = val_texts.text.apply(num_words)\n",
    "val_texts['num_uniq_words'] = val_texts.text.apply(uniq_words)\n",
    "val_texts['len_char'] = val_texts.text.apply(len_char)\n",
    "val_texts['avg_word_len'] = val_texts.text.apply(avg_word_len)\n",
    "val_texts['len_non_chars'] = val_texts.text.apply(len_non_chars)/val_texts['len_char']\n",
    "val_texts['num_caps_words'] = val_texts.text.apply(num_caps_words)/val_texts['num_words']\n",
    "val_texts['num_uniq_words'] = val_texts['num_uniq_words']/val_texts['num_words']\n",
    "val_texts['num_all_caps'] = val_texts.text.apply(num_all_caps)/val_texts['num_words']\n",
    "val_texts['num_repeat_words'] = val_texts.text.apply(num_repeat_words)/val_texts['num_words']\n",
    "\n",
    "val_texts['EMT'] = val_texts['EMT']/val_texts['num_words']\n",
    "val_texts['Eng'] = val_texts['Eng']/val_texts['num_words']\n",
    "val_texts['Hin'] = val_texts['Hin']/val_texts['num_words']\n",
    "val_texts['O'] = val_texts['O']/val_texts['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts['num_words'] = test_texts.text.apply(num_words)\n",
    "test_texts['num_uniq_words'] = test_texts.text.apply(uniq_words)\n",
    "test_texts['len_char'] = test_texts.text.apply(len_char)\n",
    "test_texts['avg_word_len'] = test_texts.text.apply(avg_word_len)\n",
    "test_texts['len_non_chars'] = test_texts.text.apply(len_non_chars)/test_texts['len_char']\n",
    "test_texts['num_caps_words'] = test_texts.text.apply(num_caps_words)/test_texts['num_words']\n",
    "test_texts['num_uniq_words'] = test_texts['num_uniq_words']/test_texts['num_words']\n",
    "test_texts['num_all_caps'] = test_texts.text.apply(num_all_caps)/test_texts['num_words']\n",
    "test_texts['num_repeat_words'] = test_texts.text.apply(num_repeat_words)/test_texts['num_words']\n",
    "\n",
    "test_texts['EMT'] = test_texts['EMT']/test_texts['num_words']\n",
    "test_texts['Eng'] = test_texts['Eng']/test_texts['num_words']\n",
    "test_texts['Hin'] = test_texts['Hin']/test_texts['num_words']\n",
    "test_texts['O'] = test_texts['O']/test_texts['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>EMT</th>\n",
       "      <th>Eng</th>\n",
       "      <th>Hin</th>\n",
       "      <th>O</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_uniq_words</th>\n",
       "      <th>len_char</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>len_non_chars</th>\n",
       "      <th>num_caps_words</th>\n",
       "      <th>num_all_caps</th>\n",
       "      <th>num_repeat_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>RT Love looks good Maddie !!! Ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110</td>\n",
       "      <td>3.826087</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ Min _ Of _ Lyching @ manakgupta Mein kahna n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>22</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>102</td>\n",
       "      <td>3.681818</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Best luck sir World Cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>53</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Yes . Great dialogues one . Also Chupke Chupke...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>28</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>124</td>\n",
       "      <td>3.464286</td>\n",
       "      <td>0.314516</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ TarekFatah Tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>26</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>116</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.284483</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>Desh bhakti baat wahi samajh sakte hai jo khud...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118</td>\n",
       "      <td>4.173913</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>Madarchod mulle ye mathura Nahi dikha tha jab ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>29</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>136</td>\n",
       "      <td>3.724138</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>Manya Pradhan Mantri mahoday Shriman Narendra ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.201550</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>_ NSharif Kiya tum apne baap ki oulad nahi .??...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>27</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>122</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>@ YouTube Are Tu aa rha h ki nhi Wo modi phir ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>121</td>\n",
       "      <td>3.357143</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment  EMT  \\\n",
       "0    8  RT Love looks good Maddie !!! Ako lang ba yung...   neutral  0.0   \n",
       "1   14  @ Min _ Of _ Lyching @ manakgupta Mein kahna n...   neutral  0.0   \n",
       "2   26  Best luck sir World Cup ke liye bhot bhot subh...  positive  0.0   \n",
       "3   27  Yes . Great dialogues one . Also Chupke Chupke...  positive  0.0   \n",
       "4   33  @ TarekFatah Tu tere baap ke liye jo bola wo k...  negative  0.0   \n",
       "5   38  Desh bhakti baat wahi samajh sakte hai jo khud...  negative  0.0   \n",
       "6   41  Madarchod mulle ye mathura Nahi dikha tha jab ...  negative  0.0   \n",
       "7   48  Manya Pradhan Mantri mahoday Shriman Narendra ...  positive  0.0   \n",
       "8   56  _ NSharif Kiya tum apne baap ki oulad nahi .??...  negative  0.0   \n",
       "9   59  @ YouTube Are Tu aa rha h ki nhi Wo modi phir ...   neutral  0.0   \n",
       "\n",
       "        Eng       Hin         O  num_words  num_uniq_words  len_char  \\\n",
       "0  0.695652  0.173913  0.130435         23        1.000000       110   \n",
       "1  0.272727  0.409091  0.318182         22        0.909091       102   \n",
       "2  0.300000  0.700000  0.000000         10        0.900000        53   \n",
       "3  0.392857  0.250000  0.357143         28        0.750000       124   \n",
       "4  0.076923  0.692308  0.230769         26        1.000000       116   \n",
       "5  0.130435  0.739130  0.130435         23        1.000000       118   \n",
       "6  0.103448  0.758621  0.137931         29        0.931034       136   \n",
       "7  0.136364  0.681818  0.181818         22        1.000000       129   \n",
       "8  0.111111  0.629630  0.259259         27        0.925926       122   \n",
       "9  0.178571  0.678571  0.142857         28        0.964286       121   \n",
       "\n",
       "   avg_word_len  len_non_chars  num_caps_words  num_all_caps  num_repeat_words  \n",
       "0      3.826087       0.245455        0.347826      0.217391          0.173913  \n",
       "1      3.681818       0.284314        0.545455      0.318182          0.136364  \n",
       "2      4.400000       0.169811        0.300000      0.000000          0.000000  \n",
       "3      3.464286       0.314516        0.785714      0.464286          0.142857  \n",
       "4      3.500000       0.284483        0.269231      0.230769          0.153846  \n",
       "5      4.173913       0.220339        0.260870      0.130435          0.130435  \n",
       "6      3.724138       0.242647        0.241379      0.137931          0.206897  \n",
       "7      4.909091       0.201550        0.636364      0.181818          0.136364  \n",
       "8      3.555556       0.295082        0.370370      0.259259          0.148148  \n",
       "9      3.357143       0.264463        0.321429      0.178571          0.178571  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=600, max_features=50000, min_df=3,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(max_df=600,min_df=3,max_features=50000,stop_words='english')\n",
    "vec.fit(train_texts.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = vec.transform(train_texts.text)\n",
    "cv_val = vec.transform(val_texts.text)\n",
    "cv_test = vec.transform(test_texts.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ako',\n",
       " 'ba',\n",
       " 'good',\n",
       " 'kasi',\n",
       " 'lang',\n",
       " 'looks',\n",
       " 'masaya',\n",
       " 'past',\n",
       " 'sobrang',\n",
       " 'sya',\n",
       " 'yung']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vec.get_feature_names()[i] for i in np.where(cv_train[0].toarray() > 0)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=15, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=15)\n",
    "lda.fit(cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train = lda.transform(cv_train)\n",
    "lda_val = lda.transform(cv_val)\n",
    "lda_test = lda.transform(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: india kya minister tha gandhi\n",
      "Topic #1: best like better miss think\n",
      "Topic #2: bhai diya kar teri wo\n",
      "Topic #3: jai ram shree sabka bharat\n",
      "Topic #4: bahut desh ek jeet aapko\n",
      "Topic #5: congress kya party tum janta\n",
      "Topic #6: hain mubarak pakistan band allah\n",
      "Topic #7: pakistan hoga mat toh tum\n",
      "Topic #8: tere teri tu sale abe\n",
      "Topic #9: big tum hy fan tco\n",
      "Topic #10: pm allah congratulations narendra karta\n",
      "Topic #11: world cup song wo super\n",
      "Topic #12: thank really __ way like\n",
      "Topic #13: hindu pe media muslim log\n",
      "Topic #14: good happy birthday day bless\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,vec.get_feature_names(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm1 = MinMaxScaler((0,1))\n",
    "mm2 = MinMaxScaler((0,1))\n",
    "mm3 = MinMaxScaler((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm1.fit(train_texts.num_words.values.reshape(-1,1))\n",
    "mm2.fit(train_texts.len_char.values.reshape(-1,1))\n",
    "mm3.fit(train_texts.avg_word_len.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts.num_words = mm1.transform(train_texts.num_words.values.reshape(-1,1))\n",
    "train_texts.len_char = mm2.transform(train_texts.len_char.values.reshape(-1,1))\n",
    "train_texts.avg_word_len = mm3.transform(train_texts.avg_word_len.values.reshape(-1,1))\n",
    "\n",
    "val_texts.num_words = mm1.transform(val_texts.num_words.values.reshape(-1,1))\n",
    "val_texts.len_char = mm2.transform(val_texts.len_char.values.reshape(-1,1))\n",
    "val_texts.avg_word_len = mm3.transform(val_texts.avg_word_len.values.reshape(-1,1))\n",
    "\n",
    "test_texts.num_words = mm1.transform(test_texts.num_words.values.reshape(-1,1))\n",
    "test_texts.len_char = mm2.transform(test_texts.len_char.values.reshape(-1,1))\n",
    "test_texts.avg_word_len = mm3.transform(test_texts.avg_word_len.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>EMT</th>\n",
       "      <th>Eng</th>\n",
       "      <th>Hin</th>\n",
       "      <th>O</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_uniq_words</th>\n",
       "      <th>len_char</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>len_non_chars</th>\n",
       "      <th>num_caps_words</th>\n",
       "      <th>num_all_caps</th>\n",
       "      <th>num_repeat_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>RT Love looks good Maddie !!! Ako lang ba yung...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>@ Min _ Of _ Lyching @ manakgupta Mein kahna n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.246261</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Best luck sir World Cup ke liye bhot bhot subh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.309677</td>\n",
       "      <td>0.333472</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Yes . Great dialogues one . Also Chupke Chupke...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.767742</td>\n",
       "      <td>0.219846</td>\n",
       "      <td>0.314516</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>@ TarekFatah Tu tere baap ke liye jo bola wo k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.716129</td>\n",
       "      <td>0.224183</td>\n",
       "      <td>0.284483</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text sentiment  EMT  \\\n",
       "0    8  RT Love looks good Maddie !!! Ako lang ba yung...   neutral  0.0   \n",
       "1   14  @ Min _ Of _ Lyching @ manakgupta Mein kahna n...   neutral  0.0   \n",
       "2   26  Best luck sir World Cup ke liye bhot bhot subh...  positive  0.0   \n",
       "3   27  Yes . Great dialogues one . Also Chupke Chupke...  positive  0.0   \n",
       "4   33  @ TarekFatah Tu tere baap ke liye jo bola wo k...  negative  0.0   \n",
       "\n",
       "        Eng       Hin         O  num_words  num_uniq_words  len_char  \\\n",
       "0  0.695652  0.173913  0.130435   0.458333        1.000000  0.677419   \n",
       "1  0.272727  0.409091  0.318182   0.437500        0.909091  0.625806   \n",
       "2  0.300000  0.700000  0.000000   0.187500        0.900000  0.309677   \n",
       "3  0.392857  0.250000  0.357143   0.562500        0.750000  0.767742   \n",
       "4  0.076923  0.692308  0.230769   0.520833        1.000000  0.716129   \n",
       "\n",
       "   avg_word_len  len_non_chars  num_caps_words  num_all_caps  num_repeat_words  \n",
       "0      0.263780       0.245455        0.347826      0.217391          0.173913  \n",
       "1      0.246261       0.284314        0.545455      0.318182          0.136364  \n",
       "2      0.333472       0.169811        0.300000      0.000000          0.000000  \n",
       "3      0.219846       0.314516        0.785714      0.464286          0.142857  \n",
       "4      0.224183       0.284483        0.269231      0.230769          0.153846  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>EMT</th>\n",
       "      <th>Eng</th>\n",
       "      <th>Hin</th>\n",
       "      <th>O</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_uniq_words</th>\n",
       "      <th>len_char</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>len_non_chars</th>\n",
       "      <th>num_caps_words</th>\n",
       "      <th>num_all_caps</th>\n",
       "      <th>num_repeat_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Keh aese Rahe ho Jaise Pakistan wale Ni karte ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.696774</td>\n",
       "      <td>0.254541</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>RT Anu's Prerna way ran saving ... And Ram Ram...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.315257</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>Shukar hai ye pathan nae warna # PTM nay pr b ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.832258</td>\n",
       "      <td>0.206555</td>\n",
       "      <td>0.291045</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>Harsh pen decision . Didn't much option arm . ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>0.311297</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>RT I keep saying Kenyan rugby beautiful save K...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.313811</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                                               text  EMT       Eng  \\\n",
       "0    1  Keh aese Rahe ho Jaise Pakistan wale Ni karte ...  0.0  0.125000   \n",
       "1    4  RT Anu's Prerna way ran saving ... And Ram Ram...  0.0  0.500000   \n",
       "2   17  Shukar hai ye pathan nae warna # PTM nay pr b ...  0.0  0.225806   \n",
       "3   18  Harsh pen decision . Didn't much option arm . ...  0.0  0.608696   \n",
       "4   34  RT I keep saying Kenyan rugby beautiful save K...  0.0  0.714286   \n",
       "\n",
       "        Hin         O  num_words  num_uniq_words  len_char  avg_word_len  \\\n",
       "0  0.791667  0.083333   0.479167        0.958333  0.696774      0.254541   \n",
       "1  0.250000  0.250000   0.395833        0.900000  0.638710      0.315257   \n",
       "2  0.580645  0.193548   0.625000        0.935484  0.832258      0.206555   \n",
       "3  0.130435  0.260870   0.458333        0.869565  0.735484      0.311297   \n",
       "4  0.095238  0.190476   0.416667        0.904762  0.670968      0.313811   \n",
       "\n",
       "   len_non_chars  num_caps_words  num_all_caps  num_repeat_words  \n",
       "0       0.247788        0.500000      0.041667          0.000000  \n",
       "1       0.365385        0.550000      0.200000          0.200000  \n",
       "2       0.291045        0.225806      0.193548          0.064516  \n",
       "3       0.268908        0.521739      0.304348          0.130435  \n",
       "4       0.256881        0.380952      0.285714          0.142857  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.hstack([train_texts.values[:,3:],lda_train])\n",
    "valX = np.hstack([val_texts.values[:,3:],lda_val])\n",
    "testX = np.hstack([test_texts.values[:,2:],lda_test])\n",
    "\n",
    "trainy = train_texts.sentiment.values\n",
    "valy = val_texts.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_features',trainX)\n",
    "np.save('val_features',valX)\n",
    "np.save('test_features',testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
