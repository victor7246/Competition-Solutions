{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/abhishek/distilbert-use-features-oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../input/transformers/transformers-master/\")\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass new_model():\\n    def __init__(self):\\n        self.basemodel = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\\n        self.pooling = torch.nn.A\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "'''\n",
    "class new_model():\n",
    "    def __init__(self):\n",
    "        self.basemodel = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "        self.pooling = torch.nn.A\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2054, 2572, 1045, 3974, 2043, 2478, 5331, 10868, 2612, 1997, 1037, 26632, 10014, 1029, 102]]\n",
      "['what', 'am', 'i', 'losing', 'when', 'using', 'extension', 'tubes', 'instead', 'of', 'a', 'macro', 'lens', '?']\n",
      "14 16\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"What am I losing when using extension tubes instead of a macro lens?\"\n",
    "tokenized = []\n",
    "x = \" \".join(sample_string.strip().split()[:300])\n",
    "tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "tokenized_text = tokenizer.tokenize(x)\n",
    "tokenized.append(tok[:512])\n",
    "print (tokenized)\n",
    "print (tokenized_text)\n",
    "print (len(tokenized_text), len(tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "input_ids = torch.tensor(padded).to(DEVICE)\n",
    "attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1536)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([last_hidden_states[0][:,0,:].cpu().numpy(),last_hidden_states[0].cpu().numpy().mean(axis=1)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    for data in tqdm(chunks(string_list, batch_size)):\n",
    "        tokenized = []\n",
    "        all_lengths = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:300])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            all_lengths.append(len(tok))\n",
    "            tokenized.append(tok[:512])\n",
    "\n",
    "        max_len = 512\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features1 = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        features2 = last_hidden_states[0].cpu().numpy().mean(axis=1)\n",
    "        features3 = np.array([last_hidden_states[0].cpu().numpy()[i,:all_lengths[i],:].mean(axis=0) for i in range(len(all_lengths))])\n",
    "        features = np.hstack([features1,features2, features3])\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [09:04,  5.73s/it]\n",
      "95it [09:36,  6.06s/it]\n",
      "95it [09:33,  6.03s/it]\n",
      "8it [00:41,  5.20s/it]\n",
      "8it [00:42,  5.31s/it]\n",
      "8it [00:42,  5.30s/it]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../input/google-quest-challenge/train.csv\").fillna(\"none\")\n",
    "df_test = pd.read_csv(\"../input/google-quest-challenge/test.csv\").fillna(\"none\")\n",
    "\n",
    "sample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\n",
    "target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n",
    "\n",
    "train_question_title_dense = fetch_vectors(df_train.question_title.values)\n",
    "train_question_body_dense = fetch_vectors(df_train.question_body.values)\n",
    "train_answer_dense = fetch_vectors(df_train.answer.values)\n",
    "\n",
    "test_question_title_dense = fetch_vectors(df_test.question_title.values)\n",
    "test_question_body_dense = fetch_vectors(df_test.question_body.values)\n",
    "test_answer_dense = fetch_vectors(df_test.answer.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 2304) (6079, 2304) (6079, 2304) (476, 2304) (476, 2304) (476, 2304)\n"
     ]
    }
   ],
   "source": [
    "print (train_question_title_dense.shape, train_question_body_dense.shape, train_answer_dense.shape, test_question_title_dense.shape, test_question_body_dense.shape, test_answer_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from numpy.random import seed\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 41) (476, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>judaism.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>graphicdesign.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS   \n",
       "\n",
       "                              host  question_asker_intent_understanding  \\\n",
       "0          photo.stackexchange.com                             1.000000   \n",
       "1            rpg.stackexchange.com                             1.000000   \n",
       "2    electronics.stackexchange.com                             0.888889   \n",
       "3        judaism.stackexchange.com                             0.888889   \n",
       "4  graphicdesign.stackexchange.com                             1.000000   \n",
       "\n",
       "   question_body_critical  question_conversational  \\\n",
       "0                0.333333                 0.000000   \n",
       "1                1.000000                 0.000000   \n",
       "2                0.666667                 0.000000   \n",
       "3                0.666667                 0.666667   \n",
       "4                0.666667                 0.000000   \n",
       "\n",
       "   question_expect_short_answer  question_fact_seeking  \\\n",
       "0                           0.0                    0.0   \n",
       "1                           0.5                    1.0   \n",
       "2                           1.0                    1.0   \n",
       "3                           1.0                    1.0   \n",
       "4                           1.0                    1.0   \n",
       "\n",
       "   question_has_commonly_accepted_answer  question_interestingness_others  \\\n",
       "0                                    0.0                         1.000000   \n",
       "1                                    1.0                         0.444444   \n",
       "2                                    1.0                         0.666667   \n",
       "3                                    1.0                         0.444444   \n",
       "4                                    1.0                         0.666667   \n",
       "\n",
       "   question_interestingness_self  question_multi_intent  \\\n",
       "0                       1.000000               0.000000   \n",
       "1                       0.444444               0.666667   \n",
       "2                       0.444444               0.333333   \n",
       "3                       0.444444               0.000000   \n",
       "4                       0.666667               0.000000   \n",
       "\n",
       "   question_not_really_a_question  question_opinion_seeking  \\\n",
       "0                             0.0                  1.000000   \n",
       "1                             0.0                  0.000000   \n",
       "2                             0.0                  0.333333   \n",
       "3                             0.0                  0.000000   \n",
       "4                             0.0                  0.000000   \n",
       "\n",
       "   question_type_choice  question_type_compare  question_type_consequence  \\\n",
       "0              0.000000               0.000000                        0.0   \n",
       "1              0.666667               0.666667                        0.0   \n",
       "2              0.000000               0.000000                        0.0   \n",
       "3              1.000000               0.000000                        0.0   \n",
       "4              0.000000               0.000000                        0.0   \n",
       "\n",
       "   question_type_definition  question_type_entity  question_type_instructions  \\\n",
       "0                  0.000000                   0.0                         1.0   \n",
       "1                  0.333333                   0.0                         0.0   \n",
       "2                  0.000000                   0.0                         1.0   \n",
       "3                  0.000000                   0.0                         0.0   \n",
       "4                  0.000000                   0.0                         1.0   \n",
       "\n",
       "   question_type_procedure  question_type_reason_explanation  \\\n",
       "0                 0.000000                          0.000000   \n",
       "1                 0.000000                          0.333333   \n",
       "2                 0.333333                          0.333333   \n",
       "3                 0.000000                          0.000000   \n",
       "4                 0.000000                          1.000000   \n",
       "\n",
       "   question_type_spelling  question_well_written  answer_helpful  \\\n",
       "0                     0.0               1.000000        1.000000   \n",
       "1                     0.0               0.888889        0.888889   \n",
       "2                     0.0               0.777778        0.777778   \n",
       "3                     0.0               0.888889        0.833333   \n",
       "4                     0.0               1.000000        1.000000   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.666667          1.000000          1.000000   \n",
       "1                     0.555556          0.888889          0.888889   \n",
       "2                     0.555556          1.000000          1.000000   \n",
       "3                     0.333333          0.833333          1.000000   \n",
       "4                     0.666667          1.000000          1.000000   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.800000                       1.0               0.000000   \n",
       "1             0.666667                       0.0               0.000000   \n",
       "2             0.666667                       0.0               0.333333   \n",
       "3             0.800000                       0.0               0.000000   \n",
       "4             0.800000                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../input/google-quest-challenge/'\n",
    "train = pd.read_csv(path_join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(path_join(data_dir, 'test.csv'))\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'    \n",
    "    ]\n",
    "\n",
    "input_columns = ['question_title', 'question_body', 'answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = re.compile(r\"^[^.]*\")\n",
    "\n",
    "train['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "\n",
    "features = ['netloc', 'category']\n",
    "merged = pd.concat([train[features], test[features]])\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(merged)\n",
    "\n",
    "features_train = ohe.transform(train[features]).toarray()\n",
    "features_test = ohe.transform(test[features]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"../input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title\n",
      "question_body\n",
      "answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302250"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in input_columns:\n",
    "    print(text)\n",
    "    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "\n",
    "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
    "\n",
    "dist_features_train = np.array([\n",
    "    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
    "]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate textual features and topic modelling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.stats import skew, kurtosis, spearmanr\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555, 12)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train.drop(targets,axis=1),test],axis=0)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len_q1'] = data.question_title.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question_body.apply(lambda x: len(str(x)))\n",
    "data['len_a'] = data.answer.apply(lambda x: len(str(x)))\n",
    "data['diff_len_q'] = data.len_q2 - data.len_q1\n",
    "data['diff_len_q_frac'] = data['diff_len_q']/data.len_q2\n",
    "\n",
    "data['diff_len_a1'] = data.len_a - data.len_q1\n",
    "data['diff_len_a2'] = data.len_a - data.len_q2\n",
    "data['diff_len_frac_a2'] = data['diff_len_a2']/data['len_a']\n",
    "\n",
    "data['len_word_q1'] = data.question_title.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question_body.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_frac_q2'] = data['len_word_q1']/data['len_word_q2']\n",
    "data['len_word_a'] = data.answer.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_frac_a'] = data['len_word_q2']/data['len_word_a']\n",
    "\n",
    "data['common_words_q'] = data.apply(lambda x: len(set(str(x['question_title']).lower().split()).intersection(set(str(x['question_body']).lower().split()))), axis=1)\n",
    "data['common_words_frac_q'] = data['common_words_q']/data.len_word_q1\n",
    "data['common_words_frac2_q'] = data['common_words_q']/data.len_word_q2\n",
    "data['common_words_a1'] = data.apply(lambda x: len(set(str(x['question_title']).lower().split()).intersection(set(str(x['answer']).lower().split()))), axis=1)\n",
    "data['common_words_a2'] = data.apply(lambda x: len(set(str(x['answer']).lower().split()).intersection(set(str(x['question_body']).lower().split()))), axis=1)\n",
    "data['common_words_frac_a2'] = data['common_words_a2']/data['len_word_a']\n",
    "data['common_words_frac2_a2'] = data['common_words_a2']/data['len_word_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def clean(data):\n",
    "    tokens = tokenizer.tokenize(data.lower())\n",
    "    stop_free = \" \".join([st.stem(i) for i in tokens if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "    \n",
    "def processSingleReview(review, d=None):\n",
    "    \"\"\"\n",
    "    Convert a raw review to a string of words\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = tokenizer.tokenize(letters_only.lower())\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [st.stem(w) for w in words if w not in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if pos_tag([w],tagset='universal')[0][1] in ['NOUN','VERB','ADJ']] #\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "data['clean_question_title'] = data.apply(lambda row :clean(row['question_title']),axis=1)\n",
    "data['clean_question_title'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_question_title']),axis=1)\n",
    "\n",
    "data['clean_question_body'] = data.apply(lambda row :clean(row['question_body']),axis=1)\n",
    "data['clean_question_body'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_question_body']),axis=1)\n",
    "\n",
    "data['clean_answer'] = data.apply(lambda row :clean(row['answer']),axis=1)\n",
    "data['clean_answer'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_answer']),axis=1)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def clean(data):\n",
    "    tokens = tokenizer.tokenize(data.lower())\n",
    "    stop_free = \" \".join([st.stem(i) for i in tokens if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "    \n",
    "def processSingleReview(review, d=None):\n",
    "    \"\"\"\n",
    "    Convert a raw review to a string of words\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = tokenizer.tokenize(letters_only.lower())\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [st.stem(w) for w in words if w not in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if pos_tag([w],tagset='universal')[0][1] in ['NOUN','VERB','ADJ']] #\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "data['clean_question_title'] = data.apply(lambda row :clean(row['question_title']),axis=1)\n",
    "data['clean_question_title'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_question_title']),axis=1)\n",
    "\n",
    "data['clean_question_body'] = data.apply(lambda row :clean(row['question_body']),axis=1)\n",
    "data['clean_question_body'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_question_body']),axis=1)\n",
    "\n",
    "data['clean_answer'] = data.apply(lambda row :clean(row['answer']),axis=1)\n",
    "data['clean_answer'] = data.apply(lambda row: re.sub(r'\\d+', '',row['clean_answer']),axis=1)\n",
    "\n",
    "data['question_title_wordlen'] = data.clean_question_title.apply(lambda x: len(x.split()))\n",
    "data['question_body_wordlen'] = data.clean_question_body.apply(lambda x: len(x.split()))\n",
    "data['answer_wordlen'] = data.clean_answer.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "6555it [00:02, 2594.30it/s]\n",
      "6555it [00:22, 286.70it/s]\n",
      "6555it [00:24, 264.82it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "input_word2vec = data.clean_question_title.tolist() + data.clean_question_body.tolist() + data.clean_answer.tolist()\n",
    "input_word2vec = [i.split() for i in input_word2vec]\n",
    "\n",
    "print (len(input_word2vec))\n",
    "\n",
    "model = Word2Vec(min_count=5)\n",
    "model.build_vocab(input_word2vec)\n",
    "model.train(input_word2vec,total_examples = model.corpus_count,epochs=15)\n",
    "\n",
    "question1_vectors = np.zeros((data.shape[0], 100))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data.question_title.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data.shape[0], 100))\n",
    "for i, q in tqdm(enumerate(data.question_body.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)\n",
    "    \n",
    "answer_vectors  = np.zeros((data.shape[0], 100))\n",
    "for i, q in tqdm(enumerate(data.answer.values)):\n",
    "    answer_vectors[i, :] = sent2vec(q)\n",
    "    \n",
    "data['cosine_distance_q'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['cosine_distance_a1'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(answer_vectors),\n",
    "                                                          np.nan_to_num(question1_vectors))]\n",
    "\n",
    "data['cosine_distance_a2'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(answer_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "\n",
    "data['euclidean_distance_q'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['euclidean_distance_a1'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(answer_vectors),\n",
    "                                                          np.nan_to_num(question1_vectors))]\n",
    "\n",
    "data['euclidean_distance_a2'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(answer_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_counts(x):\n",
    "    count = 0\n",
    "    for word in x.split():\n",
    "        if word.lower() not in model.wv.vocab and word not in model.wv.vocab:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "data[\"oov_count_title\"] = data.clean_question_title.apply(oov_counts)\n",
    "data[\"oov_count_body\"] = data.clean_question_body.apply(oov_counts)\n",
    "data[\"oov_count_answer\"] = data.clean_answer.apply(oov_counts)\n",
    "\n",
    "data[\"oov_count_title_frac\"] = data[\"oov_count_title\"]/data.question_title_wordlen\n",
    "data[\"oov_count_body_frac\"] = data[\"oov_count_body\"]/data.question_body_wordlen\n",
    "data[\"oov_count_answer_frac\"] = data[\"oov_count_answer\"]/data.answer_wordlen\n",
    "\n",
    "def count_question_words(x):\n",
    "    count = 0\n",
    "    count += x.count(\"?\")\n",
    "    for word in x.lower().split():\n",
    "        if word.startswith(\"wh\") or word.startswith(\"how\"):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "data[\"q_count_title\"] = data.question_title.apply(count_question_words)\n",
    "data[\"q_count_body\"] = data.question_body.apply(count_question_words)\n",
    "\n",
    "data[\"q_count_title_frac\"] = data[\"q_count_title\"]/data.question_title_wordlen\n",
    "data[\"q_count_body_frac\"] = data[\"q_count_body\"]/data.question_body_wordlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import scipy\n",
    "\n",
    "lda1 = LatentDirichletAllocation(n_components=20)\n",
    "lda2 = LatentDirichletAllocation(n_components=20)\n",
    "\n",
    "cv1 = CountVectorizer(max_df=.7,min_df=5,max_features=50000)\n",
    "\n",
    "answer_vector = cv1.fit_transform(data.clean_answer)\n",
    "title_vector = cv1.transform(data.clean_question_title)\n",
    "body_vector = cv1.transform(data.clean_question_body)\n",
    "\n",
    "cv2 = CountVectorizer(max_df=.7,min_df=5,max_features=50000)\n",
    "\n",
    "body_vector2 = cv2.fit_transform(data.clean_question_body)\n",
    "answer_vector2 = cv2.transform(data.clean_answer)\n",
    "title_vector2 = cv2.transform(data.clean_question_title)\n",
    "\n",
    "answer_topics = lda1.fit_transform(answer_vector)\n",
    "title_topics = lda1.transform(title_vector)\n",
    "body_topics = lda1.transform(body_vector)\n",
    "\n",
    "body_topics2 = lda2.fit_transform(body_vector2)\n",
    "answer_topics2 = lda2.transform(answer_vector2)\n",
    "title_topics2 = lda2.transform(title_vector2)\n",
    "\n",
    "\n",
    "title_topic_entropy = scipy.stats.entropy(title_topics.T)\n",
    "body_topic_entropy = scipy.stats.entropy(body_topics.T)\n",
    "answer_topic_entropy = scipy.stats.entropy(answer_topics.T)\n",
    "\n",
    "title_topic_entropy2 = scipy.stats.entropy(title_topics2.T)\n",
    "body_topic_entropy2 = scipy.stats.entropy(body_topics2.T)\n",
    "answer_topic_entropy2 = scipy.stats.entropy(answer_topics2.T)\n",
    "\n",
    "#document_topic_entropy_len_normalized = document_topic_entropy * np.sqrt(word_len/2)\n",
    "def geometric_mean(x):\n",
    "    x = [i for i in x if i!=0]\n",
    "    if len(x) > 0:\n",
    "        return scipy.stats.mstats.gmean(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def popularity(matrix):\n",
    "    matrix = matrix.toarray()\n",
    "    idf_matrix = (matrix > 0).astype(int)\n",
    "    word_freq = idf_matrix.sum(axis=0)\n",
    "    idf_matrix = idf_matrix * word_freq\n",
    "    idf_matrix = idf_matrix * 1.0/idf_matrix.shape[0]\n",
    "\n",
    "    document_popularity = np.array([geometric_mean(x) for x in idf_matrix.tolist()])\n",
    "    return document_popularity\n",
    "\n",
    "title_popularity = popularity(title_vector)\n",
    "body_popularity = popularity(body_vector)\n",
    "answer_popularity = popularity(answer_vector)\n",
    "\n",
    "title_popularity2 = popularity(title_vector2)\n",
    "body_popularity2 = popularity(body_vector2)\n",
    "answer_popularity2 = popularity(answer_vector2)\n",
    "\n",
    "data['title_entropy'] = title_topic_entropy\n",
    "data['body_entropy'] = body_topic_entropy\n",
    "data['answer_entropy'] = answer_topic_entropy\n",
    "\n",
    "data['title_entropy2'] = title_topic_entropy2\n",
    "data['body_entropy2'] = body_topic_entropy2\n",
    "data['answer_entropy2'] = answer_topic_entropy2\n",
    "\n",
    "data['title_popularity'] = title_popularity\n",
    "data['body_popularity'] = body_popularity\n",
    "data['answer_popularity'] = answer_popularity\n",
    "\n",
    "data['title_popularity2'] = title_popularity2\n",
    "data['body_popularity2'] = body_popularity2\n",
    "data['answer_popularity2'] = answer_popularity2\n",
    "\n",
    "#from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "def manhattan_distance(x,y):\n",
    "    return np.abs(x-y).sum(axis=1)\n",
    "\n",
    "def jaccard_distance(x,y):\n",
    "    return np.abs(x-y).sum(axis=1)*1.0/np.max(np.array([x.sum(1),y.sum(1)]),axis=0)\n",
    "\n",
    "data['manh_q'] = manhattan_distance(title_vector.toarray(),body_vector.toarray()) #scipy.spatial.distance.cdist(title_vector2.toarray(),body_vector2.toarray(),cityblock)\n",
    "data['manh_a1'] = manhattan_distance(title_vector.toarray(),answer_vector.toarray())\n",
    "data['manh_a2'] = manhattan_distance(body_vector.toarray(),answer_vector.toarray())\n",
    "\n",
    "data['jac_q'] = jaccard_distance(title_vector.toarray(),body_vector.toarray()) #scipy.spatial.distance.cdist(title_vector2.toarray(),body_vector2.toarray(),cityblock)\n",
    "data['jac_a1'] = jaccard_distance(title_vector.toarray(),answer_vector.toarray())\n",
    "data['jac_a2'] = jaccard_distance(body_vector.toarray(),answer_vector.toarray())\n",
    "\n",
    "data['manh_q_2'] = manhattan_distance(title_vector2.toarray(),body_vector2.toarray()) #scipy.spatial.distance.cdist(title_vector2.toarray(),body_vector2.toarray(),cityblock)\n",
    "data['manh_a1_2'] = manhattan_distance(title_vector2.toarray(),answer_vector2.toarray())\n",
    "data['manh_a2_2'] = manhattan_distance(body_vector2.toarray(),answer_vector2.toarray())\n",
    "\n",
    "data['jac_q_2'] = jaccard_distance(title_vector2.toarray(),body_vector2.toarray()) #scipy.spatial.distance.cdist(title_vector2.toarray(),body_vector2.toarray(),cityblock)\n",
    "data['jac_a1_2'] = jaccard_distance(title_vector2.toarray(),answer_vector2.toarray())\n",
    "data['jac_a2_2'] = jaccard_distance(body_vector2.toarray(),answer_vector2.toarray())\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf1 = NMF(n_components=20)\n",
    "nmf2 = NMF(n_components=20)\n",
    "\n",
    "nmf_ans = nmf1.fit_transform(answer_vector)\n",
    "nmf_title = nmf1.transform(title_vector)\n",
    "nmf_body = nmf1.transform(body_vector)\n",
    "\n",
    "nmf_body2 = nmf2.fit_transform(body_vector2)\n",
    "nmf_title2 = nmf2.transform(title_vector2)\n",
    "nmf_ans2 = nmf2.transform(answer_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_word_frac_q2\n",
      "common_words_frac2_q\n",
      "common_words_frac2_a2\n",
      "cosine_distance_q\n",
      "cosine_distance_a1\n",
      "cosine_distance_a2\n",
      "oov_count_body_frac\n",
      "q_count_body_frac\n"
     ]
    }
   ],
   "source": [
    "for col in data:\n",
    "    if data[col].isna().any():\n",
    "        print (col)\n",
    "        if 'popularity' in col or 'frac' in col:\n",
    "            data[col] = data[col].fillna(0)\n",
    "        elif 'distance' in col:\n",
    "            data[col] = data[col].fillna(1)\n",
    "        else:\n",
    "            data[col] = data[col].fillna(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    if data[col].isna().any():\n",
    "        print (col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['len_q1', 'len_q2', 'len_a', 'diff_len_q', 'diff_len_q_frac', 'diff_len_a1', 'diff_len_a2', 'diff_len_frac_a2', 'len_word_q1', 'len_word_q2', 'len_word_frac_q2', 'len_word_a', 'len_word_frac_a', 'common_words_q', 'common_words_frac_q', 'common_words_frac2_q', 'common_words_a1', 'common_words_a2', 'common_words_frac_a2', 'common_words_frac2_a2', 'question_title_wordlen', 'question_body_wordlen', 'answer_wordlen', 'cosine_distance_q', 'cosine_distance_a1', 'cosine_distance_a2', 'euclidean_distance_q', 'euclidean_distance_a1', 'euclidean_distance_a2', 'oov_count_title', 'oov_count_body', 'oov_count_answer', 'oov_count_title_frac', 'oov_count_body_frac', 'oov_count_answer_frac', 'q_count_title', 'q_count_body', 'q_count_title_frac', 'q_count_body_frac', 'title_entropy', 'body_entropy', 'answer_entropy', 'title_entropy2', 'body_entropy2', 'answer_entropy2', 'title_popularity', 'body_popularity', 'answer_popularity', 'title_popularity2', 'body_popularity2', 'answer_popularity2', 'manh_q', 'manh_a1', 'manh_a2', 'jac_q', 'jac_a1', 'jac_a2', 'manh_q_2', 'manh_a1_2', 'manh_a2_2', 'jac_q_2', 'jac_a1_2', 'jac_a2_2']\n"
     ]
    }
   ],
   "source": [
    "new_features = list(data.columns)[12:]\n",
    "new_features.remove('clean_question_title')\n",
    "new_features.remove('clean_question_body')\n",
    "new_features.remove('clean_answer')\n",
    "print (new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 8518) (476, 8518)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train, train_question_title_dense, train_question_body_dense, train_answer_dense])\n",
    "X_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test, test_question_title_dense, test_question_body_dense, test_answer_dense])\n",
    "y_train = train[targets].values\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555, 603)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "question1_vectors[np.isnan(question1_vectors)] = 0\n",
    "question2_vectors[np.isnan(question2_vectors)] = 0\n",
    "answer_vectors[np.isnan(answer_vectors)] = 0\n",
    "\n",
    "new_X = np.hstack([mm.fit_transform(data[new_features]), question1_vectors, question2_vectors, answer_vectors, nmf_ans, nmf_ans2, nmf_body, nmf_body2, nmf_title, nmf_body2, body_topics, body_topics2, title_topics, title_topics2, answer_topics, answer_topics2])\n",
    "#new_X_without_data_features = np.hstack([mm.fit_transform(data[new_features]), question1_vectors, question2_vectors, answer_vectors, nmf_ans, nmf_ans2, nmf_body, nmf_body2, nmf_title, nmf_body2, body_topics, body_topics2, title_topics, title_topics2, answer_topics, answer_topics2])\n",
    "\n",
    "print (new_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 9121) (476, 9121)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack((X_train, new_X[:X_train.shape[0],:]))\n",
    "X_test = np.hstack((X_test, new_X[X_train.shape[0]:,:]))\n",
    "\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorized = train[targets].copy()\n",
    "categorization_dict = {}\n",
    "for col in targets:\n",
    "    keys = np.sort(train[col].unique())\n",
    "    values = np.arange(len(keys))\n",
    "    categorization_dict[col] = dict(zip(keys,values))\n",
    "    y_categorized[col] = train[col].apply(lambda x: categorization_dict[col][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatible with tensorflow backend\n",
    "class SpearmanRhoCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, model_name):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        '''\n",
    "        if len(self.y_val) == 2:\n",
    "            rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        else:\n",
    "            rho_val = np.mean([spearmanr(self.y_val, y_pred_val + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation])\n",
    "        '''    \n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "            self.model.save_weights(self.model_name)\n",
    "            print (\"model saved {}\".format(self.model_name))\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "        #if self.bad_epochs >= self.patience:\n",
    "        #    print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "        #    self.model.stop_training = True\n",
    "        print('\\rval_spearman-rho: %s' % (str(round(self.value, 4))), end=100*' '+'\\n')\n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inps = Input(shape=(X_train.shape[1],))\n",
    "    x = Dropout(0.2)(inps)\n",
    "    x = Dense(512, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    #x = Dense(64, activation='elu')(x)\n",
    "    x = Dense(y_train.shape[1], activation='sigmoid')(x) #Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "    model = Model(inputs=inps, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=.0001),\n",
    "        loss=['binary_crossentropy']\n",
    "    )\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2():\n",
    "    inps = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(512, activation='elu')(inps)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='elu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) #Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "    model = Model(inputs=inps, outputs=x)\n",
    "    model.compile(metrics=['mse'],\n",
    "        optimizer=Adam(lr=.0001),\n",
    "        loss=['binary_crossentropy']\n",
    "    )\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 9121)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9121)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4670464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                3870      \n",
      "=================================================================\n",
      "Total params: 4,842,142\n",
      "Trainable params: 4,840,350\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 3s 550us/step - loss: 0.7806 - val_loss: 0.6665\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.2698                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.7105 - val_loss: 0.6442\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.2898                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 2s 347us/step - loss: 0.6812 - val_loss: 0.6147\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3051                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 2s 325us/step - loss: 0.6518 - val_loss: 0.5982\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3196                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.6219 - val_loss: 0.5626\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3253                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 2s 333us/step - loss: 0.5891 - val_loss: 0.5439\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3318                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 2s 346us/step - loss: 0.5522 - val_loss: 0.5046\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3381                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 2s 333us/step - loss: 0.5187 - val_loss: 0.4686\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3481                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.4870 - val_loss: 0.4505\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3491                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 2s 327us/step - loss: 0.4608 - val_loss: 0.4251\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3517                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 2s 330us/step - loss: 0.4385 - val_loss: 0.4084\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3568                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 2s 309us/step - loss: 0.4223 - val_loss: 0.3964\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3615                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 2s 357us/step - loss: 0.4098 - val_loss: 0.3888\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3646                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 2s 338us/step - loss: 0.4003 - val_loss: 0.3841\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3693                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 2s 324us/step - loss: 0.3928 - val_loss: 0.3812\n",
      "val_spearman-rho: 0.3693                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 2s 314us/step - loss: 0.3878 - val_loss: 0.3788\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3699                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 2s 367us/step - loss: 0.3833 - val_loss: 0.3765\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3716                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3789 - val_loss: 0.3770\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3719                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 2s 350us/step - loss: 0.3758 - val_loss: 0.3754\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3726                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 2s 327us/step - loss: 0.3728 - val_loss: 0.3743\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3754                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3702 - val_loss: 0.3740\n",
      "val_spearman-rho: 0.3754                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 2s 311us/step - loss: 0.3679 - val_loss: 0.3742\n",
      "val_spearman-rho: 0.3754                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.3657 - val_loss: 0.3742\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3761                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 2s 416us/step - loss: 0.3639 - val_loss: 0.3736\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 2s 325us/step - loss: 0.3621 - val_loss: 0.3742\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 2s 323us/step - loss: 0.3597 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 2s 401us/step - loss: 0.3593 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.3565 - val_loss: 0.3740\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 2s 334us/step - loss: 0.3549 - val_loss: 0.3748\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3804                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 2s 374us/step - loss: 0.3536 - val_loss: 0.3734\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 31/100\n",
      "4863/4863 [==============================] - 2s 330us/step - loss: 0.3523 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 32/100\n",
      "4863/4863 [==============================] - 2s 326us/step - loss: 0.3516 - val_loss: 0.3763\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 33/100\n",
      "4863/4863 [==============================] - 2s 346us/step - loss: 0.3493 - val_loss: 0.3727\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 34/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.3488 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 35/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.3473 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 36/100\n",
      "4863/4863 [==============================] - 2s 337us/step - loss: 0.3463 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 37/100\n",
      "4863/4863 [==============================] - 2s 314us/step - loss: 0.3449 - val_loss: 0.3737\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 38/100\n",
      "4863/4863 [==============================] - 2s 324us/step - loss: 0.3449 - val_loss: 0.3750\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 39/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.3433 - val_loss: 0.3736\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3824                                                                                                    \n",
      "Epoch 40/100\n",
      "4863/4863 [==============================] - 2s 339us/step - loss: 0.3429 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3824                                                                                                    \n",
      "Epoch 41/100\n",
      "4863/4863 [==============================] - 2s 330us/step - loss: 0.3422 - val_loss: 0.3747\n",
      "val_spearman-rho: 0.3824                                                                                                    \n",
      "Epoch 42/100\n",
      "4863/4863 [==============================] - 2s 329us/step - loss: 0.3410 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.3824                                                                                                    \n",
      "Epoch 43/100\n",
      "4863/4863 [==============================] - 2s 309us/step - loss: 0.3404 - val_loss: 0.3743\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "model saved weights_0.hdf5\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 44/100\n",
      "4863/4863 [==============================] - 2s 333us/step - loss: 0.3392 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 45/100\n",
      "4863/4863 [==============================] - 2s 311us/step - loss: 0.3395 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 46/100\n",
      "4863/4863 [==============================] - 2s 322us/step - loss: 0.3388 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 47/100\n",
      "4863/4863 [==============================] - 2s 379us/step - loss: 0.3380 - val_loss: 0.3748\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 48/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3377 - val_loss: 0.3762\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 49/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3373 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 50/100\n",
      "4863/4863 [==============================] - 2s 330us/step - loss: 0.3368 - val_loss: 0.3757\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 51/100\n",
      "4863/4863 [==============================] - 2s 334us/step - loss: 0.3366 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 52/100\n",
      "4863/4863 [==============================] - 2s 324us/step - loss: 0.3361 - val_loss: 0.3757\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 53/100\n",
      "4863/4863 [==============================] - 2s 338us/step - loss: 0.3355 - val_loss: 0.3757\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.6806997336971108e-05.\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 00053: early stopping\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 3s 548us/step - loss: 0.7748 - val_loss: 0.6564\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.2657                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.7087 - val_loss: 0.6423\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.2793                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 2s 372us/step - loss: 0.6803 - val_loss: 0.6129\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3048                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.6508 - val_loss: 0.5901\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3146                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 2s 340us/step - loss: 0.6201 - val_loss: 0.5660\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3257                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 2s 353us/step - loss: 0.5871 - val_loss: 0.5357\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3375                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 305us/step - loss: 0.5505 - val_loss: 0.5078\n",
      "val_spearman-rho: 0.3375                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 2s 360us/step - loss: 0.5147 - val_loss: 0.4674\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3483                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 2s 339us/step - loss: 0.4818 - val_loss: 0.4396\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3565                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.4576 - val_loss: 0.4179\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3585                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 306us/step - loss: 0.4348 - val_loss: 0.4069\n",
      "val_spearman-rho: 0.3585                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.4197 - val_loss: 0.3957\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3589                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.4070 - val_loss: 0.3890\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3594                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 2s 330us/step - loss: 0.3977 - val_loss: 0.3844\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3659                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 2s 327us/step - loss: 0.3921 - val_loss: 0.3814\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3679                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 2s 311us/step - loss: 0.3864 - val_loss: 0.3791\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3688                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3826 - val_loss: 0.3779\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3752                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.3783 - val_loss: 0.3789\n",
      "val_spearman-rho: 0.3752                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 2s 386us/step - loss: 0.3758 - val_loss: 0.3758\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3758                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 2s 350us/step - loss: 0.3719 - val_loss: 0.3753\n",
      "val_spearman-rho: 0.3758                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 2s 324us/step - loss: 0.3698 - val_loss: 0.3758\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3762                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 2s 326us/step - loss: 0.3680 - val_loss: 0.3754\n",
      "val_spearman-rho: 0.3762                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 2s 323us/step - loss: 0.3648 - val_loss: 0.3751\n",
      "val_spearman-rho: 0.3762                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 307us/step - loss: 0.3632 - val_loss: 0.3737\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3801                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 2s 406us/step - loss: 0.3616 - val_loss: 0.3750\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3801                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 2s 314us/step - loss: 0.3600 - val_loss: 0.3732\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3822                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 2s 314us/step - loss: 0.3581 - val_loss: 0.3758\n",
      "val_spearman-rho: 0.3822                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.3564 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.3822                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 2s 359us/step - loss: 0.3545 - val_loss: 0.3745\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 2s 309us/step - loss: 0.3536 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 31/100\n",
      "4863/4863 [==============================] - 2s 339us/step - loss: 0.3528 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 32/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3508 - val_loss: 0.3741\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 33/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3487 - val_loss: 0.3743\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3832                                                                                                    \n",
      "Epoch 34/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3474 - val_loss: 0.3740\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3848                                                                                                    \n",
      "Epoch 35/100\n",
      "4863/4863 [==============================] - 2s 323us/step - loss: 0.3479 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3848                                                                                                    \n",
      "Epoch 36/100\n",
      "4863/4863 [==============================] - 2s 405us/step - loss: 0.3469 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3848                                                                                                    \n",
      "Epoch 37/100\n",
      "4863/4863 [==============================] - 2s 345us/step - loss: 0.3442 - val_loss: 0.3746\n",
      "val_spearman-rho: 0.3848                                                                                                    \n",
      "Epoch 38/100\n",
      "4863/4863 [==============================] - 2s 325us/step - loss: 0.3444 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3848                                                                                                    \n",
      "Epoch 39/100\n",
      "4863/4863 [==============================] - 2s 394us/step - loss: 0.3434 - val_loss: 0.3742\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3863                                                                                                    \n",
      "Epoch 40/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.3421 - val_loss: 0.3753\n",
      "val_spearman-rho: 0.3863                                                                                                    \n",
      "Epoch 41/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.3423 - val_loss: 0.3757\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3863                                                                                                    \n",
      "Epoch 42/100\n",
      "4863/4863 [==============================] - 2s 393us/step - loss: 0.3406 - val_loss: 0.3753\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.3867                                                                                                    \n",
      "Epoch 43/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.3408 - val_loss: 0.3761\n",
      "model saved weights_1.hdf5\n",
      "val_spearman-rho: 0.387                                                                                                    \n",
      "Epoch 44/100\n",
      "4863/4863 [==============================] - 2s 318us/step - loss: 0.3403 - val_loss: 0.3755\n",
      "val_spearman-rho: 0.387                                                                                                    \n",
      "Epoch 45/100\n",
      "4863/4863 [==============================] - 1s 308us/step - loss: 0.3395 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.387                                                                                                    \n",
      "Epoch 46/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.3388 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "val_spearman-rho: 0.387                                                                                                    \n",
      "Epoch 00046: early stopping\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 3s 520us/step - loss: 0.7826 - val_loss: 0.6470\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.2572                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 303us/step - loss: 0.7118 - val_loss: 0.6458\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.2792                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 305us/step - loss: 0.6807 - val_loss: 0.6185\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.2982                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 2s 314us/step - loss: 0.6537 - val_loss: 0.5967\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3069                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 2s 350us/step - loss: 0.6254 - val_loss: 0.5666\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3178                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 2s 333us/step - loss: 0.5913 - val_loss: 0.5412\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3248                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.5556 - val_loss: 0.5013\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.329                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 2s 313us/step - loss: 0.5212 - val_loss: 0.4757\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3372                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 2s 313us/step - loss: 0.4881 - val_loss: 0.4389\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3385                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 298us/step - loss: 0.4603 - val_loss: 0.4234\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3418                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 2s 356us/step - loss: 0.4395 - val_loss: 0.4068\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3438                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.4220 - val_loss: 0.3967\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3552                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 304us/step - loss: 0.4105 - val_loss: 0.3921\n",
      "val_spearman-rho: 0.3552                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 301us/step - loss: 0.4003 - val_loss: 0.3852\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3576                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 2s 353us/step - loss: 0.3927 - val_loss: 0.3818\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3624                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 307us/step - loss: 0.3867 - val_loss: 0.3788\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3629                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 2s 321us/step - loss: 0.3821 - val_loss: 0.3775\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3666                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 2s 321us/step - loss: 0.3790 - val_loss: 0.3764\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3705                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 303us/step - loss: 0.3764 - val_loss: 0.3765\n",
      "val_spearman-rho: 0.3705                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 300us/step - loss: 0.3728 - val_loss: 0.3753\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3718                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3694 - val_loss: 0.3759\n",
      "val_spearman-rho: 0.3718                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 2s 401us/step - loss: 0.3677 - val_loss: 0.3766\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3733                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 2s 331us/step - loss: 0.3653 - val_loss: 0.3742\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3744                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 2s 348us/step - loss: 0.3641 - val_loss: 0.3759\n",
      "val_spearman-rho: 0.3744                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 2s 405us/step - loss: 0.3620 - val_loss: 0.3738\n",
      "val_spearman-rho: 0.3744                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 2s 309us/step - loss: 0.3601 - val_loss: 0.3775\n",
      "val_spearman-rho: 0.3744                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3584 - val_loss: 0.3748\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3763                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 2s 394us/step - loss: 0.3564 - val_loss: 0.3741\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3778                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 2s 318us/step - loss: 0.3553 - val_loss: 0.3744\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3789                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3543 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3794                                                                                                    \n",
      "Epoch 31/100\n",
      "4863/4863 [==============================] - 2s 309us/step - loss: 0.3517 - val_loss: 0.3741\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3809                                                                                                    \n",
      "Epoch 32/100\n",
      "4863/4863 [==============================] - 1s 303us/step - loss: 0.3508 - val_loss: 0.3746\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 33/100\n",
      "4863/4863 [==============================] - 1s 303us/step - loss: 0.3492 - val_loss: 0.3741\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3835                                                                                                    \n",
      "Epoch 34/100\n",
      "4863/4863 [==============================] - 2s 350us/step - loss: 0.3487 - val_loss: 0.3746\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3836                                                                                                    \n",
      "Epoch 35/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.3476 - val_loss: 0.3742\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3836                                                                                                    \n",
      "Epoch 36/100\n",
      "4863/4863 [==============================] - 1s 304us/step - loss: 0.3470 - val_loss: 0.3735\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3839                                                                                                    \n",
      "Epoch 37/100\n",
      "4863/4863 [==============================] - 2s 313us/step - loss: 0.3442 - val_loss: 0.3740\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.384                                                                                                    \n",
      "Epoch 38/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.3441 - val_loss: 0.3746\n",
      "val_spearman-rho: 0.384                                                                                                    \n",
      "Epoch 39/100\n",
      "4863/4863 [==============================] - 2s 333us/step - loss: 0.3430 - val_loss: 0.3750\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3841                                                                                                    \n",
      "Epoch 40/100\n",
      "4863/4863 [==============================] - 2s 341us/step - loss: 0.3422 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3841                                                                                                    \n",
      "Epoch 41/100\n",
      "4863/4863 [==============================] - 2s 311us/step - loss: 0.3420 - val_loss: 0.3751\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3841                                                                                                    \n",
      "Epoch 42/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.3410 - val_loss: 0.3748\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3841                                                                                                    \n",
      "Epoch 43/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3411 - val_loss: 0.3750\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.385                                                                                                    \n",
      "Epoch 44/100\n",
      "4863/4863 [==============================] - 1s 304us/step - loss: 0.3407 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.385                                                                                                    \n",
      "Epoch 45/100\n",
      "4863/4863 [==============================] - 2s 364us/step - loss: 0.3393 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.385                                                                                                    \n",
      "Epoch 46/100\n",
      "4863/4863 [==============================] - 2s 318us/step - loss: 0.3399 - val_loss: 0.3750\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "val_spearman-rho: 0.385                                                                                                    \n",
      "Epoch 47/100\n",
      "4863/4863 [==============================] - 1s 302us/step - loss: 0.3380 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.385                                                                                                    \n",
      "Epoch 48/100\n",
      "4863/4863 [==============================] - 1s 296us/step - loss: 0.3378 - val_loss: 0.3755\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 49/100\n",
      "4863/4863 [==============================] - 2s 351us/step - loss: 0.3377 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 50/100\n",
      "4863/4863 [==============================] - 1s 307us/step - loss: 0.3379 - val_loss: 0.3754\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 51/100\n",
      "4863/4863 [==============================] - 2s 332us/step - loss: 0.3372 - val_loss: 0.3756\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.6806997336971108e-05.\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 52/100\n",
      "4863/4863 [==============================] - 2s 323us/step - loss: 0.3370 - val_loss: 0.3753\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 53/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3359 - val_loss: 0.3751\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 54/100\n",
      "4863/4863 [==============================] - 1s 299us/step - loss: 0.3360 - val_loss: 0.3754\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 55/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.3369 - val_loss: 0.3758\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 56/100\n",
      "4863/4863 [==============================] - 2s 401us/step - loss: 0.3363 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.1764897499233484e-05.\n",
      "model saved weights_2.hdf5\n",
      "val_spearman-rho: 0.3862                                                                                                    \n",
      "Epoch 00056: early stopping\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 3s 582us/step - loss: 0.7870 - val_loss: 0.6489\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.2772                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.7150 - val_loss: 0.6391\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.2933                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.6839 - val_loss: 0.6086\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3083                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 2s 367us/step - loss: 0.6555 - val_loss: 0.5954\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3161                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 2s 328us/step - loss: 0.6239 - val_loss: 0.5655\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3294                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.5929 - val_loss: 0.5305\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3367                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.5559 - val_loss: 0.5108\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3372                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.5227 - val_loss: 0.4687\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3427                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 306us/step - loss: 0.4887 - val_loss: 0.4429\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3534                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 2s 341us/step - loss: 0.4625 - val_loss: 0.4257\n",
      "val_spearman-rho: 0.3534                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.4391 - val_loss: 0.4054\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3585                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.4239 - val_loss: 0.3949\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.362                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 2s 316us/step - loss: 0.4108 - val_loss: 0.3885\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3676                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 2s 351us/step - loss: 0.4006 - val_loss: 0.3841\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3691                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 2s 339us/step - loss: 0.3943 - val_loss: 0.3792\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3731                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 2s 336us/step - loss: 0.3874 - val_loss: 0.3783\n",
      "val_spearman-rho: 0.3731                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 306us/step - loss: 0.3835 - val_loss: 0.3750\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3754                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 2s 322us/step - loss: 0.3805 - val_loss: 0.3735\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3778                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 298us/step - loss: 0.3764 - val_loss: 0.3741\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3787                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 2s 317us/step - loss: 0.3738 - val_loss: 0.3729\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 2s 369us/step - loss: 0.3714 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 2s 326us/step - loss: 0.3690 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 2s 313us/step - loss: 0.3657 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 2s 323us/step - loss: 0.3647 - val_loss: 0.3724\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 2s 329us/step - loss: 0.3634 - val_loss: 0.3714\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3855                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.3607 - val_loss: 0.3726\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 2s 331us/step - loss: 0.3590 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3579 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 2s 313us/step - loss: 0.3565 - val_loss: 0.3726\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 1s 302us/step - loss: 0.3549 - val_loss: 0.3723\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 31/100\n",
      "4863/4863 [==============================] - 2s 320us/step - loss: 0.3521 - val_loss: 0.3712\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 32/100\n",
      "4863/4863 [==============================] - 2s 402us/step - loss: 0.3520 - val_loss: 0.3715\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 33/100\n",
      "4863/4863 [==============================] - 2s 334us/step - loss: 0.3506 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 34/100\n",
      "4863/4863 [==============================] - 2s 385us/step - loss: 0.3492 - val_loss: 0.3715\n",
      "val_spearman-rho: 0.386                                                                                                    \n",
      "Epoch 35/100\n",
      "4863/4863 [==============================] - 2s 319us/step - loss: 0.3491 - val_loss: 0.3719\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3866                                                                                                    \n",
      "Epoch 36/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.3476 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3866                                                                                                    \n",
      "Epoch 37/100\n",
      "4863/4863 [==============================] - 2s 345us/step - loss: 0.3456 - val_loss: 0.3724\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3871                                                                                                    \n",
      "Epoch 38/100\n",
      "4863/4863 [==============================] - 2s 375us/step - loss: 0.3448 - val_loss: 0.3722\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3879                                                                                                    \n",
      "Epoch 39/100\n",
      "4863/4863 [==============================] - 2s 318us/step - loss: 0.3449 - val_loss: 0.3721\n",
      "val_spearman-rho: 0.3879                                                                                                    \n",
      "Epoch 40/100\n",
      "4863/4863 [==============================] - 2s 329us/step - loss: 0.3441 - val_loss: 0.3722\n",
      "model saved weights_3.hdf5\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 41/100\n",
      "4863/4863 [==============================] - 1s 304us/step - loss: 0.3435 - val_loss: 0.3718\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 42/100\n",
      "4863/4863 [==============================] - 2s 312us/step - loss: 0.3428 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 43/100\n",
      "4863/4863 [==============================] - 2s 311us/step - loss: 0.3415 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 44/100\n",
      "4863/4863 [==============================] - 2s 334us/step - loss: 0.3417 - val_loss: 0.3730\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 45/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.3410 - val_loss: 0.3727\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 46/100\n",
      "4863/4863 [==============================] - 1s 303us/step - loss: 0.3400 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.400999692326877e-05.\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 47/100\n",
      "4863/4863 [==============================] - 1s 307us/step - loss: 0.3400 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 48/100\n",
      "4863/4863 [==============================] - 1s 302us/step - loss: 0.3392 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 49/100\n",
      "4863/4863 [==============================] - 2s 357us/step - loss: 0.3394 - val_loss: 0.3727\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 50/100\n",
      "4863/4863 [==============================] - 2s 327us/step - loss: 0.3380 - val_loss: 0.3726\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 51/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3387 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.6806997336971108e-05.\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 00051: early stopping\n",
      "Train on 4864 samples, validate on 1215 samples\n",
      "Epoch 1/100\n",
      "4864/4864 [==============================] - 2s 489us/step - loss: 0.7763 - val_loss: 0.6627\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.2592                                                                                                    \n",
      "Epoch 2/100\n",
      "4864/4864 [==============================] - 2s 346us/step - loss: 0.7093 - val_loss: 0.6355\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.2855                                                                                                    \n",
      "Epoch 3/100\n",
      "4864/4864 [==============================] - 2s 332us/step - loss: 0.6785 - val_loss: 0.6165\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3017                                                                                                    \n",
      "Epoch 4/100\n",
      "4864/4864 [==============================] - 2s 315us/step - loss: 0.6507 - val_loss: 0.5873\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3089                                                                                                    \n",
      "Epoch 5/100\n",
      "4864/4864 [==============================] - 1s 303us/step - loss: 0.6211 - val_loss: 0.5585\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3221                                                                                                    \n",
      "Epoch 6/100\n",
      "4864/4864 [==============================] - 2s 347us/step - loss: 0.5861 - val_loss: 0.5337\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3279                                                                                                    \n",
      "Epoch 7/100\n",
      "4864/4864 [==============================] - 1s 305us/step - loss: 0.5505 - val_loss: 0.4972\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3292                                                                                                    \n",
      "Epoch 8/100\n",
      "4864/4864 [==============================] - 2s 311us/step - loss: 0.5151 - val_loss: 0.4650\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.333                                                                                                    \n",
      "Epoch 9/100\n",
      "4864/4864 [==============================] - 2s 372us/step - loss: 0.4837 - val_loss: 0.4462\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3375                                                                                                    \n",
      "Epoch 10/100\n",
      "4864/4864 [==============================] - 1s 303us/step - loss: 0.4574 - val_loss: 0.4239\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3425                                                                                                    \n",
      "Epoch 11/100\n",
      "4864/4864 [==============================] - 1s 299us/step - loss: 0.4360 - val_loss: 0.4076\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3479                                                                                                    \n",
      "Epoch 12/100\n",
      "4864/4864 [==============================] - 2s 316us/step - loss: 0.4194 - val_loss: 0.3988\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3511                                                                                                    \n",
      "Epoch 13/100\n",
      "4864/4864 [==============================] - 2s 397us/step - loss: 0.4064 - val_loss: 0.3905\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3535                                                                                                    \n",
      "Epoch 14/100\n",
      "4864/4864 [==============================] - 2s 327us/step - loss: 0.3990 - val_loss: 0.3844\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3547                                                                                                    \n",
      "Epoch 15/100\n",
      "4864/4864 [==============================] - 2s 321us/step - loss: 0.3915 - val_loss: 0.3814\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3562                                                                                                    \n",
      "Epoch 16/100\n",
      "4864/4864 [==============================] - 2s 390us/step - loss: 0.3858 - val_loss: 0.3793\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.361                                                                                                    \n",
      "Epoch 17/100\n",
      "4864/4864 [==============================] - 2s 315us/step - loss: 0.3811 - val_loss: 0.3775\n",
      "val_spearman-rho: 0.361                                                                                                    \n",
      "Epoch 18/100\n",
      "4864/4864 [==============================] - 2s 319us/step - loss: 0.3776 - val_loss: 0.3767\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3639                                                                                                    \n",
      "Epoch 19/100\n",
      "4864/4864 [==============================] - 2s 368us/step - loss: 0.3750 - val_loss: 0.3762\n",
      "val_spearman-rho: 0.3639                                                                                                    \n",
      "Epoch 20/100\n",
      "4864/4864 [==============================] - 2s 336us/step - loss: 0.3715 - val_loss: 0.3771\n",
      "val_spearman-rho: 0.3639                                                                                                    \n",
      "Epoch 21/100\n",
      "4864/4864 [==============================] - 2s 324us/step - loss: 0.3692 - val_loss: 0.3759\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3643                                                                                                    \n",
      "Epoch 22/100\n",
      "4864/4864 [==============================] - 1s 304us/step - loss: 0.3669 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.3643                                                                                                    \n",
      "Epoch 23/100\n",
      "4864/4864 [==============================] - 1s 308us/step - loss: 0.3647 - val_loss: 0.3753\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3645                                                                                                    \n",
      "Epoch 24/100\n",
      "4864/4864 [==============================] - 2s 315us/step - loss: 0.3638 - val_loss: 0.3774\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3683                                                                                                    \n",
      "Epoch 25/100\n",
      "4864/4864 [==============================] - 2s 329us/step - loss: 0.3612 - val_loss: 0.3748\n",
      "model saved weights_4.hdf5\n",
      "val_spearman-rho: 0.3698                                                                                                    \n",
      "Epoch 26/100\n",
      "4544/4864 [===========================>..] - ETA: 0s - loss: 0.3591"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "all_predictions1 = np.zeros((n_splits,X_test.shape[0],y_train.shape[1]))\n",
    "oof_pred1 = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "for ind, (tr, val) in enumerate(kf.split(X_train)):\n",
    "    X_tr = X_train[tr]\n",
    "    y_tr = y_train[tr]\n",
    "    X_vl = X_train[val]\n",
    "    y_vl = y_train[val]\n",
    "    \n",
    "    model = create_model()\n",
    "    early = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, verbose=1, mode='auto', min_lr=0.000001)\n",
    "    rho = SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n",
    "                                       patience=15, model_name='weights_{}.hdf5'.format(ind))\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n",
    "        callbacks=[lr,rho,early]\n",
    "    )\n",
    "    model.load_weights('weights_{}.hdf5'.format(ind))\n",
    "    \n",
    "    oof_pred1[val,:] = model.predict(X_vl)\n",
    "    all_predictions1[ind,:,:] = model.predict(X_test)\n",
    "\n",
    "all_predictions1 = all_predictions1.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_predictions1 = np.zeros((X_test.shape[0],y_train.shape[1]))\\noof_pred1 = np.zeros((y_train.shape[0],y_train.shape[1]))\\n\\n #KFold(n_splits=n_splits, random_state=42, shuffle=True)\\n\\nfor col_ind, col in enumerate(targets):\\n    \\n    if train[col].nunique() >= 5:\\n        n_splits = 5\\n    else:\\n        n_splits = train[col].nunique()\\n    \\n    kf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\\n    \\n    temp_all_prediction = np.zeros((n_splits,X_test.shape[0]))\\n    \\n    for ind, (tr, val) in enumerate(kf.split(X_train,y_categorized[col])):\\n        X_tr = X_train[tr]\\n        y_tr = y_train[tr]\\n        X_vl = X_train[val]\\n        y_vl = y_train[val]\\n\\n        model = create_model2()\\n        early = EarlyStopping(monitor=\\'val_loss\\', patience=15, verbose=0, mode=\\'auto\\', baseline=None, restore_best_weights=False)\\n        lr = ReduceLROnPlateau(monitor=\\'val_loss\\', factor=0.7, patience=5, verbose=0, mode=\\'auto\\', min_lr=0.000001)\\n        checkpointer = ModelCheckpoint(monitor=\\'val_loss\\',filepath=\\'weights_simple_dnn_{}_{}.hdf5\\'.format(col,ind), mode=\\'min\\',verbose=0, save_best_only=True)\\n\\n        model.fit(\\n            X_tr, y_tr[:,col_ind], epochs=100, batch_size=32, validation_data=(X_vl, y_vl[:,col_ind]), verbose=0, \\n            callbacks=[early,lr,checkpointer]\\n        )\\n        model.load_weights(\\'weights_simple_dnn_{}_{}.hdf5\\'.format(col,ind))\\n\\n        oof_pred1[val,col_ind] = model.predict(X_vl)[:,0]\\n        temp_all_prediction[ind,:] = model.predict(X_test)[:,0]\\n        \\n    all_predictions1[:,col_ind] = temp_all_prediction.mean(axis=0)\\n    print (\"{} oof spearman correlation {}\".format(col, spearmanr(y_train[:,col_ind],oof_pred1[:,col_ind]).correlation))\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "all_predictions1 = np.zeros((X_test.shape[0],y_train.shape[1]))\n",
    "oof_pred1 = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    " #KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "\n",
    "for col_ind, col in enumerate(targets):\n",
    "    \n",
    "    if train[col].nunique() >= 5:\n",
    "        n_splits = 5\n",
    "    else:\n",
    "        n_splits = train[col].nunique()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    \n",
    "    temp_all_prediction = np.zeros((n_splits,X_test.shape[0]))\n",
    "    \n",
    "    for ind, (tr, val) in enumerate(kf.split(X_train,y_categorized[col])):\n",
    "        X_tr = X_train[tr]\n",
    "        y_tr = y_train[tr]\n",
    "        X_vl = X_train[val]\n",
    "        y_vl = y_train[val]\n",
    "\n",
    "        model = create_model2()\n",
    "        early = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, verbose=0, mode='auto', min_lr=0.000001)\n",
    "        checkpointer = ModelCheckpoint(monitor='val_loss',filepath='weights_simple_dnn_{}_{}.hdf5'.format(col,ind), mode='min',verbose=0, save_best_only=True)\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr[:,col_ind], epochs=100, batch_size=32, validation_data=(X_vl, y_vl[:,col_ind]), verbose=0, \n",
    "            callbacks=[early,lr,checkpointer]\n",
    "        )\n",
    "        model.load_weights('weights_simple_dnn_{}_{}.hdf5'.format(col,ind))\n",
    "\n",
    "        oof_pred1[val,col_ind] = model.predict(X_vl)[:,0]\n",
    "        temp_all_prediction[ind,:] = model.predict(X_test)[:,0]\n",
    "        \n",
    "    all_predictions1[:,col_ind] = temp_all_prediction.mean(axis=0)\n",
    "    print (\"{} oof spearman correlation {}\".format(col, spearmanr(y_train[:,col_ind],oof_pred1[:,col_ind]).correlation))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding oof spearman correlation 0.3730715276704457\n",
      "question_body_critical oof spearman correlation 0.6698296961033138\n",
      "question_conversational oof spearman correlation 0.38249373683481447\n",
      "question_expect_short_answer oof spearman correlation 0.23359051921497417\n",
      "question_fact_seeking oof spearman correlation 0.3048583449739141\n",
      "question_has_commonly_accepted_answer oof spearman correlation 0.40775509366903534\n",
      "question_interestingness_others oof spearman correlation 0.3662207009636641\n",
      "question_interestingness_self oof spearman correlation 0.5156662155226513\n",
      "question_multi_intent oof spearman correlation 0.5083096469256448\n",
      "question_not_really_a_question oof spearman correlation 0.04143603936954794\n",
      "question_opinion_seeking oof spearman correlation 0.4325142110252646\n",
      "question_type_choice oof spearman correlation 0.6531958401184711\n",
      "question_type_compare oof spearman correlation 0.3107138379267106\n",
      "question_type_consequence oof spearman correlation 0.12365837383897144\n",
      "question_type_definition oof spearman correlation 0.3471420927552371\n",
      "question_type_entity oof spearman correlation 0.41212023045716617\n",
      "question_type_instructions oof spearman correlation 0.7516438615162744\n",
      "question_type_procedure oof spearman correlation 0.29060099121322597\n",
      "question_type_reason_explanation oof spearman correlation 0.6106123782247295\n",
      "question_type_spelling oof spearman correlation 0.004015441206002841\n",
      "question_well_written oof spearman correlation 0.5295955498285302\n",
      "answer_helpful oof spearman correlation 0.236126546170275\n",
      "answer_level_of_information oof spearman correlation 0.4195279386164147\n",
      "answer_plausible oof spearman correlation 0.13367186040622117\n",
      "answer_relevance oof spearman correlation 0.16519296036517445\n",
      "answer_satisfaction oof spearman correlation 0.2990591217356978\n",
      "answer_type_instructions oof spearman correlation 0.74980887626311\n",
      "answer_type_procedure oof spearman correlation 0.25369816155733016\n",
      "answer_type_reason_explanation oof spearman correlation 0.6444710287516938\n",
      "answer_well_written oof spearman correlation 0.21406070120602572\n"
     ]
    }
   ],
   "source": [
    "all_predictions2 = np.zeros((X_test.shape[0],y_train.shape[1]))\n",
    "oof_pred2 = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "#X_train2 = new_X[:X_train.shape[0],:]\n",
    "#X_test2 = new_X[X_train.shape[0]:,:]\n",
    "\n",
    "for col_ind, col in enumerate(targets):\n",
    "    \n",
    "    if train[col].nunique() >= 5:\n",
    "        n_splits = 5\n",
    "    else:\n",
    "        n_splits = train[col].nunique()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True) #StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    \n",
    "    temp_all_prediction = np.zeros((n_splits,X_test.shape[0]))\n",
    "    \n",
    "    for ind, (tr, val) in enumerate(kf.split(X_train,y_categorized[col])):\n",
    "        X_tr = X_train[tr]\n",
    "        y_tr = y_train[tr]\n",
    "        X_vl = X_train[val]\n",
    "        y_vl = y_train[val]\n",
    "\n",
    "        model = ElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5) #RandomForestRegressor(n_estimators=100,max_features=.4, random_state=123) #BayesianRidge() #MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n",
    "        model.fit(X_tr, y_tr[:,col_ind])\n",
    "        \n",
    "        oof_pred2[val,col_ind] = model.predict(X_vl).copy()\n",
    "        temp_all_prediction[ind,:] = model.predict(X_test).copy()\n",
    "        \n",
    "    all_predictions2[:,col_ind] = temp_all_prediction.mean(axis=0).copy()\n",
    "    \n",
    "    print (\"{} oof spearman correlation {}\".format(col, spearmanr(y_train[:,col_ind],oof_pred2[:,col_ind]).correlation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred1 = np.clip(oof_pred1,0.0001,.9999)\n",
    "all_predictions1 = np.clip(all_predictions1,0.0001,.9999)\n",
    "\n",
    "oof_pred2 = np.clip(oof_pred2,0.0001,.9999)\n",
    "all_predictions2 = np.clip(all_predictions2,0.0001,.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding 0.3469617130492376 0.3730693533470354\n",
      "question_body_critical 0.6662751459827833 0.6698296961033138\n",
      "question_conversational 0.4111431713601106 0.38414231089949624\n",
      "question_expect_short_answer 0.23302461662112858 0.23358259196425132\n",
      "question_fact_seeking 0.35262325497848024 0.30484451629892223\n",
      "question_has_commonly_accepted_answer 0.3996983056658262 0.4074688857102291\n",
      "question_interestingness_others 0.3437042400025455 0.3662207009636641\n",
      "question_interestingness_self 0.4929949698209517 0.5156662155226513\n",
      "question_multi_intent 0.5056850916768006 0.5082320168834893\n",
      "question_not_really_a_question 0.05375430331113943 0.04143605830533567\n",
      "question_opinion_seeking 0.45541580436233997 0.43250998394591345\n",
      "question_type_choice 0.6419733407163112 0.653154688282517\n",
      "question_type_compare 0.3359199284595235 0.3134468608665377\n",
      "question_type_consequence 0.16153433250898258 0.12340812733756015\n",
      "question_type_definition 0.34908810825050024 0.3538972210043042\n",
      "question_type_entity 0.42001918779825737 0.4148296819853092\n",
      "question_type_instructions 0.7568625353402488 0.7517240363051277\n",
      "question_type_procedure 0.318253623594053 0.29061916033552126\n",
      "question_type_reason_explanation 0.6117041711019734 0.6105809896353077\n",
      "question_type_spelling 0.05124914331875605 0.004015441206002841\n",
      "question_well_written 0.5283289083040406 0.5295957923042585\n",
      "answer_helpful 0.20920828919605086 0.236126546170275\n",
      "answer_level_of_information 0.4035189871387066 0.4195279386164147\n",
      "answer_plausible 0.14220905435626827 0.13367186040622117\n",
      "answer_relevance 0.1563831696368581 0.16519296044238638\n",
      "answer_satisfaction 0.31338818545911584 0.2990591217356978\n",
      "answer_type_instructions 0.7454069203176875 0.749918645570687\n",
      "answer_type_procedure 0.2658399358090789 0.25369395661651023\n",
      "answer_type_reason_explanation 0.6421136120631172 0.644383269633455\n",
      "answer_well_written 0.16927971227104716 0.21406070120602572\n",
      "Avg scores 0.3827853920823973, 0.3799303109868142\n"
     ]
    }
   ],
   "source": [
    "score1 = 0\n",
    "score2 = 0\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    score1 += spearmanr(y_train[:,i],oof_pred1[:,i]).correlation\n",
    "    score2 += spearmanr(y_train[:,i],oof_pred2[:,i]).correlation\n",
    "    print (val,spearmanr(y_train[:,i],oof_pred1[:,i]).correlation, spearmanr(y_train[:,i],oof_pred2[:,i]).correlation) #\n",
    "\n",
    "print (\"Avg scores {}, {}\".format(score1/30, score2/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pred1 = all_predictions1.copy() #np.zeros((all_predictions1.shape[0],all_predictions1.shape[1]))\n",
    "best_oof_pred1 = oof_pred1.copy() #np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    if spearmanr(y_train[:,i],oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],oof_pred2[:,i]).correlation:\n",
    "        best_oof_pred1[:,i] = oof_pred1[:,i]\n",
    "        main_pred1[:,i] = all_predictions1[:,i]\n",
    "    else:\n",
    "        best_oof_pred1[:,i] = oof_pred2[:,i]\n",
    "        main_pred1[:,i] = all_predictions1[:,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = create_model()\\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\\nall_predictions.append(model.predict(X_test))\\n    \\nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\\nmodel.fit(X_train, y_train)\\nall_predictions.append(model.predict(X_test))\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\n",
    "all_predictions.append(model.predict(X_test))\n",
    "    \n",
    "model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "all_predictions.append(model.predict(X_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.2        0.26666667 0.3        0.33333333 0.33333333\n",
      " 0.4        0.44444444 0.46666667 0.5        0.53333333 0.55555556\n",
      " 0.6        0.66666667 0.66666667 0.7        0.73333333 0.77777778\n",
      " 0.8        0.83333333 0.86666667 0.88888889 0.9        0.93333333\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "uniq_numbers = np.unique(y_train.flatten())\n",
    "print (uniq_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounder(values):\n",
    "    def f(x):\n",
    "        idx = np.argmin(np.abs(values - x))\n",
    "        return values[idx]\n",
    "    return np.frompyfunc(f, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_oof_pred1 = np.array([rounder(uniq_numbers)(i) for i in oof_pred1])\n",
    "#rounded_oof_pred1[:,9] = oof_pred1[:,9]\n",
    "rounded_oof_pred2 = np.array([rounder(uniq_numbers)(i) for i in oof_pred2])\n",
    "#rounded_oof_pred2[:,9] = oof_pred2[:,9]\n",
    "\n",
    "rounded_oof_pred1 = np.clip(rounded_oof_pred1,.0001,.9999)\n",
    "rounded_oof_pred2 = np.clip(rounded_oof_pred2,.0001,.9999)\n",
    "\n",
    "rounded_all_prediction1 = np.array([rounder(uniq_numbers)(i) for i in all_predictions1])\n",
    "#rounded_all_prediction1[:,9] = all_prediction1[:,9]\n",
    "rounded_all_prediction2 = np.array([rounder(uniq_numbers)(i) for i in all_predictions2])\n",
    "#rounded_all_prediction2[:,9] = all_prediction2[:,9]\n",
    "\n",
    "rounded_all_prediction1 = np.clip(rounded_all_prediction1,.0001,.9999)\n",
    "rounded_all_prediction2 = np.clip(rounded_all_prediction2,.0001,.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding 0.332018696542939 0.3662668507234609\n",
      "question_body_critical 0.6642385312477265 0.6694122138881343\n",
      "question_conversational 0.49307135356507065 0.44393693543088025\n",
      "question_expect_short_answer 0.23302850657187904 0.23387967545502003\n",
      "question_fact_seeking 0.35210442544296666 0.30464092502758877\n",
      "question_has_commonly_accepted_answer 0.40562349924498414 0.40598448205699256\n",
      "question_interestingness_others 0.3373435498009048 0.3537033912011825\n",
      "question_interestingness_self 0.49232989490729073 0.5158843222110728\n",
      "question_multi_intent 0.49812346733427487 0.5025292771173461\n",
      "question_not_really_a_question 0.05375430331113943 0.04143605830533567\n",
      "question_opinion_seeking 0.4513018485291257 0.4310858819589078\n",
      "question_type_choice 0.6418314318406168 0.651241305023619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_type_compare 0.4724780446461082 0.41528496381743224\n",
      "question_type_consequence 0.11823528064988638 0.12340812733756015\n",
      "question_type_definition 0.5946577908868264 0.5809641451583457\n",
      "question_type_entity 0.5397049791879915 0.4695930375832562\n",
      "question_type_instructions 0.7571368386537496 0.7518912940196588\n",
      "question_type_procedure 0.2929005097920065 0.26862363999933897\n",
      "question_type_reason_explanation 0.6107535392704313 0.608780426388559\n",
      "question_type_spelling 0.30106450177380784 0.004015441206002841\n",
      "question_well_written 0.5265860360291801 0.5256164658213264\n",
      "answer_helpful 0.19996383864088219 0.21858677044022753\n",
      "answer_level_of_information 0.3939750063555886 0.40263667847063056\n",
      "answer_plausible 0.1284534115225415 0.08708058483724049\n",
      "answer_relevance 0.15555641875005233 0.15168264549570454\n",
      "answer_satisfaction 0.3094953658227542 0.2900052177519507\n",
      "answer_type_instructions 0.7456826371651855 0.7493561093239108\n",
      "answer_type_procedure 0.22525003281426723 0.2142287936121605\n",
      "answer_type_reason_explanation 0.6396773262582883 0.6428146159300724\n",
      "answer_well_written 0.15613609677186843 0.2003562672661831\n",
      "Avg scores 0.40408257211101106, 0.3874975514286367\n"
     ]
    }
   ],
   "source": [
    "score1 = 0\n",
    "score2 = 0\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    val1 = spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation\n",
    "    val2 = spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation\n",
    "\n",
    "    if pd.notnull(val1) == False:\n",
    "        val1 = spearmanr(y_train[:,i],oof_pred1[:,i]).correlation\n",
    "    if pd.notnull(val2) == False:\n",
    "        val2 = spearmanr(y_train[:,i],oof_pred2[:,i]).correlation\n",
    "    \n",
    "    score1 += val1\n",
    "    score2 += val2\n",
    "    print (val,val1, val2)\n",
    "            \n",
    "\n",
    "print (\"Avg scores {}, {}\".format(score1/30, score2/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pred2 = rounded_all_prediction1.copy() #np.zeros((all_predictions1.shape[0],all_predictions1.shape[1]))\n",
    "best_oof_pred2 = rounded_oof_pred1.copy() #np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    if spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation:\n",
    "        best_oof_pred2[:,i] = rounded_oof_pred1[:,i]\n",
    "        main_pred2[:,i] = rounded_all_prediction1[:,i]\n",
    "    else:\n",
    "        best_oof_pred2[:,i] = rounded_oof_pred2[:,i]\n",
    "        main_pred2[:,i] = rounded_all_prediction2[:,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(targets):\n",
    "    if pd.notnull(spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation) == False:\n",
    "        best_oof_pred2[:,i] = best_oof_pred1[:,i].copy()\n",
    "        main_pred2[:,i] = main_pred1[:,i].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmain_pred = np.zeros((all_predictions1.shape[0],all_predictions1.shape[1]))\\nfinal_score = 0\\nfor i, val in enumerate(targets):\\n    if spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation:\\n        final_score += spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation\\n        print (val, spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation)\\n        main_pred[:,i] = main_pred1[:,i]\\n    else:\\n        final_score += spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation\\n        print (val, spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation)\\n        main_pred[:,i] = main_pred2[:,i]\\n\\nprint (\"Avg scores {}\".format(final_score/30))\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_pred = main_pred2.copy()\n",
    "'''\n",
    "main_pred = np.zeros((all_predictions1.shape[0],all_predictions1.shape[1]))\n",
    "final_score = 0\n",
    "for i, val in enumerate(targets):\n",
    "    if spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation:\n",
    "        final_score += spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation\n",
    "        print (val, spearmanr(y_train[:,i],best_oof_pred1[:,i]).correlation)\n",
    "        main_pred[:,i] = main_pred1[:,i]\n",
    "    else:\n",
    "        final_score += spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation\n",
    "        print (val, spearmanr(y_train[:,i],best_oof_pred2[:,i]).correlation)\n",
    "        main_pred[:,i] = main_pred2[:,i]\n",
    "\n",
    "print (\"Avg scores {}\".format(final_score/30))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_pred1 = all_predictions1.mean(axis=0)\\ntest_pred2 = all_predictions2.mean(axis=0)\\nmain_pred = np.zeros((test_pred1.shape[0],test_pred1.shape[1]))\\n\\nfor i in range(test_pred1.shape[1]):\\n    if spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation:\\n        main_pred[:,i] = rounder(uniq_numbers)(test_pred1[:,i])\\n    else:\\n        main_pred[:,i] = rounder(uniq_numbers)(test_pred2[:,i])\\n        \\nfor i in range(main_pred.shape[1]):\\n    if main_pred[:,i].sum() == 0:\\n        if spearmanr(y_train[:,i],oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],oof_pred2[:,i]).correlation:\\n            main_pred[:,i] = test_pred1[:,i]\\n        else:\\n            main_pred[:,i] = test_pred2[:,i]\\n            \\nmain_pred = np.clip(main_pred,0.0001,0.9999)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_pred1 = all_predictions1.mean(axis=0)\n",
    "test_pred2 = all_predictions2.mean(axis=0)\n",
    "main_pred = np.zeros((test_pred1.shape[0],test_pred1.shape[1]))\n",
    "\n",
    "for i in range(test_pred1.shape[1]):\n",
    "    if spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation:\n",
    "        main_pred[:,i] = rounder(uniq_numbers)(test_pred1[:,i])\n",
    "    else:\n",
    "        main_pred[:,i] = rounder(uniq_numbers)(test_pred2[:,i])\n",
    "        \n",
    "for i in range(main_pred.shape[1]):\n",
    "    if main_pred[:,i].sum() == 0:\n",
    "        if spearmanr(y_train[:,i],oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],oof_pred2[:,i]).correlation:\n",
    "            main_pred[:,i] = test_pred1[:,i]\n",
    "        else:\n",
    "            main_pred[:,i] = test_pred2[:,i]\n",
    "            \n",
    "main_pred = np.clip(main_pred,0.0001,0.9999)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5426.500000000001 420.67637777777844\n",
      "1 3618.833333333333 273.6999999999994\n",
      "2 348.3333333333333 10.34359999999994\n",
      "3 4246.333333333333 333.6990999999997\n",
      "4 4696.833333333333 388.43163333333405\n",
      "5 4824.833333333334 398.3824888888896\n",
      "6 3571.277777777778 275.6111111111099\n",
      "7 3083.722222222222 232.81111111111167\n",
      "8 1451.3333333333333 111.68677777777816\n",
      "9 27.166666666666664 2.2950573642912793\n",
      "10 2613.833333333333 185.54684444444476\n",
      "11 1732.0 136.29678888888927\n",
      "12 231.83333333333331 6.622977777777768\n",
      "13 61.0 3.669311378145359\n",
      "14 187.0 4.823677777777785\n",
      "15 396.5 13.498855555555515\n",
      "16 3024.833333333333 265.3636555555557\n",
      "17 1009.5 75.80368888888944\n",
      "18 2348.833333333333 189.6831777777778\n",
      "19 5.0 1.349946885622922\n",
      "20 4862.777777777777 373.9000000000003\n",
      "21 5625.555555555556 439.7426444444451\n",
      "22 3980.666666666667 312.2777777777774\n",
      "23 5836.166666666666 465.15688888889235\n",
      "24 5888.277777777779 469.6061444444483\n",
      "25 5195.6 411.96666666666704\n",
      "26 2915.166666666667 248.80369999999962\n",
      "27 794.1666666666665 61.38656666666712\n",
      "28 3054.5 240.67887777777761\n",
      "29 5521.277777777778 431.111111111111\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print (i, y_train[:,i].sum(), main_pred[:,i].sum()) #main_pred[:,i].max(), main_pred[:,i].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\n",
    "submission[targets] = main_pred\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.011951</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.933333                0.600000   \n",
       "1     46                             0.866667                0.466667   \n",
       "2     70                             0.900000                0.666667   \n",
       "3    132                             0.833333                0.300000   \n",
       "4    200                             0.933333                0.555556   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                   0.0001                      0.666667   \n",
       "1                   0.0001                      0.733333   \n",
       "2                   0.0001                      0.700000   \n",
       "3                   0.0001                      0.700000   \n",
       "4                   0.0001                      0.900000   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.888889                               0.666667   \n",
       "1               0.777778                               0.933333   \n",
       "2               0.933333                               0.933333   \n",
       "3               0.800000                               0.933333   \n",
       "4               0.800000                               0.866667   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.666667                       0.555556   \n",
       "1                         0.555556                       0.466667   \n",
       "2                         0.600000                       0.500000   \n",
       "3                         0.533333                       0.444444   \n",
       "4                         0.600000                       0.600000   \n",
       "\n",
       "   question_multi_intent  question_not_really_a_question  \\\n",
       "0               0.400000                        0.003708   \n",
       "1               0.000100                        0.004768   \n",
       "2               0.200000                        0.003357   \n",
       "3               0.266667                        0.004319   \n",
       "4               0.300000                        0.003336   \n",
       "\n",
       "   question_opinion_seeking  question_type_choice  question_type_compare  \\\n",
       "0                  0.266667              0.666667                 0.0001   \n",
       "1                  0.444444              0.333333                 0.0001   \n",
       "2                  0.200000              0.555556                 0.0001   \n",
       "3                  0.533333              0.333333                 0.0001   \n",
       "4                  0.400000              0.466667                 0.0001   \n",
       "\n",
       "   question_type_consequence  question_type_definition  question_type_entity  \\\n",
       "0                   0.041251                    0.0001                0.0001   \n",
       "1                   0.001602                    0.0001                0.0001   \n",
       "2                   0.011951                    0.0001                0.0001   \n",
       "3                   0.005078                    0.0001                0.0001   \n",
       "4                   0.004108                    0.0001                0.0001   \n",
       "\n",
       "   question_type_instructions  question_type_procedure  \\\n",
       "0                    0.000100                   0.0001   \n",
       "1                    0.933333                   0.2000   \n",
       "2                    0.200000                   0.0001   \n",
       "3                    0.833333                   0.2000   \n",
       "4                    0.200000                   0.2000   \n",
       "\n",
       "   question_type_reason_explanation  question_type_spelling  \\\n",
       "0                          0.700000                0.002718   \n",
       "1                          0.200000                0.002144   \n",
       "2                          0.533333                0.002183   \n",
       "3                          0.666667                0.002819   \n",
       "4                          0.533333                0.002433   \n",
       "\n",
       "   question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0               0.866667        0.888889                     0.600000   \n",
       "1               0.555556        0.933333                     0.600000   \n",
       "2               0.866667        0.888889                     0.600000   \n",
       "3               0.733333        0.933333                     0.700000   \n",
       "4               0.800000        0.900000                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          0.933333          0.999900             0.833333   \n",
       "1          0.999900          0.999900             0.866667   \n",
       "2          0.999900          0.933333             0.833333   \n",
       "3          0.999900          0.999900             0.900000   \n",
       "4          0.999900          0.999900             0.888889   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                    0.2000                 0.0001   \n",
       "1                    0.9000                 0.0001   \n",
       "2                    0.0001                 0.0001   \n",
       "3                    0.8000                 0.2000   \n",
       "4                    0.4000                 0.2000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.733333             0.900000  \n",
       "1                        0.200000             0.888889  \n",
       "2                        0.833333             0.900000  \n",
       "3                        0.555556             0.900000  \n",
       "4                        0.500000             0.888889  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5029.186975</td>\n",
       "      <td>0.883774</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.021730</td>\n",
       "      <td>0.701049</td>\n",
       "      <td>0.816033</td>\n",
       "      <td>0.836938</td>\n",
       "      <td>0.579015</td>\n",
       "      <td>0.489099</td>\n",
       "      <td>0.234636</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.389804</td>\n",
       "      <td>0.286338</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.007709</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.028359</td>\n",
       "      <td>0.557487</td>\n",
       "      <td>0.159251</td>\n",
       "      <td>0.398494</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.785504</td>\n",
       "      <td>0.923829</td>\n",
       "      <td>0.656046</td>\n",
       "      <td>0.977220</td>\n",
       "      <td>0.986568</td>\n",
       "      <td>0.865476</td>\n",
       "      <td>0.522697</td>\n",
       "      <td>0.128963</td>\n",
       "      <td>0.505628</td>\n",
       "      <td>0.905696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2812.670060</td>\n",
       "      <td>0.042249</td>\n",
       "      <td>0.144524</td>\n",
       "      <td>0.077757</td>\n",
       "      <td>0.107386</td>\n",
       "      <td>0.116399</td>\n",
       "      <td>0.129285</td>\n",
       "      <td>0.049321</td>\n",
       "      <td>0.085507</td>\n",
       "      <td>0.146316</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.182906</td>\n",
       "      <td>0.213596</td>\n",
       "      <td>0.068516</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>0.352534</td>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.271395</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.102480</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.048916</td>\n",
       "      <td>0.033009</td>\n",
       "      <td>0.027314</td>\n",
       "      <td>0.047660</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.114637</td>\n",
       "      <td>0.240487</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2572.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5093.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7482.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.009005</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9640.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.017064</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.076983</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             qa_id  question_asker_intent_understanding  \\\n",
       "count   476.000000                           476.000000   \n",
       "mean   5029.186975                             0.883774   \n",
       "std    2812.670060                             0.042249   \n",
       "min      39.000000                             0.777778   \n",
       "25%    2572.000000                             0.866667   \n",
       "50%    5093.000000                             0.888889   \n",
       "75%    7482.000000                             0.900000   \n",
       "max    9640.000000                             0.999900   \n",
       "\n",
       "       question_body_critical  question_conversational  \\\n",
       "count              476.000000               476.000000   \n",
       "mean                 0.575000                 0.021730   \n",
       "std                  0.144524                 0.077757   \n",
       "min                  0.266667                 0.000100   \n",
       "25%                  0.466667                 0.000100   \n",
       "50%                  0.555556                 0.000100   \n",
       "75%                  0.700000                 0.000100   \n",
       "max                  0.933333                 0.733333   \n",
       "\n",
       "       question_expect_short_answer  question_fact_seeking  \\\n",
       "count                    476.000000             476.000000   \n",
       "mean                       0.701049               0.816033   \n",
       "std                        0.107386               0.116399   \n",
       "min                        0.266667               0.200000   \n",
       "25%                        0.666667               0.777778   \n",
       "50%                        0.700000               0.833333   \n",
       "75%                        0.777778               0.900000   \n",
       "max                        0.999900               0.999900   \n",
       "\n",
       "       question_has_commonly_accepted_answer  question_interestingness_others  \\\n",
       "count                             476.000000                       476.000000   \n",
       "mean                                0.836938                         0.579015   \n",
       "std                                 0.129285                         0.049321   \n",
       "min                                 0.200000                         0.500000   \n",
       "25%                                 0.777778                         0.533333   \n",
       "50%                                 0.866667                         0.555556   \n",
       "75%                                 0.933333                         0.600000   \n",
       "max                                 0.999900                         0.700000   \n",
       "\n",
       "       question_interestingness_self  question_multi_intent  \\\n",
       "count                     476.000000             476.000000   \n",
       "mean                        0.489099               0.234636   \n",
       "std                         0.085507               0.146316   \n",
       "min                         0.333333               0.000100   \n",
       "25%                         0.444444               0.200000   \n",
       "50%                         0.466667               0.200000   \n",
       "75%                         0.533333               0.333333   \n",
       "max                         0.800000               0.666667   \n",
       "\n",
       "       question_not_really_a_question  question_opinion_seeking  \\\n",
       "count                      476.000000                476.000000   \n",
       "mean                         0.004822                  0.389804   \n",
       "std                          0.002701                  0.182906   \n",
       "min                          0.000386                  0.000100   \n",
       "25%                          0.003002                  0.266667   \n",
       "50%                          0.004310                  0.400000   \n",
       "75%                          0.005812                  0.500000   \n",
       "max                          0.017064                  0.933333   \n",
       "\n",
       "       question_type_choice  question_type_compare  question_type_consequence  \\\n",
       "count            476.000000             476.000000                 476.000000   \n",
       "mean               0.286338               0.013914                   0.007709   \n",
       "std                0.213596               0.068516                   0.008685   \n",
       "min                0.000100               0.000100                   0.000933   \n",
       "25%                0.200000               0.000100                   0.003090   \n",
       "50%                0.200000               0.000100                   0.004936   \n",
       "75%                0.400000               0.000100                   0.009005   \n",
       "max                0.999900               0.733333                   0.076983   \n",
       "\n",
       "       question_type_definition  question_type_entity  \\\n",
       "count                476.000000            476.000000   \n",
       "mean                   0.010134              0.028359   \n",
       "std                    0.057426              0.103146   \n",
       "min                    0.000100              0.000100   \n",
       "25%                    0.000100              0.000100   \n",
       "50%                    0.000100              0.000100   \n",
       "75%                    0.000100              0.000100   \n",
       "max                    0.600000              0.800000   \n",
       "\n",
       "       question_type_instructions  question_type_procedure  \\\n",
       "count                  476.000000               476.000000   \n",
       "mean                     0.557487                 0.159251   \n",
       "std                      0.352534                 0.116440   \n",
       "min                      0.000100                 0.000100   \n",
       "25%                      0.200000                 0.000100   \n",
       "50%                      0.700000                 0.200000   \n",
       "75%                      0.866667                 0.200000   \n",
       "max                      0.999900                 0.555556   \n",
       "\n",
       "       question_type_reason_explanation  question_type_spelling  \\\n",
       "count                        476.000000              476.000000   \n",
       "mean                           0.398494                0.002836   \n",
       "std                            0.271395                0.001410   \n",
       "min                            0.000100                0.000697   \n",
       "25%                            0.200000                0.001880   \n",
       "50%                            0.316667                0.002536   \n",
       "75%                            0.600000                0.003384   \n",
       "max                            0.999900                0.011921   \n",
       "\n",
       "       question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "count             476.000000      476.000000                   476.000000   \n",
       "mean                0.785504        0.923829                     0.656046   \n",
       "std                 0.102480        0.024865                     0.048916   \n",
       "min                 0.500000        0.833333                     0.533333   \n",
       "25%                 0.700000        0.900000                     0.600000   \n",
       "50%                 0.800000        0.933333                     0.666667   \n",
       "75%                 0.866667        0.933333                     0.666667   \n",
       "max                 0.933333        0.999900                     0.833333   \n",
       "\n",
       "       answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "count        476.000000        476.000000           476.000000   \n",
       "mean           0.977220          0.986568             0.865476   \n",
       "std            0.033009          0.027314             0.047660   \n",
       "min            0.866667          0.888889             0.666667   \n",
       "25%            0.933333          0.999900             0.833333   \n",
       "50%            0.999900          0.999900             0.866667   \n",
       "75%            0.999900          0.999900             0.900000   \n",
       "max            0.999900          0.999900             0.933333   \n",
       "\n",
       "       answer_type_instructions  answer_type_procedure  \\\n",
       "count                476.000000             476.000000   \n",
       "mean                   0.522697               0.128963   \n",
       "std                    0.292696               0.114637   \n",
       "min                    0.000100               0.000100   \n",
       "25%                    0.266667               0.000100   \n",
       "50%                    0.555556               0.200000   \n",
       "75%                    0.777778               0.200000   \n",
       "max                    0.999900               0.466667   \n",
       "\n",
       "       answer_type_reason_explanation  answer_well_written  \n",
       "count                      476.000000           476.000000  \n",
       "mean                         0.505628             0.905696  \n",
       "std                          0.240487             0.021100  \n",
       "min                          0.000100             0.833333  \n",
       "25%                          0.333333             0.888889  \n",
       "50%                          0.500000             0.900000  \n",
       "75%                          0.666667             0.933333  \n",
       "max                          0.999900             0.933333  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
