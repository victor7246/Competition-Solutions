{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../input/transformers/transformers-master/\")\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "    max_len = 512\n",
    "    fin_features = []\n",
    "    for data in tqdm(chunks(string_list, batch_size)):\n",
    "        tokenized = []\n",
    "        all_lengths = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:max_len])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:max_len])\n",
    "            all_lengths.append(len(tok))\n",
    "            \n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        #print (padded)\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        #print (attention_mask)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features1 = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        #features2 = last_hidden_states[0].cpu().numpy().mean(axis=1)\n",
    "        #features3 = np.array([last_hidden_states[0].cpu().numpy()[i,:all_lengths[i],:].mean(axis=0) for i in range(len(all_lengths))])\n",
    "        #features = np.hstack([features1,features2])\n",
    "        #features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features1)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def processSingleReview(review, d=None):\n",
    "    \"\"\"\n",
    "    Convert a raw review to a string of words\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    words = tokenizer.tokenize(letters_only.lower())\n",
    "    words = [i for i in words if i not in exclude]\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    normalized = [lemma.lemmatize(word) for word in words if word not in stops]\n",
    "    \n",
    "    #meaningful_words = [st.stem(w) for w in words if w not in stops]\n",
    "    meaningful_words = [w for w in normalized if pos_tag([w],tagset='universal')[0][1] in ['NOUN','VERB','ADJ']] #\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'losing using extension tube macro lens'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processSingleReview(\"What am I losing when using extension tubes instead of a macro lens?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from numpy.random import seed\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 41) (476, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  ...  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
       "\n",
       "  question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0              1.000000        1.000000                     0.666667   \n",
       "1              0.888889        0.888889                     0.555556   \n",
       "2              0.777778        0.777778                     0.555556   \n",
       "3              0.888889        0.833333                     0.333333   \n",
       "4              1.000000        1.000000                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          1.000000          1.000000             0.800000   \n",
       "1          0.888889          0.888889             0.666667   \n",
       "2          1.000000          1.000000             0.666667   \n",
       "3          0.833333          1.000000             0.800000   \n",
       "4          1.000000          1.000000             0.800000   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                       1.0               0.000000   \n",
       "1                       0.0               0.000000   \n",
       "2                       0.0               0.333333   \n",
       "3                       0.0               0.000000   \n",
       "4                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../input/google-quest-challenge/'\n",
    "train = pd.read_csv(path_join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(path_join(data_dir, 'test.csv'))\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain[\"clean_answer\"] = train.answer.apply(processSingleReview)\\ntrain[\"clean_question\"] = train.question_body.apply(processSingleReview)\\n\\ntest[\"clean_answer\"] = test.answer.apply(processSingleReview)\\ntest[\"clean_question\"] = test.question_body.apply(processSingleReview)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train[\"clean_answer\"] = train.answer.apply(processSingleReview)\n",
    "train[\"clean_question\"] = train.question_body.apply(processSingleReview)\n",
    "\n",
    "test[\"clean_answer\"] = test.answer.apply(processSingleReview)\n",
    "test[\"clean_question\"] = test.question_body.apply(processSingleReview)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [01:28,  1.08it/s]\n",
      "95it [01:27,  1.09it/s]\n",
      "8it [00:06,  1.17it/s]\n",
      "8it [00:07,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\n",
    "target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n",
    "\n",
    "#train_question_title_dense = fetch_vectors(df_train.question_title.values)\n",
    "train_question_body_dense = fetch_vectors(train.question_body.values)\n",
    "train_answer_dense = fetch_vectors(train.answer.values)\n",
    "\n",
    "#test_question_title_dense = fetch_vectors(df_test.question_title.values)\n",
    "test_question_body_dense = fetch_vectors(test.question_body.values)\n",
    "test_answer_dense = fetch_vectors(test.answer.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'    \n",
    "    ]\n",
    "\n",
    "input_columns = ['question_title', 'question_body', 'answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = re.compile(r\"^[^.]*\")\n",
    "\n",
    "train['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "\n",
    "features = ['netloc', 'category']\n",
    "merged = pd.concat([train[features], test[features]])\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(merged)\n",
    "\n",
    "features_train = ohe.transform(train[features]).toarray()\n",
    "features_test = ohe.transform(test[features]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"../input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title\n",
      "question_body\n",
      "answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in input_columns:\n",
    "    print(text)\n",
    "    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "\n",
    "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
    "\n",
    "dist_features_train = np.array([\n",
    "    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\n",
    "X_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\n",
    "y_train = train[targets].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nxgboost_oof_train = np.load('../input/qa-challenge-external-data/train_oof.npy')\\nxgboost_oof_test = np.load('../input/qa-challenge-external-data/test_pred1.npy')\\n\\ntrain_tm_features = np.load('../input/qa-challenge-external-data/train_features.npy')\\ntest_tm_features = np.load('../input/qa-challenge-external-data/test_features.npy')\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nclf = MinMaxScaler()\\n\\ntrain_tm_features = clf.fit_transform(train_tm_features)\\ntest_tm_features = clf.transform(test_tm_features)\\n\\nprint (xgboost_oof_train.shape, xgboost_oof_test.shape, train_tm_features.shape, test_tm_features.shape)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "xgboost_oof_train = np.load('../input/qa-challenge-external-data/train_oof.npy')\n",
    "xgboost_oof_test = np.load('../input/qa-challenge-external-data/test_pred1.npy')\n",
    "\n",
    "train_tm_features = np.load('../input/qa-challenge-external-data/train_features.npy')\n",
    "test_tm_features = np.load('../input/qa-challenge-external-data/test_features.npy')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "clf = MinMaxScaler()\n",
    "\n",
    "train_tm_features = clf.fit_transform(train_tm_features)\n",
    "test_tm_features = clf.transform(test_tm_features)\n",
    "\n",
    "print (xgboost_oof_train.shape, xgboost_oof_test.shape, train_tm_features.shape, test_tm_features.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, train_question_body_dense, train_answer_dense))\n",
    "X_test = np.hstack((X_test, test_question_body_dense, test_answer_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6079, 3142), (476, 3142))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatible with tensorflow backend\n",
    "class SpearmanRhoCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, model_name):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "            self.model.save_weights(self.model_name)\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inps = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(512, activation='elu')(inps)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='elu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "    model = Model(inputs=inps, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=.0001),\n",
    "        loss=['binary_crossentropy']\n",
    "    )\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3142)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1609216   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 1,748,254\n",
      "Trainable params: 1,748,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 1s 219us/step - loss: 0.4327 - val_loss: 0.3984\n",
      "val_spearman-rho: 0.2825                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 164us/step - loss: 0.4023 - val_loss: 0.3890\n",
      "val_spearman-rho: 0.3264                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3922 - val_loss: 0.3839\n",
      "val_spearman-rho: 0.3442                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3860 - val_loss: 0.3803\n",
      "val_spearman-rho: 0.3549                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3801 - val_loss: 0.3781\n",
      "val_spearman-rho: 0.3635                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3764 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3694                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 166us/step - loss: 0.3724 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.3721                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 153us/step - loss: 0.3701 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3747                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3677 - val_loss: 0.3733\n",
      "val_spearman-rho: 0.3765                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3643 - val_loss: 0.3727\n",
      "val_spearman-rho: 0.3804                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3626 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3606 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3806                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3586 - val_loss: 0.3728\n",
      "val_spearman-rho: 0.3825                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3573 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3831                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3551 - val_loss: 0.3714\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3538 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3834                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3521 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3823                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 161us/step - loss: 0.3507 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3828                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 171us/step - loss: 0.3498 - val_loss: 0.3724\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3475 - val_loss: 0.3728\n",
      "val_spearman-rho: 0.3809                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3463 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3817                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3449 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3445 - val_loss: 0.3747\n",
      "val_spearman-rho: 0.38                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3437 - val_loss: 0.3743\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3794                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3421 - val_loss: 0.3751\n",
      "val_spearman-rho: 0.3797                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 154us/step - loss: 0.3411 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3796                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3408 - val_loss: 0.3746\n",
      "val_spearman-rho: 0.3795                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3399 - val_loss: 0.3747\n",
      "val_spearman-rho: 0.3787                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3397 - val_loss: 0.3753\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "Epoch 00028: early stopping Threshold\n",
      "val_spearman-rho: 0.378                                                                                                    \n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 1s 216us/step - loss: 0.4402 - val_loss: 0.3976\n",
      "val_spearman-rho: 0.2796                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.4048 - val_loss: 0.3888\n",
      "val_spearman-rho: 0.322                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3944 - val_loss: 0.3843\n",
      "val_spearman-rho: 0.3443                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 154us/step - loss: 0.3875 - val_loss: 0.3807\n",
      "val_spearman-rho: 0.3564                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3826 - val_loss: 0.3774\n",
      "val_spearman-rho: 0.366                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3774 - val_loss: 0.3757\n",
      "val_spearman-rho: 0.3733                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3737 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3794                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3707 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 159us/step - loss: 0.3679 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3843                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3657 - val_loss: 0.3715\n",
      "val_spearman-rho: 0.3857                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 141us/step - loss: 0.3631 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3873                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3610 - val_loss: 0.3707\n",
      "val_spearman-rho: 0.3876                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 171us/step - loss: 0.3592 - val_loss: 0.3707\n",
      "val_spearman-rho: 0.39                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 156us/step - loss: 0.3576 - val_loss: 0.3707\n",
      "val_spearman-rho: 0.3884                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3561 - val_loss: 0.3708\n",
      "val_spearman-rho: 0.3888                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 141us/step - loss: 0.3545 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3907                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 140us/step - loss: 0.3529 - val_loss: 0.3720\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3898                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3507 - val_loss: 0.3708\n",
      "val_spearman-rho: 0.392                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3496 - val_loss: 0.3707\n",
      "val_spearman-rho: 0.3904                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 141us/step - loss: 0.3489 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3906                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 175us/step - loss: 0.3479 - val_loss: 0.3707\n",
      "val_spearman-rho: 0.3911                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3467 - val_loss: 0.3718\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3901                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3452 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3908                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3443 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3899                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 165us/step - loss: 0.3436 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3902                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3433 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3899                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3422 - val_loss: 0.3727\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3891                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3417 - val_loss: 0.3726\n",
      "val_spearman-rho: 0.3894                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3410 - val_loss: 0.3734\n",
      "val_spearman-rho: 0.3894                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3407 - val_loss: 0.3726\n",
      "Epoch 00029: early stopping Threshold\n",
      "val_spearman-rho: 0.3887                                                                                                    \n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 1s 274us/step - loss: 0.4388 - val_loss: 0.3979\n",
      "val_spearman-rho: 0.283                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 151us/step - loss: 0.4041 - val_loss: 0.3897\n",
      "val_spearman-rho: 0.3177                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 156us/step - loss: 0.3937 - val_loss: 0.3849\n",
      "val_spearman-rho: 0.336                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 159us/step - loss: 0.3862 - val_loss: 0.3809\n",
      "val_spearman-rho: 0.3499                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 141us/step - loss: 0.3809 - val_loss: 0.3785\n",
      "val_spearman-rho: 0.3586                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 208us/step - loss: 0.3768 - val_loss: 0.3774\n",
      "val_spearman-rho: 0.3648                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3731 - val_loss: 0.3751\n",
      "val_spearman-rho: 0.3694                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3699 - val_loss: 0.3740\n",
      "val_spearman-rho: 0.3737                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 152us/step - loss: 0.3672 - val_loss: 0.3730\n",
      "val_spearman-rho: 0.3774                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3648 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.3778                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 154us/step - loss: 0.3624 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3804                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 168us/step - loss: 0.3602 - val_loss: 0.3727\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 156us/step - loss: 0.3590 - val_loss: 0.3730\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3570 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3829                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 152us/step - loss: 0.3556 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3836                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3538 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 172us/step - loss: 0.3524 - val_loss: 0.3724\n",
      "val_spearman-rho: 0.3817                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3513 - val_loss: 0.3714\n",
      "val_spearman-rho: 0.3827                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3494 - val_loss: 0.3722\n",
      "val_spearman-rho: 0.3814                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3487 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3806                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 140us/step - loss: 0.3474 - val_loss: 0.3731\n",
      "val_spearman-rho: 0.3813                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3460 - val_loss: 0.3737\n",
      "val_spearman-rho: 0.3821                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3453 - val_loss: 0.3735\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.381                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3433 - val_loss: 0.3731\n",
      "val_spearman-rho: 0.3813                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 151us/step - loss: 0.3419 - val_loss: 0.3741\n",
      "val_spearman-rho: 0.3804                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.3409 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3797                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3411 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3784                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 1s 158us/step - loss: 0.3399 - val_loss: 0.3745\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "Epoch 00027: early stopping Threshold\n",
      "val_spearman-rho: 0.3791                                                                                                    \n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 1s 200us/step - loss: 0.4362 - val_loss: 0.3974\n",
      "val_spearman-rho: 0.2721                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 143us/step - loss: 0.4033 - val_loss: 0.3880\n",
      "val_spearman-rho: 0.3149                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3931 - val_loss: 0.3832\n",
      "val_spearman-rho: 0.3367                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3867 - val_loss: 0.3794\n",
      "val_spearman-rho: 0.3521                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3812 - val_loss: 0.3770\n",
      "val_spearman-rho: 0.3612                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3770 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3677                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 144us/step - loss: 0.3732 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3701                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 142us/step - loss: 0.3703 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3737                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3679 - val_loss: 0.3731\n",
      "val_spearman-rho: 0.3777                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3652 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3784                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3634 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3796                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 164us/step - loss: 0.3609 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3792                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3595 - val_loss: 0.3711\n",
      "val_spearman-rho: 0.3818                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 145us/step - loss: 0.3571 - val_loss: 0.3711\n",
      "val_spearman-rho: 0.3819                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 151us/step - loss: 0.3563 - val_loss: 0.3733\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 146us/step - loss: 0.3549 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 150us/step - loss: 0.3534 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3818                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3517 - val_loss: 0.3715\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3837                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 142us/step - loss: 0.3491 - val_loss: 0.3724\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 164us/step - loss: 0.3481 - val_loss: 0.3731\n",
      "val_spearman-rho: 0.3819                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3472 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.3823                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3464 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3453 - val_loss: 0.3719\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3812                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 171us/step - loss: 0.3439 - val_loss: 0.3721\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 161us/step - loss: 0.3433 - val_loss: 0.3721\n",
      "val_spearman-rho: 0.3809                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 148us/step - loss: 0.3427 - val_loss: 0.3730\n",
      "val_spearman-rho: 0.3801                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3423 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3806                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 1s 147us/step - loss: 0.3414 - val_loss: 0.3732\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3797                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 1s 149us/step - loss: 0.3405 - val_loss: 0.3725\n",
      "val_spearman-rho: 0.3802                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 1s 142us/step - loss: 0.3395 - val_loss: 0.3732\n",
      "Epoch 00029: early stopping Threshold\n",
      "val_spearman-rho: 0.3795                                                                                                    \n",
      "Train on 4864 samples, validate on 1215 samples\n",
      "Epoch 1/100\n",
      "4864/4864 [==============================] - 1s 190us/step - loss: 0.4349 - val_loss: 0.3962\n",
      "val_spearman-rho: 0.2759                                                                                                    \n",
      "Epoch 2/100\n",
      "4864/4864 [==============================] - 1s 146us/step - loss: 0.4028 - val_loss: 0.3880\n",
      "val_spearman-rho: 0.3193                                                                                                    \n",
      "Epoch 3/100\n",
      "4864/4864 [==============================] - 1s 140us/step - loss: 0.3931 - val_loss: 0.3835\n",
      "val_spearman-rho: 0.3391                                                                                                    \n",
      "Epoch 4/100\n",
      "4864/4864 [==============================] - 1s 141us/step - loss: 0.3861 - val_loss: 0.3814\n",
      "val_spearman-rho: 0.3532                                                                                                    \n",
      "Epoch 5/100\n",
      "4864/4864 [==============================] - 1s 161us/step - loss: 0.3803 - val_loss: 0.3782\n",
      "val_spearman-rho: 0.3615                                                                                                    \n",
      "Epoch 6/100\n",
      "4864/4864 [==============================] - 1s 142us/step - loss: 0.3760 - val_loss: 0.3766\n",
      "val_spearman-rho: 0.3657                                                                                                    \n",
      "Epoch 7/100\n",
      "4864/4864 [==============================] - 1s 140us/step - loss: 0.3724 - val_loss: 0.3748\n",
      "val_spearman-rho: 0.3715                                                                                                    \n",
      "Epoch 8/100\n",
      "4864/4864 [==============================] - 1s 141us/step - loss: 0.3694 - val_loss: 0.3754\n",
      "val_spearman-rho: 0.3739                                                                                                    \n",
      "Epoch 9/100\n",
      "4864/4864 [==============================] - 1s 139us/step - loss: 0.3665 - val_loss: 0.3738\n",
      "val_spearman-rho: 0.3745                                                                                                    \n",
      "Epoch 10/100\n",
      "4864/4864 [==============================] - 1s 148us/step - loss: 0.3643 - val_loss: 0.3738\n",
      "val_spearman-rho: 0.3758                                                                                                    \n",
      "Epoch 11/100\n",
      "4864/4864 [==============================] - 1s 193us/step - loss: 0.3618 - val_loss: 0.3747\n",
      "val_spearman-rho: 0.3769                                                                                                    \n",
      "Epoch 12/100\n",
      "4864/4864 [==============================] - 1s 167us/step - loss: 0.3600 - val_loss: 0.3736\n",
      "val_spearman-rho: 0.3769                                                                                                    \n",
      "Epoch 13/100\n",
      "4864/4864 [==============================] - 1s 146us/step - loss: 0.3580 - val_loss: 0.3732\n",
      "val_spearman-rho: 0.377                                                                                                    \n",
      "Epoch 14/100\n",
      "4864/4864 [==============================] - 1s 159us/step - loss: 0.3569 - val_loss: 0.3736\n",
      "val_spearman-rho: 0.3759                                                                                                    \n",
      "Epoch 15/100\n",
      "4864/4864 [==============================] - 1s 157us/step - loss: 0.3549 - val_loss: 0.3737\n",
      "val_spearman-rho: 0.3775                                                                                                    \n",
      "Epoch 16/100\n",
      "4864/4864 [==============================] - 1s 142us/step - loss: 0.3530 - val_loss: 0.3760\n",
      "val_spearman-rho: 0.3749                                                                                                    \n",
      "Epoch 17/100\n",
      "4864/4864 [==============================] - 1s 207us/step - loss: 0.3522 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.3778                                                                                                    \n",
      "Epoch 18/100\n",
      "4864/4864 [==============================] - 1s 152us/step - loss: 0.3503 - val_loss: 0.3757\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "val_spearman-rho: 0.3762                                                                                                    \n",
      "Epoch 19/100\n",
      "4864/4864 [==============================] - 1s 153us/step - loss: 0.3479 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3753                                                                                                    \n",
      "Epoch 20/100\n",
      "4864/4864 [==============================] - 1s 148us/step - loss: 0.3470 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.3746                                                                                                    \n",
      "Epoch 21/100\n",
      "4864/4864 [==============================] - 1s 141us/step - loss: 0.3459 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3743                                                                                                    \n",
      "Epoch 22/100\n",
      "4864/4864 [==============================] - 1s 148us/step - loss: 0.3452 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3722                                                                                                    \n",
      "Epoch 23/100\n",
      "4864/4864 [==============================] - 1s 154us/step - loss: 0.3444 - val_loss: 0.3762\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "val_spearman-rho: 0.3729                                                                                                    \n",
      "Epoch 24/100\n",
      "4864/4864 [==============================] - 1s 148us/step - loss: 0.3424 - val_loss: 0.3775\n",
      "val_spearman-rho: 0.3721                                                                                                    \n",
      "Epoch 25/100\n",
      "4864/4864 [==============================] - 1s 147us/step - loss: 0.3418 - val_loss: 0.3757\n",
      "val_spearman-rho: 0.3729                                                                                                    \n",
      "Epoch 26/100\n",
      "4864/4864 [==============================] - 1s 154us/step - loss: 0.3417 - val_loss: 0.3763\n",
      "val_spearman-rho: 0.3722                                                                                                    \n",
      "Epoch 27/100\n",
      "4864/4864 [==============================] - 1s 146us/step - loss: 0.3410 - val_loss: 0.3769\n",
      "val_spearman-rho: 0.3708                                                                                                    \n",
      "Epoch 28/100\n",
      "4864/4864 [==============================] - 1s 142us/step - loss: 0.3402 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.4299996332265434e-05.\n",
      "val_spearman-rho: 0.3708                                                                                                    \n",
      "Epoch 29/100\n",
      "4864/4864 [==============================] - 1s 168us/step - loss: 0.3390 - val_loss: 0.3784\n",
      "val_spearman-rho: 0.3702                                                                                                    \n",
      "Epoch 30/100\n",
      "4864/4864 [==============================] - 1s 150us/step - loss: 0.3386 - val_loss: 0.3774\n",
      "Epoch 00029: early stopping Threshold\n",
      "val_spearman-rho: 0.3693                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "all_predictions1 = np.zeros((n_splits,X_test.shape[0],y_train.shape[1]))\n",
    "oof_pred1 = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "for ind, (tr, val) in enumerate(kf.split(X_train)):\n",
    "    X_tr = X_train[tr]\n",
    "    y_tr = y_train[tr]\n",
    "    X_vl = X_train[val]\n",
    "    y_vl = y_train[val]\n",
    "    \n",
    "    model = create_model()\n",
    "    early = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min', baseline=None, restore_best_weights=False)\n",
    "    lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, verbose=1, mode='min', min_lr=0.000001)\n",
    "    rho = SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n",
    "                                       patience=15, model_name='weights_simple_lstm_deep_cnn_{}.hdf5'.format(ind))\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n",
    "        callbacks=[lr,rho]\n",
    "    )\n",
    "    model.load_weights('weights_simple_lstm_deep_cnn_{}.hdf5'.format(ind))\n",
    "    \n",
    "    oof_pred1[val,:] = model.predict(X_vl)\n",
    "    all_predictions1[ind,:,:] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:1803: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.49911147169041, tolerance: 0.8890928173433562\n",
      "  check_random_state(self.random_state), random)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:1803: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 75.4950920255269, tolerance: 0.8902148281023502\n",
      "  check_random_state(self.random_state), random)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:1803: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.44062210144875, tolerance: 0.898013530452929\n",
      "  check_random_state(self.random_state), random)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:1803: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.96183073001839, tolerance: 0.8963124509333517\n",
      "  check_random_state(self.random_state), random)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:1803: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.03579676706886, tolerance: 0.8919041259695826\n",
      "  check_random_state(self.random_state), random)\n"
     ]
    }
   ],
   "source": [
    "all_predictions2 = np.zeros((n_splits,X_test.shape[0],y_train.shape[1]))\n",
    "oof_pred2 = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "kf = KFold(n_splits=n_splits, random_state=2019, shuffle=True)\n",
    "for ind, (tr, val) in enumerate(kf.split(X_train)):\n",
    "    X_tr = X_train[tr]\n",
    "    y_tr = y_train[tr]\n",
    "    X_vl = X_train[val]\n",
    "    y_vl = y_train[val]\n",
    "    \n",
    "    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    oof_pred2[val,:] = model.predict(X_vl)\n",
    "    all_predictions2[ind,:,:] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred1 = np.clip(oof_pred1,0.0001,.9999)\n",
    "all_predictions1 = np.clip(all_predictions1,0.0001,.9999)\n",
    "\n",
    "oof_pred2 = np.clip(oof_pred2,0.0001,.9999)\n",
    "all_predictions2 = np.clip(all_predictions2,0.0001,.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding 0.34972623301942457 0.3482761226428539\n",
      "question_body_critical 0.6376436045798501 0.6437699887886108\n",
      "question_conversational 0.4097644461617697 0.3941330399442278\n",
      "question_expect_short_answer 0.2379809075593805 0.23151784332480524\n",
      "question_fact_seeking 0.32328066280178136 0.30825234359601855\n",
      "question_has_commonly_accepted_answer 0.4072594053576393 0.40199360352868513\n",
      "question_interestingness_others 0.35454104233421185 0.3405281563338947\n",
      "question_interestingness_self 0.49407717654471267 0.5008552750827329\n",
      "question_multi_intent 0.46332534315475044 0.4706399256903671\n",
      "question_not_really_a_question 0.051592891864929914 0.04311265997144773\n",
      "question_opinion_seeking 0.42482026832061365 0.42997733255833387\n",
      "question_type_choice 0.6310866042582306 0.6518507337954111\n",
      "question_type_compare 0.33566595535197596 0.3215648806679264\n",
      "question_type_consequence 0.15700745480374584 0.14804115312813232\n",
      "question_type_definition 0.34995736465199473 0.3555698915298218\n",
      "question_type_entity 0.41776757666871606 0.4252497779080297\n",
      "question_type_instructions 0.7584064432487492 0.7578946425179431\n",
      "question_type_procedure 0.3047342331445923 0.29423606075715275\n",
      "question_type_reason_explanation 0.6024938703082301 0.615942847200684\n",
      "question_type_spelling 0.06014830240740127 0.06804597603222358\n",
      "question_well_written 0.5072353201265212 0.5029438121173344\n",
      "answer_helpful 0.22354964868654742 0.20639941296912725\n",
      "answer_level_of_information 0.39634344967478086 0.4069206234713002\n",
      "answer_plausible 0.14164383048574114 0.08847346384509318\n",
      "answer_relevance 0.15083842757666144 0.12072917732026596\n",
      "answer_satisfaction 0.3065851086524255 0.3006633936203477\n",
      "answer_type_instructions 0.7561539903933373 0.7570186774912276\n",
      "answer_type_procedure 0.26593520527403725 0.25739183012309336\n",
      "answer_type_reason_explanation 0.6496558296995112 0.6539003091833245\n",
      "answer_well_written 0.18868599785450618 0.19304187765776823\n",
      "Avg scores 0.3785968864988923, 0.3746311610932728\n"
     ]
    }
   ],
   "source": [
    "score1 = 0\n",
    "score2 = 0\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    score1 += spearmanr(y_train[:,i],oof_pred1[:,i]).correlation\n",
    "    score2+= spearmanr(y_train[:,i],oof_pred2[:,i]).correlation\n",
    "    print (val,spearmanr(y_train[:,i],oof_pred1[:,i]).correlation,spearmanr(y_train[:,i],oof_pred2[:,i]).correlation)\n",
    "\n",
    "print (\"Avg scores {}, {}\".format(score1/30, score2/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = create_model()\\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\\nall_predictions.append(model.predict(X_test))\\n    \\nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\\nmodel.fit(X_train, y_train)\\nall_predictions.append(model.predict(X_test))\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\n",
    "all_predictions.append(model.predict(X_test))\n",
    "    \n",
    "model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "all_predictions.append(model.predict(X_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.2        0.26666667 0.3        0.33333333 0.33333333\n",
      " 0.4        0.44444444 0.46666667 0.5        0.53333333 0.55555556\n",
      " 0.6        0.66666667 0.66666667 0.7        0.73333333 0.77777778\n",
      " 0.8        0.83333333 0.86666667 0.88888889 0.9        0.93333333\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "uniq_numbers = np.unique(y_train.flatten())\n",
    "print (uniq_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounder(values):\n",
    "    def f(x):\n",
    "        idx = np.argmin(np.abs(values - x))\n",
    "        return values[idx]\n",
    "    return np.frompyfunc(f, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_oof_pred1 = np.array([rounder(uniq_numbers)(i) for i in oof_pred1])\n",
    "rounded_oof_pred1[:,9] = oof_pred1[:,9]\n",
    "rounded_oof_pred2 = np.array([rounder(uniq_numbers)(i) for i in oof_pred2])\n",
    "rounded_oof_pred2[:,9] = oof_pred2[:,9]\n",
    "\n",
    "rounded_oof_pred1 = np.clip(rounded_oof_pred1,.0001,.9999)\n",
    "rounded_oof_pred2 = np.clip(rounded_oof_pred2,.0001,.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding 0.333226791873266 0.3431997074745129\n",
      "question_body_critical 0.6368451058559078 0.6433186682145369\n",
      "question_conversational 0.4766927398293525 0.45552084272806087\n",
      "question_expect_short_answer 0.23906069406416539 0.23069050762169727\n",
      "question_fact_seeking 0.32207221662493296 0.309243986437451\n",
      "question_has_commonly_accepted_answer 0.4115795211369651 0.40266776200674165\n",
      "question_interestingness_others 0.34694079751419543 0.3376264665053459\n",
      "question_interestingness_self 0.4944770135639902 0.5004143758950514\n",
      "question_multi_intent 0.4563973882735221 0.46589928705007755\n",
      "question_not_really_a_question 0.051592891864929914 0.04311265997144773\n",
      "question_opinion_seeking 0.42342139205139884 0.4281399847433677\n",
      "question_type_choice 0.6308801152968414 0.6511438645745767\n",
      "question_type_compare 0.4358450447662802 0.4157164817383669\n",
      "question_type_consequence 0.18575466380241445 0.1832892651520298\n",
      "question_type_definition 0.616461775935941 0.5776756311509881\n",
      "question_type_entity 0.5205427003006051 0.48128955284271185\n",
      "question_type_instructions 0.7582162255113667 0.758305659662685\n",
      "question_type_procedure 0.289593708152716 0.27256421541267867\n",
      "question_type_reason_explanation 0.6014900896820012 0.6138309053260865\n",
      "question_type_spelling 0.30106450177380784 0.4254353035625318\n",
      "question_well_written 0.5051611830795928 0.4996478816219552\n",
      "answer_helpful 0.21079513523487298 0.20023657836140243\n",
      "answer_level_of_information 0.3885626059418558 0.40118493741716765\n",
      "answer_plausible 0.1251172517009632 0.06871338993651413\n",
      "answer_relevance 0.13903644046579988 0.11348410217685537\n",
      "answer_satisfaction 0.2998668437621302 0.29628581824943734\n",
      "answer_type_instructions 0.7564238287273589 0.7577266525243849\n",
      "answer_type_procedure 0.2400782649273225 0.23253175535550213\n",
      "answer_type_reason_explanation 0.6473385390165448 0.6527399871518862\n",
      "answer_well_written 0.18165606569926954 0.18919868127417924\n",
      "Avg scores 0.40087305121434375, 0.3983611637380076\n"
     ]
    }
   ],
   "source": [
    "score1 = 0\n",
    "score2 = 0\n",
    "\n",
    "for i, val in enumerate(targets):\n",
    "    score1 += spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation\n",
    "    score2+= spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation\n",
    "    print (val,spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation,spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation)\n",
    "\n",
    "print (\"Avg scores {}, {}\".format(score1/30, score2/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1 = all_predictions1.mean(axis=0)\n",
    "test_pred2 = all_predictions2.mean(axis=0)\n",
    "main_pred = np.zeros((test_pred1.shape[0],test_pred1.shape[1]))\n",
    "\n",
    "for i in range(test_pred1.shape[1]):\n",
    "    if spearmanr(y_train[:,i],rounded_oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],rounded_oof_pred2[:,i]).correlation:\n",
    "        main_pred[:,i] = rounder(uniq_numbers)(test_pred1[:,i])\n",
    "    else:\n",
    "        main_pred[:,i] = rounder(uniq_numbers)(test_pred2[:,i])\n",
    "        \n",
    "for i in range(main_pred.shape[1]):\n",
    "    if main_pred[:,i].sum() == 0:\n",
    "        if spearmanr(y_train[:,i],oof_pred1[:,i]).correlation > spearmanr(y_train[:,i],oof_pred2[:,i]).correlation:\n",
    "            main_pred[:,i] = test_pred1[:,i]\n",
    "        else:\n",
    "            main_pred[:,i] = test_pred2[:,i]\n",
    "            \n",
    "main_pred = np.clip(main_pred,0.0001,0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5426.500000000001 420.9414444444445\n",
      "1 3618.833333333333 275.4222222222222\n",
      "2 348.3333333333333 12.2426\n",
      "3 4246.333333333333 345.4555555555555\n",
      "4 4696.833333333333 374.5220222222222\n",
      "5 4824.833333333334 403.8206222222223\n",
      "6 3571.277777777778 271.47777777777776\n",
      "7 3083.722222222222 231.36666666666667\n",
      "8 1451.3333333333333 112.14303333333334\n",
      "9 27.166666666666664 2.872566092014313\n",
      "10 2613.833333333333 190.6904888888889\n",
      "11 1732.0 137.2752666666667\n",
      "12 231.83333333333331 6.278233333333333\n",
      "13 61.0 0.6472999999999998\n",
      "14 187.0 4.034888888888888\n",
      "15 396.5 13.309466666666667\n",
      "16 3024.833333333333 258.08197777777775\n",
      "17 1009.5 75.01320000000001\n",
      "18 2348.833333333333 183.54934444444444\n",
      "19 5.0 1.2398295919683018\n",
      "20 4862.777777777777 373.22222222222223\n",
      "21 5625.555555555556 441.31812222222226\n",
      "22 3980.666666666667 312.7222222222222\n",
      "23 5836.166666666666 458.10052222222225\n",
      "24 5888.277777777779 467.05408888888894\n",
      "25 5195.6 410.7666666666667\n",
      "26 2915.166666666667 248.73703333333336\n",
      "27 794.1666666666665 63.528511111111115\n",
      "28 3054.5 240.05675555555558\n",
      "29 5521.277777777778 430.27677777777785\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print (i, y_train[:,i].sum(), main_pred[:,i].sum()) #main_pred[:,i].max(), main_pred[:,i].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\n",
    "submission[targets] = main_pred\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.888889                0.600000   \n",
       "1     46                             0.866667                0.555556   \n",
       "2     70                             0.866667                0.600000   \n",
       "3    132                             0.833333                0.333333   \n",
       "4    200                             0.999900                0.600000   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                   0.0001                      0.733333   \n",
       "1                   0.0001                      0.666667   \n",
       "2                   0.0001                      0.733333   \n",
       "3                   0.0001                      0.666667   \n",
       "4                   0.0001                      0.933333   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.800000                               0.833333   \n",
       "1               0.833333                               0.933333   \n",
       "2               0.900000                               0.999900   \n",
       "3               0.800000                               0.900000   \n",
       "4               0.833333                               0.933333   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.666667                       0.600000   \n",
       "1                         0.533333                       0.444444   \n",
       "2                         0.555556                       0.400000   \n",
       "3                         0.555556                       0.444444   \n",
       "4                         0.666667                       0.600000   \n",
       "\n",
       "   question_multi_intent  question_not_really_a_question  \\\n",
       "0               0.400000                        0.006864   \n",
       "1               0.000100                        0.007136   \n",
       "2               0.200000                        0.006114   \n",
       "3               0.333333                        0.008391   \n",
       "4               0.333333                        0.005043   \n",
       "\n",
       "   question_opinion_seeking  question_type_choice  question_type_compare  \\\n",
       "0                  0.466667              0.600000                 0.0001   \n",
       "1                  0.466667              0.266667                 0.0001   \n",
       "2                  0.200000              0.444444                 0.0001   \n",
       "3                  0.400000              0.400000                 0.0001   \n",
       "4                  0.400000              0.555556                 0.0001   \n",
       "\n",
       "   question_type_consequence  question_type_definition  question_type_entity  \\\n",
       "0                     0.0001                    0.0001                0.0001   \n",
       "1                     0.0001                    0.0001                0.0001   \n",
       "2                     0.0001                    0.0001                0.0001   \n",
       "3                     0.0001                    0.0001                0.0001   \n",
       "4                     0.0001                    0.0001                0.0001   \n",
       "\n",
       "   question_type_instructions  question_type_procedure  \\\n",
       "0                    0.200000                   0.0001   \n",
       "1                    0.900000                   0.2000   \n",
       "2                    0.333333                   0.0001   \n",
       "3                    0.600000                   0.2000   \n",
       "4                    0.200000                   0.2000   \n",
       "\n",
       "   question_type_reason_explanation  question_type_spelling  \\\n",
       "0                          0.500000                0.000100   \n",
       "1                          0.200000                0.000555   \n",
       "2                          0.600000                0.004068   \n",
       "3                          0.666667                0.001043   \n",
       "4                          0.400000                0.007875   \n",
       "\n",
       "   question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0               0.866667        0.900000                     0.600000   \n",
       "1               0.600000        0.933333                     0.600000   \n",
       "2               0.866667        0.933333                     0.600000   \n",
       "3               0.700000        0.933333                     0.700000   \n",
       "4               0.833333        0.933333                     0.666667   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0          0.933333          0.933333             0.833333   \n",
       "1          0.933333          0.999900             0.833333   \n",
       "2          0.999900          0.933333             0.800000   \n",
       "3          0.999900          0.999900             0.900000   \n",
       "4          0.999900          0.999900             0.866667   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                  0.000100                 0.0001   \n",
       "1                  0.999900                 0.2000   \n",
       "2                  0.200000                 0.0001   \n",
       "3                  0.700000                 0.2000   \n",
       "4                  0.266667                 0.2000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.833333             0.933333  \n",
       "1                        0.000100             0.866667  \n",
       "2                        0.800000             0.900000  \n",
       "3                        0.666667             0.866667  \n",
       "4                        0.555556             0.933333  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5029.186975</td>\n",
       "      <td>0.884331</td>\n",
       "      <td>0.578618</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.725747</td>\n",
       "      <td>0.786811</td>\n",
       "      <td>0.848363</td>\n",
       "      <td>0.570331</td>\n",
       "      <td>0.486064</td>\n",
       "      <td>0.235595</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.400610</td>\n",
       "      <td>0.288393</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.008477</td>\n",
       "      <td>0.027961</td>\n",
       "      <td>0.542189</td>\n",
       "      <td>0.157591</td>\n",
       "      <td>0.385608</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.784080</td>\n",
       "      <td>0.927139</td>\n",
       "      <td>0.656979</td>\n",
       "      <td>0.962396</td>\n",
       "      <td>0.981206</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>0.522557</td>\n",
       "      <td>0.133463</td>\n",
       "      <td>0.504321</td>\n",
       "      <td>0.903943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2812.670060</td>\n",
       "      <td>0.051376</td>\n",
       "      <td>0.132950</td>\n",
       "      <td>0.080119</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.114598</td>\n",
       "      <td>0.131663</td>\n",
       "      <td>0.054579</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.154696</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.159719</td>\n",
       "      <td>0.218487</td>\n",
       "      <td>0.055703</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.048846</td>\n",
       "      <td>0.093466</td>\n",
       "      <td>0.289485</td>\n",
       "      <td>0.102264</td>\n",
       "      <td>0.235498</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.092532</td>\n",
       "      <td>0.034610</td>\n",
       "      <td>0.054508</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.049591</td>\n",
       "      <td>0.299874</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.252459</td>\n",
       "      <td>0.033715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2572.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5093.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7482.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9640.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.041476</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             qa_id  question_asker_intent_understanding  \\\n",
       "count   476.000000                           476.000000   \n",
       "mean   5029.186975                             0.884331   \n",
       "std    2812.670060                             0.051376   \n",
       "min      39.000000                             0.700000   \n",
       "25%    2572.000000                             0.833333   \n",
       "50%    5093.000000                             0.888889   \n",
       "75%    7482.000000                             0.933333   \n",
       "max    9640.000000                             0.999900   \n",
       "\n",
       "       question_body_critical  question_conversational  \\\n",
       "count              476.000000               476.000000   \n",
       "mean                 0.578618                 0.025720   \n",
       "std                  0.132950                 0.080119   \n",
       "min                  0.200000                 0.000100   \n",
       "25%                  0.500000                 0.000100   \n",
       "50%                  0.555556                 0.000100   \n",
       "75%                  0.666667                 0.000100   \n",
       "max                  0.900000                 0.555556   \n",
       "\n",
       "       question_expect_short_answer  question_fact_seeking  \\\n",
       "count                    476.000000             476.000000   \n",
       "mean                       0.725747               0.786811   \n",
       "std                        0.112030               0.114598   \n",
       "min                        0.266667               0.266667   \n",
       "25%                        0.666667               0.733333   \n",
       "50%                        0.733333               0.800000   \n",
       "75%                        0.800000               0.866667   \n",
       "max                        0.933333               0.999900   \n",
       "\n",
       "       question_has_commonly_accepted_answer  question_interestingness_others  \\\n",
       "count                             476.000000                       476.000000   \n",
       "mean                                0.848363                         0.570331   \n",
       "std                                 0.131663                         0.054579   \n",
       "min                                 0.200000                         0.466667   \n",
       "25%                                 0.833333                         0.533333   \n",
       "50%                                 0.900000                         0.555556   \n",
       "75%                                 0.933333                         0.600000   \n",
       "max                                 0.999900                         0.733333   \n",
       "\n",
       "       question_interestingness_self  question_multi_intent  \\\n",
       "count                     476.000000             476.000000   \n",
       "mean                        0.486064               0.235595   \n",
       "std                         0.094727               0.154696   \n",
       "min                         0.333333               0.000100   \n",
       "25%                         0.400000               0.200000   \n",
       "50%                         0.466667               0.200000   \n",
       "75%                         0.533333               0.333333   \n",
       "max                         0.777778               0.733333   \n",
       "\n",
       "       question_not_really_a_question  question_opinion_seeking  \\\n",
       "count                      476.000000                476.000000   \n",
       "mean                         0.006035                  0.400610   \n",
       "std                          0.003917                  0.159719   \n",
       "min                          0.001330                  0.000100   \n",
       "25%                          0.003631                  0.300000   \n",
       "50%                          0.004988                  0.400000   \n",
       "75%                          0.007145                  0.500000   \n",
       "max                          0.041476                  0.933333   \n",
       "\n",
       "       question_type_choice  question_type_compare  question_type_consequence  \\\n",
       "count            476.000000             476.000000                 476.000000   \n",
       "mean               0.288393               0.013190                   0.001360   \n",
       "std                0.218487               0.055703                   0.015836   \n",
       "min                0.000100               0.000100                   0.000100   \n",
       "25%                0.200000               0.000100                   0.000100   \n",
       "50%                0.266667               0.000100                   0.000100   \n",
       "75%                0.400000               0.000100                   0.000100   \n",
       "max                0.999900               0.500000                   0.200000   \n",
       "\n",
       "       question_type_definition  question_type_entity  \\\n",
       "count                476.000000            476.000000   \n",
       "mean                   0.008477              0.027961   \n",
       "std                    0.048846              0.093466   \n",
       "min                    0.000100              0.000100   \n",
       "25%                    0.000100              0.000100   \n",
       "50%                    0.000100              0.000100   \n",
       "75%                    0.000100              0.000100   \n",
       "max                    0.555556              0.700000   \n",
       "\n",
       "       question_type_instructions  question_type_procedure  \\\n",
       "count                  476.000000               476.000000   \n",
       "mean                     0.542189                 0.157591   \n",
       "std                      0.289485                 0.102264   \n",
       "min                      0.000100                 0.000100   \n",
       "25%                      0.300000                 0.000100   \n",
       "50%                      0.600000                 0.200000   \n",
       "75%                      0.777778                 0.200000   \n",
       "max                      0.999900                 0.333333   \n",
       "\n",
       "       question_type_reason_explanation  question_type_spelling  \\\n",
       "count                        476.000000              476.000000   \n",
       "mean                           0.385608                0.002605   \n",
       "std                            0.235498                0.003555   \n",
       "min                            0.000100                0.000100   \n",
       "25%                            0.200000                0.000100   \n",
       "50%                            0.333333                0.001051   \n",
       "75%                            0.533333                0.004013   \n",
       "max                            0.999900                0.025882   \n",
       "\n",
       "       question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "count             476.000000      476.000000                   476.000000   \n",
       "mean                0.784080        0.927139                     0.656979   \n",
       "std                 0.092532        0.034610                     0.054508   \n",
       "min                 0.500000        0.733333                     0.500000   \n",
       "25%                 0.700000        0.933333                     0.600000   \n",
       "50%                 0.800000        0.933333                     0.666667   \n",
       "75%                 0.866667        0.933333                     0.700000   \n",
       "max                 0.933333        0.999900                     0.833333   \n",
       "\n",
       "       answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "count        476.000000        476.000000           476.000000   \n",
       "mean           0.962396          0.981206             0.862955   \n",
       "std            0.035354          0.031486             0.049591   \n",
       "min            0.833333          0.866667             0.600000   \n",
       "25%            0.933333          0.933333             0.833333   \n",
       "50%            0.933333          0.999900             0.866667   \n",
       "75%            0.999900          0.999900             0.900000   \n",
       "max            0.999900          0.999900             0.933333   \n",
       "\n",
       "       answer_type_instructions  answer_type_procedure  \\\n",
       "count                476.000000             476.000000   \n",
       "mean                   0.522557               0.133463   \n",
       "std                    0.299874               0.103900   \n",
       "min                    0.000100               0.000100   \n",
       "25%                    0.266667               0.000100   \n",
       "50%                    0.555556               0.200000   \n",
       "75%                    0.777778               0.200000   \n",
       "max                    0.999900               0.444444   \n",
       "\n",
       "       answer_type_reason_explanation  answer_well_written  \n",
       "count                      476.000000           476.000000  \n",
       "mean                         0.504321             0.903943  \n",
       "std                          0.252459             0.033715  \n",
       "min                          0.000100             0.777778  \n",
       "25%                          0.333333             0.888889  \n",
       "50%                          0.500000             0.900000  \n",
       "75%                          0.666667             0.933333  \n",
       "max                          0.999900             0.999900  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
