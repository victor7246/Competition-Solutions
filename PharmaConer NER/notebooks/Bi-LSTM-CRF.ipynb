{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forked from https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os, csv, math, codecs\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading\n",
    "\n",
    "Read training, dev and validation data. Dataset are in below format\n",
    "\n",
    "document id | sentence number | word | NER tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/subtask1_train.csv\")\n",
    "val = pd.read_csv(\"../data/subtask1_dev.csv\")\n",
    "test = pd.read_csv('../data/subtask1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['doc_sent'] = train.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "val['doc_sent'] = val.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "test['doc_sent'] = test.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.tag != 'NULL']\n",
    "val = val[val.tag != 'NULL']\n",
    "train = train[pd.notnull(train.tag)]\n",
    "val = val[pd.notnull(val.tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97244, 6) (49051, 8)\n",
      "(49051, 8)\n",
      "(872149, 7)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print (train.shape, val.shape)\n",
    "#train['word'] = train['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#train['wordlen'] = train.word.apply(lambda x: len(x))\n",
    "#train = train[train.wordlen >= 2]\n",
    "#print (train.shape)\n",
    "#val['word'] = val['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#val['wordlen'] = val.word.apply(lambda x: len(x))\n",
    "#val = val[val.wordlen >= 2]\n",
    "print (val.shape)\n",
    "test.replace(np.nan,'',inplace=True)\n",
    "#test['word'] = test['word'].apply(lambda x: x.lower())\n",
    "#test['word'] = test['word'].apply(lambda x: re.sub(r'[^\\w]','',str(x)))\n",
    "#test['wordlen'] = test.word.apply(lambda x: len(x))\n",
    "#test = test[test.wordlen >= 2]\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>caso</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>mujer</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence         word    tag   pos  \\\n",
       "0  S0004-06142008000100008-1         0  presentamos  OTHER  VERB   \n",
       "1  S0004-06142008000100008-1         0         caso  OTHER  NOUN   \n",
       "2  S0004-06142008000100008-1         0        mujer  OTHER  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>caso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>mujer</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>NUM</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>años</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence  n1  n2         word   pos  \\\n",
       "0  S0004-06142008000100008-1         0   0  11  presentamos  VERB   \n",
       "1  S0004-06142008000100008-1         0  15  19         caso  NOUN   \n",
       "2  S0004-06142008000100008-1         0  27  32        mujer  NOUN   \n",
       "3  S0004-06142008000100008-1         0  36  38           30   NUM   \n",
       "4  S0004-06142008000100008-1         0  39  43         años  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  \n",
       "3  S0004-06142008000100008-1_0  \n",
       "4  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test = pd.merge(test,pd.concat([train,val],axis=0),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER               47127\n",
       "NORMALIZABLES        1048\n",
       "PROTEINAS             836\n",
       "UNCLEAR                27\n",
       "NO_NORMALIZABLES       13\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49051 872149\n"
     ]
    }
   ],
   "source": [
    "print (len(test[pd.notnull(test.tag)]), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2315.000000\n",
       "mean       38.097624\n",
       "std        33.411630\n",
       "min         1.000000\n",
       "25%        16.000000\n",
       "50%        29.000000\n",
       "75%        50.000000\n",
       "max       359.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19048.000000\n",
       "mean        41.185426\n",
       "std         37.742500\n",
       "min          1.000000\n",
       "25%         17.000000\n",
       "50%         31.000000\n",
       "75%         53.000000\n",
       "max        454.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1086.000000\n",
       "mean       40.960405\n",
       "std        34.282256\n",
       "min         1.000000\n",
       "25%        18.000000\n",
       "50%        32.000000\n",
       "75%        52.000000\n",
       "max       264.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80159 5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "MAX_LEN = 300\n",
    "EMBEDDING = 300\n",
    "MAX_NB_WORDS = 80000\n",
    "n_tags = train.tag.nunique()\n",
    "words = list(set(test.word))\n",
    "n_words = len(words)\n",
    "print (n_words, n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, BatchNormalization, GRU, CuDNNLSTM, CuDNNGRU\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing for LSTM model\n",
    "\n",
    "Convert the input sentences into sequence of words with maximum length as 300. For outputs we one hot encode. Additionally, we add 'PAD' to shorter input texts, as well as in the outputs for TAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = list(set(train.word))\n",
    "tags = list(set(train.tag))\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315, 300) (1086, 300) (2315, 300, 6) (1086, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "# Convert each sentence from list of Token to list of word_index\n",
    "trainX = [[word2idx[w] for w in list(train[train.doc_sent == s].word)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainX = pad_sequences(maxlen=MAX_LEN, sequences=trainX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valX = [[word2idx.get(w,1) for w in list(val[val.doc_sent == s].word)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valX = pad_sequences(maxlen=MAX_LEN, sequences=valX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "trainy = [[tag2idx[w] for w in list(train[train.doc_sent == s].tag)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainy = pad_sequences(maxlen=MAX_LEN, sequences=trainy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valy = [[tag2idx[w] for w in list(val[val.doc_sent == s].tag)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valy = pad_sequences(maxlen=MAX_LEN, sequences=valy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# One-Hot encode\n",
    "trainy = [to_categorical(i, num_classes=n_tags+1) for i in trainy]  # n_tags+1(PAD)\n",
    "valy = [to_categorical(i, num_classes=n_tags+1) for i in valy]  # n_tags+1(PAD)\n",
    "\n",
    "print (np.array(trainX).shape, np.array(valX).shape, np.array(trainy).shape, np.array(valy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [00:00, 9594.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:50, 11727.05it/s]\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/Users/victor/Documents/Models/cc.es.300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "42579\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "EMBEDDING = 300\n",
    "words_not_found = []\n",
    "embedding_matrix = np.random.uniform(low=-.25,high=.25,size=(n_words+2, EMBEDDING))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print (len(words_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 300, 300)          24048300  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 300, 400)          801600    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 300, 50)           20050     \n",
      "_________________________________________________________________\n",
      "crf_3 (CRF)                  (None, 300, 6)            354       \n",
      "=================================================================\n",
      "Total params: 24,870,304\n",
      "Trainable params: 24,870,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=200, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/15\n",
      " - 132s - loss: 32.5977 - crf_viterbi_accuracy: 0.8750 - val_loss: 26.2727 - val_crf_viterbi_accuracy: 0.9605\n",
      "Epoch 2/15\n",
      " - 130s - loss: 32.2128 - crf_viterbi_accuracy: 0.9641 - val_loss: 26.1917 - val_crf_viterbi_accuracy: 0.9712\n",
      "Epoch 3/15\n",
      " - 123s - loss: 32.1252 - crf_viterbi_accuracy: 0.9799 - val_loss: 26.1837 - val_crf_viterbi_accuracy: 0.9791\n",
      "Epoch 4/15\n",
      " - 123s - loss: 32.1005 - crf_viterbi_accuracy: 0.9918 - val_loss: 26.1736 - val_crf_viterbi_accuracy: 0.9839\n",
      "Epoch 5/15\n",
      " - 123s - loss: 32.0876 - crf_viterbi_accuracy: 0.9963 - val_loss: 26.1796 - val_crf_viterbi_accuracy: 0.9855\n",
      "Epoch 6/15\n",
      " - 123s - loss: 32.0834 - crf_viterbi_accuracy: 0.9969 - val_loss: 26.1778 - val_crf_viterbi_accuracy: 0.9859\n",
      "Epoch 7/15\n",
      " - 151s - loss: 32.0810 - crf_viterbi_accuracy: 0.9982 - val_loss: 26.1980 - val_crf_viterbi_accuracy: 0.9855\n",
      "Epoch 8/15\n",
      " - 125s - loss: 32.0796 - crf_viterbi_accuracy: 0.9986 - val_loss: 26.1882 - val_crf_viterbi_accuracy: 0.9865\n",
      "Epoch 9/15\n",
      " - 122s - loss: 32.0784 - crf_viterbi_accuracy: 0.9991 - val_loss: 26.1908 - val_crf_viterbi_accuracy: 0.9863\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit(np.array(trainX), np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=(np.array(valX),np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_withoutfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_withoutfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(valX)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.95      0.70      0.81      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      1.00      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.88      0.73      0.79       836\n",
      "         UNCLEAR       0.80      0.30      0.43        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.77      0.62      0.67    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_withoutfasttext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 300, 300)          24048300  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 300, 400)          801600    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 300, 50)           20050     \n",
      "_________________________________________________________________\n",
      "crf_4 (CRF)                  (None, 300, 6)            354       \n",
      "=================================================================\n",
      "Total params: 24,870,304\n",
      "Trainable params: 24,870,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING,weights=[embedding_matrix],trainable=True, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=200, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/15\n",
      " - 127s - loss: 32.3987 - crf_viterbi_accuracy: 0.9106 - val_loss: 26.2271 - val_crf_viterbi_accuracy: 0.9617\n",
      "Epoch 2/15\n",
      " - 122s - loss: 32.1623 - crf_viterbi_accuracy: 0.9715 - val_loss: 26.1740 - val_crf_viterbi_accuracy: 0.9786\n",
      "Epoch 3/15\n",
      " - 122s - loss: 32.1059 - crf_viterbi_accuracy: 0.9901 - val_loss: 26.1664 - val_crf_viterbi_accuracy: 0.9839\n",
      "Epoch 4/15\n",
      " - 122s - loss: 32.0876 - crf_viterbi_accuracy: 0.9957 - val_loss: 26.1661 - val_crf_viterbi_accuracy: 0.9847\n",
      "Epoch 5/15\n",
      " - 122s - loss: 32.0820 - crf_viterbi_accuracy: 0.9975 - val_loss: 26.1702 - val_crf_viterbi_accuracy: 0.9854\n",
      "Epoch 6/15\n",
      " - 339s - loss: 32.0792 - crf_viterbi_accuracy: 0.9988 - val_loss: 26.1747 - val_crf_viterbi_accuracy: 0.9855\n",
      "Epoch 7/15\n",
      " - 122s - loss: 32.0789 - crf_viterbi_accuracy: 0.9988 - val_loss: 26.1825 - val_crf_viterbi_accuracy: 0.9854\n",
      "Epoch 8/15\n",
      " - 122s - loss: 32.0774 - crf_viterbi_accuracy: 0.9994 - val_loss: 26.1843 - val_crf_viterbi_accuracy: 0.9850\n",
      "Epoch 9/15\n",
      " - 122s - loss: 32.0767 - crf_viterbi_accuracy: 0.9996 - val_loss: 26.1863 - val_crf_viterbi_accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit(np.array(trainX), np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=(np.array(valX),np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_withfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_withfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(valX)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.93      0.70      0.80      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      1.00      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.82      0.76      0.79       836\n",
      "         UNCLEAR       0.71      0.37      0.49        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.74      0.64      0.68    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:15<00:00, 69.33it/s]\n"
     ]
    }
   ],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_withfasttext.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          24048300  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 400)          801600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 300, 50)           20050     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 300, 6)            354       \n",
      "=================================================================\n",
      "Total params: 24,870,304\n",
      "Trainable params: 24,870,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING,weights=[embedding_matrix],trainable=True, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=200, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/model_bilstm_withfasttext.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3401, 300) (3401, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "train_valX = np.concatenate([np.array(trainX),np.array(valX)],axis=0)\n",
    "train_valy = np.concatenate([np.array(trainy),np.array(valy)],axis=0)\n",
    "\n",
    "print (train_valX.shape, train_valy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3060 samples, validate on 341 samples\n",
      "Epoch 1/15\n",
      " - 131s - loss: 31.3312 - crf_viterbi_accuracy: 0.8797 - val_loss: 26.0546 - val_crf_viterbi_accuracy: 0.9648\n",
      "Epoch 2/15\n",
      " - 125s - loss: 30.7561 - crf_viterbi_accuracy: 0.9680 - val_loss: 25.9855 - val_crf_viterbi_accuracy: 0.9774\n",
      "Epoch 3/15\n",
      " - 124s - loss: 30.6855 - crf_viterbi_accuracy: 0.9861 - val_loss: 25.9672 - val_crf_viterbi_accuracy: 0.9839\n",
      "Epoch 4/15\n",
      " - 124s - loss: 30.6582 - crf_viterbi_accuracy: 0.9950 - val_loss: 25.9667 - val_crf_viterbi_accuracy: 0.9865\n",
      "Epoch 5/15\n",
      " - 124s - loss: 30.6490 - crf_viterbi_accuracy: 0.9975 - val_loss: 25.9676 - val_crf_viterbi_accuracy: 0.9874\n",
      "Epoch 6/15\n",
      " - 123s - loss: 30.6454 - crf_viterbi_accuracy: 0.9986 - val_loss: 25.9723 - val_crf_viterbi_accuracy: 0.9878\n",
      "Epoch 7/15\n",
      " - 124s - loss: 30.6434 - crf_viterbi_accuracy: 0.9990 - val_loss: 25.9761 - val_crf_viterbi_accuracy: 0.9876\n",
      "Epoch 8/15\n",
      " - 124s - loss: 30.6427 - crf_viterbi_accuracy: 0.9993 - val_loss: 25.9829 - val_crf_viterbi_accuracy: 0.9875\n",
      "Epoch 9/15\n",
      " - 124s - loss: 30.6423 - crf_viterbi_accuracy: 0.9996 - val_loss: 25.9821 - val_crf_viterbi_accuracy: 0.9880\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "\n",
    "history = model.fit(train_valX, train_valy, batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,validation_split=.1,\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17962, 300)\n"
     ]
    }
   ],
   "source": [
    "test_without_tag = test[pd.notnull(test.tag) == False]\n",
    "\n",
    "testX = np.load('../data/testX.npy')\n",
    "'''\n",
    "testX = [[word2idx.get(w,1) for w in list(test_without_tag[test_without_tag.doc_sent == s].word)] for s in test_without_tag.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "testX = pad_sequences(maxlen=MAX_LEN, sequences=testX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "'''\n",
    "print (testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(testX)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "  0%|          | 0/17962 [00:00<?, ?it/s]/Users/victor/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "100%|██████████| 17962/17962 [2:45:30<00:00,  2.89it/s]    \n"
     ]
    }
   ],
   "source": [
    "test_without_tag['tag'] = ''\n",
    "for i, val in enumerate(tqdm(test_without_tag.doc_sent.unique())):\n",
    "    if len(test_without_tag[test_without_tag.doc_sent == val]) <= MAX_LEN:\n",
    "        test_without_tag.loc[test_without_tag.doc_sent == val,'tag'] = pred_tag[i][:len(test_without_tag[test_without_tag.doc_sent == val])]\n",
    "    else:\n",
    "        test_without_tag.loc[test_without_tag.doc_sent == val,'tag'] = pred_tag[i] + ['OTHER']*(len(test_without_tag[test_without_tag.doc_sent == val]) - MAX_LEN)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[pd.notnull(test.tag) == False,'tag'] = test_without_tag.tag\n",
    "test = test.drop_duplicates(['doc_sent','word','n1','n2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/model_bilstm_withfasttext_test.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/test_submission/bilstm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
