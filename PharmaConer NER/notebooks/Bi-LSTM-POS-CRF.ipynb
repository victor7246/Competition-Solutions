{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os, csv, math, codecs\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading\n",
    "\n",
    "Read training, dev and validation data. Dataset are in below format\n",
    "\n",
    "document id | sentence number | word | NER tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/subtask1_train.csv\")\n",
    "val = pd.read_csv(\"../data/subtask1_dev.csv\")\n",
    "test = pd.read_csv('../data/subtask1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['doc_sent'] = train.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "val['doc_sent'] = val.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "test['doc_sent'] = test.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.tag != 'NULL']\n",
    "val = val[val.tag != 'NULL']\n",
    "train = train[pd.notnull(train.tag)]\n",
    "val = val[pd.notnull(val.tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97244, 6) (49051, 8)\n",
      "(49051, 8)\n",
      "(872149, 7)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print (train.shape, val.shape)\n",
    "#train['word'] = train['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#train['wordlen'] = train.word.apply(lambda x: len(x))\n",
    "#train = train[train.wordlen >= 2]\n",
    "#print (train.shape)\n",
    "#val['word'] = val['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#val['wordlen'] = val.word.apply(lambda x: len(x))\n",
    "#val = val[val.wordlen >= 2]\n",
    "print (val.shape)\n",
    "test.replace(np.nan,'',inplace=True)\n",
    "#test['word'] = test['word'].apply(lambda x: x.lower())\n",
    "#test['word'] = test['word'].apply(lambda x: re.sub(r'[^\\w]','',str(x)))\n",
    "#test['wordlen'] = test.word.apply(lambda x: len(x))\n",
    "#test = test[test.wordlen >= 2]\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>caso</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>mujer</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence         word    tag   pos  \\\n",
       "0  S0004-06142008000100008-1         0  presentamos  OTHER  VERB   \n",
       "1  S0004-06142008000100008-1         0         caso  OTHER  NOUN   \n",
       "2  S0004-06142008000100008-1         0        mujer  OTHER  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>caso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>mujer</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>NUM</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>años</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence  n1  n2         word   pos  \\\n",
       "0  S0004-06142008000100008-1         0   0  11  presentamos  VERB   \n",
       "1  S0004-06142008000100008-1         0  15  19         caso  NOUN   \n",
       "2  S0004-06142008000100008-1         0  27  32        mujer  NOUN   \n",
       "3  S0004-06142008000100008-1         0  36  38           30   NUM   \n",
       "4  S0004-06142008000100008-1         0  39  43         años  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  \n",
       "3  S0004-06142008000100008-1_0  \n",
       "4  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test = pd.merge(test,pd.concat([train,val],axis=0),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER               47127\n",
       "NORMALIZABLES        1048\n",
       "PROTEINAS             836\n",
       "UNCLEAR                27\n",
       "NO_NORMALIZABLES       13\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49051 872149\n"
     ]
    }
   ],
   "source": [
    "print (len(test[pd.notnull(test.tag)]), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2315.000000\n",
       "mean       38.097624\n",
       "std        33.411630\n",
       "min         1.000000\n",
       "25%        16.000000\n",
       "50%        29.000000\n",
       "75%        50.000000\n",
       "max       359.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19048.000000\n",
       "mean        41.185426\n",
       "std         37.742500\n",
       "min          1.000000\n",
       "25%         17.000000\n",
       "50%         31.000000\n",
       "75%         53.000000\n",
       "max        454.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1086.000000\n",
       "mean       40.960405\n",
       "std        34.282256\n",
       "min         1.000000\n",
       "25%        18.000000\n",
       "50%        32.000000\n",
       "75%        52.000000\n",
       "max       264.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.groupby(['document','sentence'])['word'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80159 5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "MAX_LEN = 300\n",
    "EMBEDDING = 300\n",
    "MAX_NB_WORDS = 80000\n",
    "n_tags = train.tag.nunique()\n",
    "words = list(set(test.word))\n",
    "n_words = len(words)\n",
    "print (n_words, n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, BatchNormalization, GRU, CuDNNLSTM, CuDNNGRU, Concatenate\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing for LSTM model\n",
    "\n",
    "Convert the input sentences into sequence of words with maximum length as 300. For outputs we one hot encode. Additionally, we add 'PAD' to shorter input texts, as well as in the outputs for TAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = list(set(train.word))\n",
    "poss = list(set(test.pos))\n",
    "tags = list(set(train.tag))\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "pos2idx = {w: i + 2 for i, w in enumerate(poss)}\n",
    "pos2idx[\"UNK\"] = 1 # Padding\n",
    "pos2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "idx2pos = {i: w for w, i in pos2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SCONJ': 2,\n",
       " 'NUM': 3,\n",
       " 'PROPN': 4,\n",
       " 'SPACE': 5,\n",
       " 'VERB': 6,\n",
       " 'INTJ': 7,\n",
       " 'CONJ': 8,\n",
       " 'PUNCT': 9,\n",
       " 'PRON': 10,\n",
       " 'ADJ': 11,\n",
       " 'DET': 12,\n",
       " 'NOUN': 13,\n",
       " 'SYM': 14,\n",
       " 'ADV': 15,\n",
       " 'ADP': 16,\n",
       " 'AUX': 17,\n",
       " 'UNK': 1,\n",
       " 'PAD': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315, 300) (1086, 300) (2315, 300, 6) (1086, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "# Convert each sentence from list of Token to list of word_index\n",
    "trainX = [[word2idx[w] for w in list(train[train.doc_sent == s].word)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainX = pad_sequences(maxlen=MAX_LEN, sequences=trainX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valX = [[word2idx.get(w,1) for w in list(val[val.doc_sent == s].word)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valX = pad_sequences(maxlen=MAX_LEN, sequences=valX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "trainX_pos = [[pos2idx[w] for w in list(train[train.doc_sent == s].pos)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainX_pos = pad_sequences(maxlen=MAX_LEN, sequences=trainX_pos, padding=\"post\", value=pos2idx[\"PAD\"])\n",
    "\n",
    "valX_pos = [[pos2idx.get(w,1) for w in list(val[val.doc_sent == s].pos)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valX_pos = pad_sequences(maxlen=MAX_LEN, sequences=valX_pos, padding=\"post\", value=pos2idx[\"PAD\"])\n",
    "\n",
    "trainy = [[tag2idx[w] for w in list(train[train.doc_sent == s].tag)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainy = pad_sequences(maxlen=MAX_LEN, sequences=trainy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valy = [[tag2idx[w] for w in list(val[val.doc_sent == s].tag)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valy = pad_sequences(maxlen=MAX_LEN, sequences=valy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# One-Hot encode\n",
    "trainy = [to_categorical(i, num_classes=n_tags+1) for i in trainy]  # n_tags+1(PAD)\n",
    "valy = [to_categorical(i, num_classes=n_tags+1) for i in valy]  # n_tags+1(PAD)\n",
    "\n",
    "print (np.array(trainX).shape, np.array(valX).shape, np.array(trainy).shape, np.array(valy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1853it [00:00, 8736.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:38, 12628.75it/s]\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/Users/victor/Documents/Models/cc.es.300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "42579\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "EMBEDDING = 300\n",
    "words_not_found = []\n",
    "embedding_matrix = np.random.uniform(low=-.25,high=.25,size=(n_words+2, EMBEDDING))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print (len(words_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 300, 300)     24048300    input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 300, 300)     5400        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 300, 200)     320800      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 300, 200)     320800      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 300, 400)     0           bidirectional_9[0][0]            \n",
      "                                                                 bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 300, 50)      20050       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, 300, 6)       354         time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,715,704\n",
      "Trainable params: 24,715,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input1 = Input(shape=(MAX_LEN,))\n",
    "in_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input1)  # default: 20-dim embedding\n",
    "\n",
    "input2 = Input(shape=(MAX_LEN,))\n",
    "in_pos = Embedding(input_dim=len(pos2idx), output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input2)  # default: 20-dim embedding\n",
    "\n",
    "model1 = Bidirectional(LSTM(units=100, return_sequences=True))(in_word)  # variational biLSTM\n",
    "model2 = Bidirectional(LSTM(units=100, return_sequences=True))(in_pos)\n",
    "\n",
    "model = Concatenate()([model1,model2])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model([input1,input2], out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/15\n",
      " - 109s - loss: 32.3614 - crf_viterbi_accuracy: 0.9490 - val_loss: 26.2058 - val_crf_viterbi_accuracy: 0.9650\n",
      "Epoch 2/15\n",
      " - 102s - loss: 32.1360 - crf_viterbi_accuracy: 0.9788 - val_loss: 26.1591 - val_crf_viterbi_accuracy: 0.9836\n",
      "Epoch 3/15\n",
      " - 98s - loss: 32.0930 - crf_viterbi_accuracy: 0.9937 - val_loss: 26.1645 - val_crf_viterbi_accuracy: 0.9854\n",
      "Epoch 4/15\n",
      " - 99s - loss: 32.0829 - crf_viterbi_accuracy: 0.9974 - val_loss: 26.1678 - val_crf_viterbi_accuracy: 0.9864\n",
      "Epoch 5/15\n",
      " - 99s - loss: 32.0801 - crf_viterbi_accuracy: 0.9983 - val_loss: 26.1785 - val_crf_viterbi_accuracy: 0.9865\n",
      "Epoch 6/15\n",
      " - 101s - loss: 32.0782 - crf_viterbi_accuracy: 0.9991 - val_loss: 26.1837 - val_crf_viterbi_accuracy: 0.9865\n",
      "Epoch 7/15\n",
      " - 105s - loss: 32.0774 - crf_viterbi_accuracy: 0.9994 - val_loss: 26.1827 - val_crf_viterbi_accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit([np.array(trainX),np.array(trainX_pos)], np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=([np.array(valX),np.array(valX_pos)],np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_pos_withoutfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_pos_withoutfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict([valX,valX_pos])\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.96      0.69      0.80      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      1.00      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.87      0.73      0.79       836\n",
      "         UNCLEAR       0.46      0.22      0.30        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.71      0.61      0.65    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:29<00:00, 36.31it/s]\n"
     ]
    }
   ],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_pos_withoutfasttext.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 300, 300)     24048300    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 300, 300)     5400        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 300, 200)     320800      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 300, 200)     320800      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 300, 400)     0           bidirectional_11[0][0]           \n",
      "                                                                 bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 300, 50)      20050       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, 300, 6)       354         time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,715,704\n",
      "Trainable params: 24,715,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input1 = Input(shape=(MAX_LEN,))\n",
    "in_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING,weights=[embedding_matrix],trainable=True, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input1)  # default: 20-dim embedding\n",
    "\n",
    "input2 = Input(shape=(MAX_LEN,))\n",
    "in_pos = Embedding(input_dim=len(pos2idx), output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input2)  # default: 20-dim embedding\n",
    "\n",
    "model1 = Bidirectional(LSTM(units=100, return_sequences=True))(in_word)  # variational biLSTM\n",
    "model2 = Bidirectional(LSTM(units=100, return_sequences=True))(in_pos)\n",
    "\n",
    "model = Concatenate()([model1,model2])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model([input1,input2], out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/15\n",
      " - 112s - loss: 32.3184 - crf_viterbi_accuracy: 0.9501 - val_loss: 26.2071 - val_crf_viterbi_accuracy: 0.9615\n",
      "Epoch 2/15\n",
      " - 97s - loss: 32.1420 - crf_viterbi_accuracy: 0.9749 - val_loss: 26.1531 - val_crf_viterbi_accuracy: 0.9819\n",
      "Epoch 3/15\n",
      " - 98s - loss: 32.0971 - crf_viterbi_accuracy: 0.9924 - val_loss: 26.1451 - val_crf_viterbi_accuracy: 0.9853\n",
      "Epoch 4/15\n",
      " - 94s - loss: 32.0845 - crf_viterbi_accuracy: 0.9969 - val_loss: 26.1549 - val_crf_viterbi_accuracy: 0.9862\n",
      "Epoch 5/15\n",
      " - 94s - loss: 32.0806 - crf_viterbi_accuracy: 0.9981 - val_loss: 26.1499 - val_crf_viterbi_accuracy: 0.9864\n",
      "Epoch 6/15\n",
      " - 98s - loss: 32.0789 - crf_viterbi_accuracy: 0.9987 - val_loss: 26.1603 - val_crf_viterbi_accuracy: 0.9868\n",
      "Epoch 7/15\n",
      " - 94s - loss: 32.0773 - crf_viterbi_accuracy: 0.9994 - val_loss: 26.1614 - val_crf_viterbi_accuracy: 0.9868\n",
      "Epoch 8/15\n",
      " - 94s - loss: 32.0766 - crf_viterbi_accuracy: 0.9997 - val_loss: 26.1648 - val_crf_viterbi_accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit([np.array(trainX),np.array(trainX_pos)], np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=([np.array(valX),np.array(valX_pos)],np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_pos_withfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_pos_withfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict([valX,valX_pos])\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.94      0.73      0.82      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      1.00      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.84      0.75      0.80       836\n",
      "         UNCLEAR       0.38      0.37      0.38        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.69      0.64      0.66    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:15<00:00, 70.80it/s]\n"
     ]
    }
   ],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_pos_withfasttext.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     24048300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     5400        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300, 200)     320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 300, 200)     320800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300, 400)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 300, 50)      20050       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 300, 6)       354         time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,715,704\n",
      "Trainable params: 24,715,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input1 = Input(shape=(MAX_LEN,))\n",
    "in_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input1)  # default: 20-dim embedding\n",
    "\n",
    "input2 = Input(shape=(MAX_LEN,))\n",
    "in_pos = Embedding(input_dim=len(pos2idx), output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input2)  # default: 20-dim embedding\n",
    "\n",
    "model1 = Bidirectional(LSTM(units=100, return_sequences=True))(in_word)  # variational biLSTM\n",
    "model2 = Bidirectional(LSTM(units=100, return_sequences=True))(in_pos)\n",
    "\n",
    "model = Concatenate()([model1,model2])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model([input1,input2], out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3401, 300) (3401, 300) (3401, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "train_valX = np.concatenate([np.array(trainX),np.array(valX)],axis=0)\n",
    "train_val_posX = np.concatenate([np.array(trainX_pos),np.array(valX_pos)],axis=0)\n",
    "train_valy = np.concatenate([np.array(trainy),np.array(valy)],axis=0)\n",
    "\n",
    "print (train_valX.shape, train_val_posX.shape, train_valy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3060 samples, validate on 341 samples\n",
      "Epoch 1/15\n",
      " - 141s - loss: 30.6966 - crf_viterbi_accuracy: 0.9806 - val_loss: 25.9425 - val_crf_viterbi_accuracy: 0.9882\n",
      "Epoch 2/15\n",
      " - 137s - loss: 30.6562 - crf_viterbi_accuracy: 0.9945 - val_loss: 25.9488 - val_crf_viterbi_accuracy: 0.9895\n",
      "Epoch 3/15\n",
      " - 135s - loss: 30.6472 - crf_viterbi_accuracy: 0.9976 - val_loss: 25.9523 - val_crf_viterbi_accuracy: 0.9904\n",
      "Epoch 4/15\n",
      " - 136s - loss: 30.6448 - crf_viterbi_accuracy: 0.9983 - val_loss: 25.9616 - val_crf_viterbi_accuracy: 0.9902\n",
      "Epoch 5/15\n",
      " - 135s - loss: 30.6435 - crf_viterbi_accuracy: 0.9989 - val_loss: 25.9678 - val_crf_viterbi_accuracy: 0.9897\n",
      "Epoch 6/15\n",
      " - 135s - loss: 30.6427 - crf_viterbi_accuracy: 0.9993 - val_loss: 25.9686 - val_crf_viterbi_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit([np.array(train_valX),np.array(train_val_posX)], np.array(train_valy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_split=.1,\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17962, 300) (17962, 300)\n"
     ]
    }
   ],
   "source": [
    "test_without_tag = test[pd.notnull(test.tag) == False]\n",
    "\n",
    "testX = np.load('../data/testX.npy')\n",
    "'''\n",
    "testX = [[word2idx.get(w,1) for w in list(test_without_tag[test_without_tag.doc_sent == s].word)] for s in test_without_tag.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "testX = pad_sequences(maxlen=MAX_LEN, sequences=testX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "'''\n",
    "testX_pos = [[word2idx.get(w,1) for w in list(test_without_tag[test_without_tag.doc_sent == s].pos)] for s in test_without_tag.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "testX_pos = pad_sequences(maxlen=MAX_LEN, sequences=testX_pos, padding=\"post\", value=pos2idx[\"PAD\"])\n",
    "\n",
    "np.save('../data/testX_pos',testX_pos)\n",
    "\n",
    "print (testX.shape,testX_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict([testX,testX_pos])\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "100%|██████████| 17962/17962 [44:26<00:00,  5.27it/s]   \n"
     ]
    }
   ],
   "source": [
    "test_without_tag['tag'] = ''\n",
    "output = []\n",
    "for i, val in enumerate(tqdm(test_without_tag.doc_sent.unique())):\n",
    "    if len(test_without_tag[test_without_tag.doc_sent == val]) <= MAX_LEN:\n",
    "        output += pred_tag[i][:len(test_without_tag[test_without_tag.doc_sent == val])]\n",
    "    else:\n",
    "        output += pred_tag[i] + ['OTHER']*(len(test_without_tag[test_without_tag.doc_sent == val]) - MAX_LEN)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_without_tag['tag'] = output\n",
    "test.loc[pd.notnull(test.tag) == False,'tag'] = test_without_tag.tag\n",
    "test = test.drop_duplicates(['doc_sent','word','n1','n2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/model_bilstm_pos_withoutfasttext_test.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/test_submission/bilstm_tag_crf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
