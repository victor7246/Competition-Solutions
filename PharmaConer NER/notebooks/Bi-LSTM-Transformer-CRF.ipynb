{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os, csv, math, codecs\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading\n",
    "\n",
    "Read training, dev and validation data. Dataset are in below format\n",
    "\n",
    "document id | sentence number | word | NER tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/subtask1_train.csv\")\n",
    "val = pd.read_csv(\"../data/subtask1_dev.csv\")\n",
    "test = pd.read_csv('../data/subtask1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['doc_sent'] = train.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "val['doc_sent'] = val.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)\n",
    "test['doc_sent'] = test.apply(lambda x: \"{}_{}\".format(str(x['document']),str(x['sentence'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.tag != 'NULL']\n",
    "val = val[val.tag != 'NULL']\n",
    "train = train[pd.notnull(train.tag)]\n",
    "val = val[pd.notnull(val.tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER               93482\n",
       "NORMALIZABLES        2174\n",
       "PROTEINAS            1501\n",
       "UNCLEAR                68\n",
       "NO_NORMALIZABLES       19\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER               47127\n",
       "NORMALIZABLES        1048\n",
       "PROTEINAS             836\n",
       "UNCLEAR                27\n",
       "NO_NORMALIZABLES       13\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97244, 6) (49051, 8)\n",
      "(49051, 8)\n",
      "(872149, 7)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print (train.shape, val.shape)\n",
    "#train['word'] = train['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#train['wordlen'] = train.word.apply(lambda x: len(x))\n",
    "#train = train[train.wordlen >= 2]\n",
    "#print (train.shape)\n",
    "#val['word'] = val['word'].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "#val['wordlen'] = val.word.apply(lambda x: len(x))\n",
    "#val = val[val.wordlen >= 2]\n",
    "print (val.shape)\n",
    "test.replace(np.nan,'',inplace=True)\n",
    "#test['word'] = test['word'].apply(lambda x: x.lower())\n",
    "#test['word'] = test['word'].apply(lambda x: re.sub(r'[^\\w]','',str(x)))\n",
    "#test['wordlen'] = test.word.apply(lambda x: len(x))\n",
    "#test = test[test.wordlen >= 2]\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>caso</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>mujer</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence         word    tag   pos  \\\n",
       "0  S0004-06142008000100008-1         0  presentamos  OTHER  VERB   \n",
       "1  S0004-06142008000100008-1         0         caso  OTHER  NOUN   \n",
       "2  S0004-06142008000100008-1         0        mujer  OTHER  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>doc_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>presentamos</td>\n",
       "      <td>VERB</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>caso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>mujer</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>NUM</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142008000100008-1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>años</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S0004-06142008000100008-1_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    document  sentence  n1  n2         word   pos  \\\n",
       "0  S0004-06142008000100008-1         0   0  11  presentamos  VERB   \n",
       "1  S0004-06142008000100008-1         0  15  19         caso  NOUN   \n",
       "2  S0004-06142008000100008-1         0  27  32        mujer  NOUN   \n",
       "3  S0004-06142008000100008-1         0  36  38           30   NUM   \n",
       "4  S0004-06142008000100008-1         0  39  43         años  NOUN   \n",
       "\n",
       "                      doc_sent  \n",
       "0  S0004-06142008000100008-1_0  \n",
       "1  S0004-06142008000100008-1_0  \n",
       "2  S0004-06142008000100008-1_0  \n",
       "3  S0004-06142008000100008-1_0  \n",
       "4  S0004-06142008000100008-1_0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test = pd.merge(test,pd.concat([train,val],axis=0),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER               47127\n",
       "NORMALIZABLES        1048\n",
       "PROTEINAS             836\n",
       "UNCLEAR                27\n",
       "NO_NORMALIZABLES       13\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49051 872149\n"
     ]
    }
   ],
   "source": [
    "print (len(test[pd.notnull(test.tag)]), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80159"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "all_chars = list(set(list(\" \".join(test.word))))\n",
    "print (len(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80159 5 143\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "MAX_LEN = 300\n",
    "EMBEDDING = 300\n",
    "MAX_NB_WORDS = 80000\n",
    "n_tags = train.tag.nunique()\n",
    "words = list(set(test.word))\n",
    "n_words = len(words)\n",
    "n_chars = len(all_chars)\n",
    "print (n_words, n_tags, n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, BatchNormalization, GRU, CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Conv1D, Conv2D, Concatenate, Dropout, MaxPooling1D, MaxPooling2D, Flatten, GlobalAveragePooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing for LSTM model\n",
    "\n",
    "Convert the input sentences into sequence of words with maximum length as 300. For outputs we one hot encode. Additionally, we add 'PAD' to shorter input texts, as well as in the outputs for TAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = list(set(train.word))\n",
    "tags = list(set(train.tag))\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315, 300) (1086, 300) (2315, 300, 6) (1086, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "# Convert each sentence from list of Token to list of word_index\n",
    "trainX_word = [[word2idx[w] for w in list(train[train.doc_sent == s].word)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainX_word = pad_sequences(maxlen=MAX_LEN, sequences=trainX_word, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valX_word = [[word2idx.get(w,1) for w in list(val[val.doc_sent == s].word)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valX_word = pad_sequences(maxlen=MAX_LEN, sequences=valX_word, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "trainy = [[tag2idx[w] for w in list(train[train.doc_sent == s].tag)] for s in train.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "trainy = pad_sequences(maxlen=MAX_LEN, sequences=trainy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "valy = [[tag2idx[w] for w in list(val[val.doc_sent == s].tag)] for s in val.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "valy = pad_sequences(maxlen=MAX_LEN, sequences=valy, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# One-Hot encode\n",
    "trainy = [to_categorical(i, num_classes=n_tags+1) for i in trainy]  # n_tags+1(PAD)\n",
    "valy = [to_categorical(i, num_classes=n_tags+1) for i in valy]  # n_tags+1(PAD)\n",
    "\n",
    "print (np.array(trainX_word).shape, np.array(valX_word).shape, np.array(trainy).shape, np.array(valy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 \n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "'''\n",
    "output dimention: [batch_size, time_step, nb_head*size_per_head]\n",
    "every word can be represented as a vector [nb_head*size_per_head]\n",
    "'''\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "747it [00:00, 7464.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [05:25, 6138.48it/s]\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/Users/victor/Documents/Models/cc.es.300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "42579\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "EMBEDDING = 300\n",
    "words_not_found = []\n",
    "embedding_matrix = np.random.uniform(low=-.25,high=.25,size=(n_words+2, EMBEDDING))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print (len(words_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     24048300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 300, 200)     320800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_2 (Position (None, 300, 200)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 300, 100)     60000       position__embedding_2[0][0]      \n",
      "                                                                 position__embedding_2[0][0]      \n",
      "                                                                 position__embedding_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 300, 50)      5050        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, 300, 6)       354         time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,434,504\n",
      "Trainable params: 24,434,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input_word = Input(shape=(MAX_LEN,))\n",
    "out_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=False)(input_word)  # default: 20-dim embedding\n",
    "\n",
    "\n",
    "#lstm1 = Bidirectional(LSTM(units=150, return_sequences=True))(out_word)  # variational biLSTM\n",
    "#conv1 = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv2 = Conv1D(kernel_size=5, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv3 = Conv1D(kernel_size=7, filters=300, padding='same', strides=1)(out_word)\n",
    "\n",
    "#model = Concatenate()([lstm1,conv1,conv2,conv3])\n",
    "\n",
    "#model = Bidirectional(LSTM(units=150, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(model)\n",
    "\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True))(out_word)\n",
    "\n",
    "model = Position_Embedding()(model)\n",
    "model = Attention(nb_head=10,size_per_head=10)([model,model,model])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "\n",
    "encoding = Model(input_word, model)\n",
    "\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/30\n",
      " - 85s - loss: 0.2415 - crf_viterbi_accuracy: 0.9239 - val_loss: 0.0365 - val_crf_viterbi_accuracy: 0.9930\n",
      "Epoch 2/30\n",
      " - 84s - loss: 0.0293 - crf_viterbi_accuracy: 0.9942 - val_loss: 0.0298 - val_crf_viterbi_accuracy: 0.9939\n",
      "Epoch 3/30\n",
      " - 84s - loss: 0.0270 - crf_viterbi_accuracy: 0.9944 - val_loss: 0.0275 - val_crf_viterbi_accuracy: 0.9940\n",
      "Epoch 4/30\n",
      " - 82s - loss: 0.0206 - crf_viterbi_accuracy: 0.9944 - val_loss: 0.0199 - val_crf_viterbi_accuracy: 0.9938\n",
      "Epoch 5/30\n",
      " - 80s - loss: 0.0120 - crf_viterbi_accuracy: 0.9955 - val_loss: 0.0146 - val_crf_viterbi_accuracy: 0.9957\n",
      "Epoch 6/30\n",
      " - 87s - loss: 0.0068 - crf_viterbi_accuracy: 0.9971 - val_loss: 0.0143 - val_crf_viterbi_accuracy: 0.9962\n",
      "Epoch 7/30\n",
      " - 82s - loss: 0.0046 - crf_viterbi_accuracy: 0.9982 - val_loss: 0.0171 - val_crf_viterbi_accuracy: 0.9964\n",
      "Epoch 8/30\n",
      " - 81s - loss: 0.0032 - crf_viterbi_accuracy: 0.9988 - val_loss: 0.0142 - val_crf_viterbi_accuracy: 0.9970\n",
      "Epoch 9/30\n",
      " - 86s - loss: 0.0020 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0160 - val_crf_viterbi_accuracy: 0.9974\n",
      "Epoch 10/30\n",
      " - 87s - loss: 0.0013 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0159 - val_crf_viterbi_accuracy: 0.9975\n",
      "Epoch 11/30\n",
      " - 81s - loss: 7.8463e-04 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0173 - val_crf_viterbi_accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit(np.array(trainX_word), np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=(np.array(valX_word),np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_cnn_transformer_withoutfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_cnn_transformer_withoutfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(valX_word)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.91      0.60      0.72      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      1.00      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.82      0.69      0.75       836\n",
      "         UNCLEAR       1.00      0.04      0.07        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.79      0.55      0.59    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:30<00:00, 32.66it/s]\n"
     ]
    }
   ],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_transformer_withoutfasttext.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 300, 300)     24048300    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_3 (Position (None, 300, 300)     0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 300, 100)     90000       position__embedding_3[0][0]      \n",
      "                                                                 position__embedding_3[0][0]      \n",
      "                                                                 position__embedding_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 300, 50)      5050        attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, 300, 6)       354         time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,143,704\n",
      "Trainable params: 24,143,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input_word = Input(shape=(MAX_LEN,))\n",
    "out_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING,weights=[embedding_matrix],trainable=True, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=False)(input_word)  # default: 20-dim embedding\n",
    "\n",
    "\n",
    "#lstm1 = Bidirectional(LSTM(units=150, return_sequences=True))(out_word)  # variational biLSTM\n",
    "#conv1 = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv2 = Conv1D(kernel_size=5, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv3 = Conv1D(kernel_size=7, filters=300, padding='same', strides=1)(out_word)\n",
    "\n",
    "#model = Concatenate()([lstm1,conv1,conv2,conv3])\n",
    "\n",
    "#model = Bidirectional(LSTM(units=150, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(model)\n",
    "\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True))(out_word)\n",
    "\n",
    "model = Position_Embedding()(out_word)\n",
    "model = Attention(nb_head=10,size_per_head=10)([model,model,model])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "\n",
    "encoding = Model(input_word, model)\n",
    "\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2315 samples, validate on 1086 samples\n",
      "Epoch 1/30\n",
      " - 62s - loss: 0.0973 - crf_viterbi_accuracy: 0.9755 - val_loss: 0.0301 - val_crf_viterbi_accuracy: 0.9940\n",
      "Epoch 2/30\n",
      " - 52s - loss: 0.0228 - crf_viterbi_accuracy: 0.9947 - val_loss: 0.0181 - val_crf_viterbi_accuracy: 0.9951\n",
      "Epoch 3/30\n",
      " - 53s - loss: 0.0108 - crf_viterbi_accuracy: 0.9967 - val_loss: 0.0143 - val_crf_viterbi_accuracy: 0.9959\n",
      "Epoch 4/30\n",
      " - 48s - loss: 0.0064 - crf_viterbi_accuracy: 0.9981 - val_loss: 0.0132 - val_crf_viterbi_accuracy: 0.9969\n",
      "Epoch 5/30\n",
      " - 51s - loss: 0.0041 - crf_viterbi_accuracy: 0.9990 - val_loss: 0.0149 - val_crf_viterbi_accuracy: 0.9969\n",
      "Epoch 6/30\n",
      " - 51s - loss: 0.0031 - crf_viterbi_accuracy: 0.9992 - val_loss: 0.0134 - val_crf_viterbi_accuracy: 0.9970\n",
      "Epoch 7/30\n",
      " - 49s - loss: 0.0025 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0150 - val_crf_viterbi_accuracy: 0.9975\n",
      "Epoch 8/30\n",
      " - 53s - loss: 0.0020 - crf_viterbi_accuracy: 0.9995 - val_loss: 0.0154 - val_crf_viterbi_accuracy: 0.9973\n",
      "Epoch 9/30\n",
      " - 53s - loss: 0.0017 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0140 - val_crf_viterbi_accuracy: 0.9964\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "history = model.fit(np.array(trainX_word), np.array(trainy), batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,\n",
    "                    validation_data=(np.array(valX_word),np.array(valy)),\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model_bilstm_cnn_transformer_withfasttext.json\", \"w\") as output:\n",
    "    output.write(model.to_json())\n",
    "    \n",
    "model.save_weights(\"../models/model_bilstm_cnn_transformer_withfasttext.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(valX_word)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "valy_true = np.argmax(valy, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   NORMALIZABLES       0.67      0.70      0.68      1048\n",
      "NO_NORMALIZABLES       0.00      0.00      0.00        13\n",
      "           OTHER       0.99      0.99      0.99     47099\n",
      "             PAD       1.00      1.00      1.00    276777\n",
      "       PROTEINAS       0.68      0.72      0.70       836\n",
      "         UNCLEAR       0.71      0.19      0.29        27\n",
      "\n",
      "       micro avg       1.00      1.00      1.00    325800\n",
      "       macro avg       0.68      0.60      0.61    325800\n",
      "    weighted avg       1.00      1.00      1.00    325800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "valy_true_tag = [[idx2tag[i] for i in row] for row in valy_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=valy_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:28<00:00, 37.66it/s]\n"
     ]
    }
   ],
   "source": [
    "val['tag_pred'] = ''\n",
    "for i, value in enumerate(tqdm(val.doc_sent.unique())):\n",
    "    if len(val[val.doc_sent == value]) <= MAX_LEN:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i][:len(val[val.doc_sent == value])]\n",
    "    else:\n",
    "        val.loc[val.doc_sent == value,'tag_pred'] = pred_tag[i] + ['OTHER']*(len(val[val.doc_sent == value]) - MAX_LEN)\n",
    "        \n",
    "val.to_csv('../data/val_submission/bilstm_transformer_withfasttext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     24048300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300, 200)     320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_1 (Position (None, 300, 200)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 300, 100)     60000       position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 300, 50)      5050        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 300, 6)       354         time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 24,434,504\n",
      "Trainable params: 24,434,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input_word = Input(shape=(MAX_LEN,))\n",
    "out_word = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=False)(input_word)  # default: 20-dim embedding\n",
    "\n",
    "\n",
    "#lstm1 = Bidirectional(LSTM(units=150, return_sequences=True))(out_word)  # variational biLSTM\n",
    "#conv1 = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv2 = Conv1D(kernel_size=5, filters=300, padding='same', strides=1)(out_word)\n",
    "#conv3 = Conv1D(kernel_size=7, filters=300, padding='same', strides=1)(out_word)\n",
    "\n",
    "#model = Concatenate()([lstm1,conv1,conv2,conv3])\n",
    "\n",
    "#model = Bidirectional(LSTM(units=150, return_sequences=True))(model)  # variational biLSTM\n",
    "#model = Conv1D(kernel_size=3, filters=300, padding='same', strides=1)(model)\n",
    "\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True))(out_word)\n",
    "\n",
    "model = Position_Embedding()(model)\n",
    "model = Attention(nb_head=10,size_per_head=10)([model,model,model])\n",
    "\n",
    "#model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "#model = BatchNormalization()(model)\n",
    "#model = Bidirectional(LSTM(units=100, activation='relu', return_sequences=True))(model)\n",
    "model = TimeDistributed(Dense(50))(model)\n",
    "\n",
    "encoding = Model(input_word, model)\n",
    "\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input_word, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3401, 300) (3401, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "train_valX = np.concatenate([np.array(trainX_word),np.array(valX_word)],axis=0)\n",
    "train_valy = np.concatenate([np.array(trainy),np.array(valy)],axis=0)\n",
    "\n",
    "print (train_valX.shape, train_valy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3060 samples, validate on 341 samples\n",
      "Epoch 1/15\n",
      " - 132s - loss: 0.1391 - crf_viterbi_accuracy: 0.9519 - val_loss: 0.0342 - val_crf_viterbi_accuracy: 0.9942\n",
      "Epoch 2/15\n",
      " - 125s - loss: 0.0303 - crf_viterbi_accuracy: 0.9943 - val_loss: 0.0303 - val_crf_viterbi_accuracy: 0.9944\n",
      "Epoch 3/15\n",
      " - 127s - loss: 0.0251 - crf_viterbi_accuracy: 0.9944 - val_loss: 0.0206 - val_crf_viterbi_accuracy: 0.9944\n",
      "Epoch 4/15\n",
      " - 126s - loss: 0.0140 - crf_viterbi_accuracy: 0.9954 - val_loss: 0.0145 - val_crf_viterbi_accuracy: 0.9960\n",
      "Epoch 5/15\n",
      " - 129s - loss: 0.0078 - crf_viterbi_accuracy: 0.9977 - val_loss: 0.0133 - val_crf_viterbi_accuracy: 0.9976\n",
      "Epoch 6/15\n",
      " - 135s - loss: 0.0049 - crf_viterbi_accuracy: 0.9989 - val_loss: 0.0125 - val_crf_viterbi_accuracy: 0.9980\n",
      "Epoch 7/15\n",
      " - 131s - loss: 0.0034 - crf_viterbi_accuracy: 0.9994 - val_loss: 0.0127 - val_crf_viterbi_accuracy: 0.9982\n",
      "Epoch 8/15\n",
      " - 126s - loss: 0.0026 - crf_viterbi_accuracy: 0.9996 - val_loss: 0.0114 - val_crf_viterbi_accuracy: 0.9983\n",
      "Epoch 9/15\n",
      " - 124s - loss: 0.0022 - crf_viterbi_accuracy: 0.9997 - val_loss: 0.0130 - val_crf_viterbi_accuracy: 0.9983\n",
      "Epoch 10/15\n",
      " - 119s - loss: 0.0020 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0129 - val_crf_viterbi_accuracy: 0.9984\n",
      "Epoch 11/15\n",
      " - 121s - loss: 0.0019 - crf_viterbi_accuracy: 0.9998 - val_loss: 0.0153 - val_crf_viterbi_accuracy: 0.9982\n",
      "Epoch 12/15\n",
      " - 125s - loss: 0.0016 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0146 - val_crf_viterbi_accuracy: 0.9983\n",
      "Epoch 13/15\n",
      " - 124s - loss: 0.0014 - crf_viterbi_accuracy: 0.9999 - val_loss: 0.0153 - val_crf_viterbi_accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_crf_viterbi_accuracy', factor=0.2,\n",
    "#                              patience=3, min_lr=0.005)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0002, \n",
    "                                           patience=5, verbose=0, mode='min')\n",
    "\n",
    "history = model.fit(train_valX, train_valy, batch_size=BATCH_SIZE, epochs=EPOCHS,verbose=2,validation_split=.1,\n",
    "                   callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/model_bilstm_cnn_transformer_withoutfasttext_test.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17962, 300)\n"
     ]
    }
   ],
   "source": [
    "test_without_tag = test[pd.notnull(test.tag) == False]\n",
    "\n",
    "testX = np.load('../data/testX.npy')\n",
    "'''\n",
    "testX = [[word2idx.get(w,1) for w in list(test_without_tag[test_without_tag.doc_sent == s].word)] for s in test_without_tag.doc_sent.unique()]\n",
    "# Padding each sentence to have the same lenght\n",
    "testX = pad_sequences(maxlen=MAX_LEN, sequences=testX, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "'''\n",
    "print (testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cat = model.predict(testX)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "100%|██████████| 17962/17962 [26:50<00:00, 11.18it/s]\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_without_tag['tag'] = ''\n",
    "output = []\n",
    "for i, val in enumerate(tqdm(test_without_tag.doc_sent.unique())):\n",
    "    if len(test_without_tag[test_without_tag.doc_sent == val]) <= MAX_LEN:\n",
    "        output += pred_tag[i][:len(test_without_tag[test_without_tag.doc_sent == val])]\n",
    "    else:\n",
    "        output += pred_tag[i] + ['OTHER']*(len(test_without_tag[test_without_tag.doc_sent == val]) - MAX_LEN)\n",
    "        \n",
    "test_without_tag['tag'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[pd.notnull(test.tag) == False,'tag'] = test_without_tag.tag\n",
    "test = test.drop_duplicates(['doc_sent','word','n1','n2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/test_submission/bilstm_transformer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
